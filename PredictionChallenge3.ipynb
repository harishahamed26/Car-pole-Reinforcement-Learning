{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harishahamed26/Car-pole-Reinforcement-Learning/blob/main/PredictionChallenge3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKK5DA390wRe"
      },
      "source": [
        "# Skeleton Code for Prediction Challenge 3\n",
        "Below is partial code to get you started on prediction challenge 3. You need to select values for the parameters that have question marks."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-rl2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gtr01pedYmmf",
        "outputId": "a32580f8-25ae-4ac4-e933-dcb2256ba20a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras-rl2\n",
            "  Downloading keras_rl2-1.0.5-py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.1/52.1 KB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.9/dist-packages (from keras-rl2) (2.11.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (0.31.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (4.5.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (3.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (2.11.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (2.2.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (23.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (0.4.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.4.0)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (2.11.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (3.19.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (57.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (15.0.6.1)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (2.11.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.51.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (23.3.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.38.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2.2.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2.16.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2.25.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (6.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2022.12.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (3.2.2)\n",
            "Installing collected packages: keras-rl2\n",
            "Successfully installed keras-rl2-1.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install gym"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwlNl77gYky4",
        "outputId": "19dd1944-9ea9-4b3a-bc0a-34a1492a87cc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.9/dist-packages (0.25.2)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.9/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.9/dist-packages (from gym) (1.22.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from gym) (6.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.8.0->gym) (3.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# load the gym module\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "# import the usual Keras modules for creating deep neural networks\n",
        "from keras import Sequential\n",
        "from keras.layers import Input, Flatten, Dense\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "ENV_NAME = 'CartPole-v0'\n",
        "env = gym.make(ENV_NAME)\n",
        "     \n",
        "\n",
        "import rl\n",
        "from rl.memory import SequentialMemory  # import the exerience replay buffer module\n",
        "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy # import the policy\n",
        "from rl.agents.dqn import DQNAgent   \n"
      ],
      "metadata": {
        "id": "Aw0JkEYOYhHL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Input(shape=(1,env.observation_space.shape[0])))  # The input is 1 observation vector, and the number of observations in that vector \n",
        "model.add(Flatten())\n",
        "# add extra layers here\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(env.action_space.n, activation='linear'))   # the output is the number of actions in the action space\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeM5rpqobHPM",
        "outputId": "983fe78a-ee5c-4b2e-a183-6c70b5a0172a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_1 (Flatten)         (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 16)                80        \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 32)                544       \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 64)                2112      \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 2)                 130       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,866\n",
            "Trainable params: 2,866\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8ZiiRbxlH2D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aebc0b4e-94af-40ac-bcbb-2f91f275dcb4"
      },
      "source": [
        "memory = SequentialMemory(limit=1000 ,window_length=1)\n",
        "\n",
        "\n",
        "policy =  LinearAnnealedPolicy(inner_policy= EpsGreedyQPolicy(), \n",
        "                               attr='eps',            \n",
        "                               value_max=1,\n",
        "                               value_min=2, \n",
        "                               value_test=.05,\n",
        "                               nb_steps=10000)\n",
        "\n",
        "# define the agent\n",
        "dqn = DQNAgent(model=model, \n",
        "               nb_actions=env.action_space.n,\n",
        "               memory=memory,\n",
        "               nb_steps_warmup=20,\n",
        "               target_model_update=0.001, \n",
        "               policy=policy) \n",
        "\n",
        "dqn.compile(Adam(lr = 0.001), metrics=['mae'])\n",
        "\n",
        "history = dqn.fit(env, nb_steps=10000, visualize=False, verbose=2)\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 10000 steps ...\n",
            "   38/10000: episode: 1, duration: 1.915s, episode steps:  38, steps per second:  20, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 0.433249, mae: 0.515194, mean_q: 0.120417, mean_eps: 2.000000\n",
            "   58/10000: episode: 2, duration: 0.129s, episode steps:  20, steps per second: 155, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.165149, mae: 0.523334, mean_q: 0.550164, mean_eps: 2.000000\n",
            "   71/10000: episode: 3, duration: 0.104s, episode steps:  13, steps per second: 125, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 0.041810, mae: 0.616707, mean_q: 1.082444, mean_eps: 2.000000\n",
            "  132/10000: episode: 4, duration: 0.415s, episode steps:  61, steps per second: 147, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.574 [0.000, 1.000],  loss: 0.009623, mae: 0.572499, mean_q: 1.104080, mean_eps: 2.000000\n",
            "  145/10000: episode: 5, duration: 0.102s, episode steps:  13, steps per second: 128, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.001459, mae: 0.570308, mean_q: 1.128889, mean_eps: 2.000000\n",
            "  176/10000: episode: 6, duration: 0.223s, episode steps:  31, steps per second: 139, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 0.001435, mae: 0.574834, mean_q: 1.137703, mean_eps: 2.000000\n",
            "  196/10000: episode: 7, duration: 0.145s, episode steps:  20, steps per second: 138, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 0.001592, mae: 0.584097, mean_q: 1.152383, mean_eps: 2.000000\n",
            "  204/10000: episode: 8, duration: 0.063s, episode steps:   8, steps per second: 126, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.001228, mae: 0.588813, mean_q: 1.169416, mean_eps: 2.000000\n",
            "  227/10000: episode: 9, duration: 0.153s, episode steps:  23, steps per second: 151, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 0.001383, mae: 0.588744, mean_q: 1.169896, mean_eps: 2.000000\n",
            "  250/10000: episode: 10, duration: 0.152s, episode steps:  23, steps per second: 151, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 0.001341, mae: 0.592093, mean_q: 1.177197, mean_eps: 2.000000\n",
            "  276/10000: episode: 11, duration: 0.167s, episode steps:  26, steps per second: 155, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.577 [0.000, 1.000],  loss: 0.001246, mae: 0.599962, mean_q: 1.199115, mean_eps: 2.000000\n",
            "  314/10000: episode: 12, duration: 0.253s, episode steps:  38, steps per second: 150, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.553 [0.000, 1.000],  loss: 0.001070, mae: 0.598891, mean_q: 1.200109, mean_eps: 2.000000\n",
            "  333/10000: episode: 13, duration: 0.129s, episode steps:  19, steps per second: 148, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 0.001415, mae: 0.607136, mean_q: 1.209958, mean_eps: 2.000000\n",
            "  346/10000: episode: 14, duration: 0.088s, episode steps:  13, steps per second: 148, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.002054, mae: 0.607275, mean_q: 1.211606, mean_eps: 2.000000\n",
            "  358/10000: episode: 15, duration: 0.079s, episode steps:  12, steps per second: 153, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.001859, mae: 0.622094, mean_q: 1.230843, mean_eps: 2.000000\n",
            "  377/10000: episode: 16, duration: 0.126s, episode steps:  19, steps per second: 151, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 0.001781, mae: 0.615900, mean_q: 1.225717, mean_eps: 2.000000\n",
            "  399/10000: episode: 17, duration: 0.150s, episode steps:  22, steps per second: 146, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.409 [0.000, 1.000],  loss: 0.001519, mae: 0.619282, mean_q: 1.230809, mean_eps: 2.000000\n",
            "  417/10000: episode: 18, duration: 0.126s, episode steps:  18, steps per second: 143, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 0.001592, mae: 0.623059, mean_q: 1.245618, mean_eps: 2.000000\n",
            "  470/10000: episode: 19, duration: 0.355s, episode steps:  53, steps per second: 149, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.566 [0.000, 1.000],  loss: 0.002059, mae: 0.635536, mean_q: 1.263984, mean_eps: 2.000000\n",
            "  480/10000: episode: 20, duration: 0.069s, episode steps:  10, steps per second: 146, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.002512, mae: 0.645026, mean_q: 1.282043, mean_eps: 2.000000\n",
            "  506/10000: episode: 21, duration: 0.166s, episode steps:  26, steps per second: 156, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.001917, mae: 0.645786, mean_q: 1.280216, mean_eps: 2.000000\n",
            "  527/10000: episode: 22, duration: 0.140s, episode steps:  21, steps per second: 150, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.003118, mae: 0.653557, mean_q: 1.290187, mean_eps: 2.000000\n",
            "  546/10000: episode: 23, duration: 0.125s, episode steps:  19, steps per second: 153, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.368 [0.000, 1.000],  loss: 0.002442, mae: 0.656421, mean_q: 1.301848, mean_eps: 2.000000\n",
            "  558/10000: episode: 24, duration: 0.081s, episode steps:  12, steps per second: 149, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.001623, mae: 0.660868, mean_q: 1.314559, mean_eps: 2.000000\n",
            "  579/10000: episode: 25, duration: 0.140s, episode steps:  21, steps per second: 150, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 0.002936, mae: 0.666047, mean_q: 1.321679, mean_eps: 2.000000\n",
            "  598/10000: episode: 26, duration: 0.131s, episode steps:  19, steps per second: 145, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.368 [0.000, 1.000],  loss: 0.003778, mae: 0.676997, mean_q: 1.330333, mean_eps: 2.000000\n",
            "  614/10000: episode: 27, duration: 0.116s, episode steps:  16, steps per second: 138, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.003939, mae: 0.675183, mean_q: 1.328316, mean_eps: 2.000000\n",
            "  630/10000: episode: 28, duration: 0.105s, episode steps:  16, steps per second: 153, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.003929, mae: 0.683173, mean_q: 1.345248, mean_eps: 2.000000\n",
            "  647/10000: episode: 29, duration: 0.118s, episode steps:  17, steps per second: 144, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.706 [0.000, 1.000],  loss: 0.002774, mae: 0.688311, mean_q: 1.364354, mean_eps: 2.000000\n",
            "  664/10000: episode: 30, duration: 0.114s, episode steps:  17, steps per second: 150, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 0.003394, mae: 0.687512, mean_q: 1.357700, mean_eps: 2.000000\n",
            "  678/10000: episode: 31, duration: 0.107s, episode steps:  14, steps per second: 130, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 0.003908, mae: 0.698386, mean_q: 1.373325, mean_eps: 2.000000\n",
            "  696/10000: episode: 32, duration: 0.127s, episode steps:  18, steps per second: 141, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.004035, mae: 0.700285, mean_q: 1.376496, mean_eps: 2.000000\n",
            "  706/10000: episode: 33, duration: 0.084s, episode steps:  10, steps per second: 120, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.004498, mae: 0.713944, mean_q: 1.399867, mean_eps: 2.000000\n",
            "  722/10000: episode: 34, duration: 0.107s, episode steps:  16, steps per second: 149, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 0.004476, mae: 0.707759, mean_q: 1.390625, mean_eps: 2.000000\n",
            "  746/10000: episode: 35, duration: 0.164s, episode steps:  24, steps per second: 146, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 0.003493, mae: 0.713384, mean_q: 1.401633, mean_eps: 2.000000\n",
            "  772/10000: episode: 36, duration: 0.172s, episode steps:  26, steps per second: 151, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 0.004268, mae: 0.722843, mean_q: 1.417400, mean_eps: 2.000000\n",
            "  796/10000: episode: 37, duration: 0.157s, episode steps:  24, steps per second: 153, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 0.004800, mae: 0.726729, mean_q: 1.421608, mean_eps: 2.000000\n",
            "  813/10000: episode: 38, duration: 0.119s, episode steps:  17, steps per second: 142, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 0.004295, mae: 0.732694, mean_q: 1.435657, mean_eps: 2.000000\n",
            "  829/10000: episode: 39, duration: 0.109s, episode steps:  16, steps per second: 147, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.004832, mae: 0.737440, mean_q: 1.444317, mean_eps: 2.000000\n",
            "  843/10000: episode: 40, duration: 0.095s, episode steps:  14, steps per second: 148, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 0.004310, mae: 0.741103, mean_q: 1.451919, mean_eps: 2.000000\n",
            "  853/10000: episode: 41, duration: 0.069s, episode steps:  10, steps per second: 144, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.004515, mae: 0.743967, mean_q: 1.456044, mean_eps: 2.000000\n",
            "  863/10000: episode: 42, duration: 0.073s, episode steps:  10, steps per second: 138, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.005364, mae: 0.758727, mean_q: 1.483114, mean_eps: 2.000000\n",
            "  886/10000: episode: 43, duration: 0.162s, episode steps:  23, steps per second: 142, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  loss: 0.004875, mae: 0.757409, mean_q: 1.483799, mean_eps: 2.000000\n",
            "  908/10000: episode: 44, duration: 0.155s, episode steps:  22, steps per second: 142, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.007694, mae: 0.768521, mean_q: 1.483768, mean_eps: 2.000000\n",
            "  929/10000: episode: 45, duration: 0.142s, episode steps:  21, steps per second: 148, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.005829, mae: 0.776315, mean_q: 1.505576, mean_eps: 2.000000\n",
            "  949/10000: episode: 46, duration: 0.157s, episode steps:  20, steps per second: 128, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.007280, mae: 0.780629, mean_q: 1.519003, mean_eps: 2.000000\n",
            "  966/10000: episode: 47, duration: 0.117s, episode steps:  17, steps per second: 146, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 0.005923, mae: 0.780294, mean_q: 1.525395, mean_eps: 2.000000\n",
            "  988/10000: episode: 48, duration: 0.158s, episode steps:  22, steps per second: 139, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.409 [0.000, 1.000],  loss: 0.005576, mae: 0.782679, mean_q: 1.527011, mean_eps: 2.000000\n",
            "  998/10000: episode: 49, duration: 0.069s, episode steps:  10, steps per second: 144, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.005071, mae: 0.787596, mean_q: 1.539899, mean_eps: 2.000000\n",
            " 1017/10000: episode: 50, duration: 0.126s, episode steps:  19, steps per second: 151, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.368 [0.000, 1.000],  loss: 0.006365, mae: 0.795366, mean_q: 1.552637, mean_eps: 2.000000\n",
            " 1043/10000: episode: 51, duration: 0.182s, episode steps:  26, steps per second: 143, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 0.006276, mae: 0.799301, mean_q: 1.562759, mean_eps: 2.000000\n",
            " 1063/10000: episode: 52, duration: 0.140s, episode steps:  20, steps per second: 143, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.006844, mae: 0.813426, mean_q: 1.583891, mean_eps: 2.000000\n",
            " 1105/10000: episode: 53, duration: 0.273s, episode steps:  42, steps per second: 154, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 0.006364, mae: 0.816394, mean_q: 1.593033, mean_eps: 2.000000\n",
            " 1132/10000: episode: 54, duration: 0.177s, episode steps:  27, steps per second: 152, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 0.005994, mae: 0.821733, mean_q: 1.601968, mean_eps: 2.000000\n",
            " 1148/10000: episode: 55, duration: 0.112s, episode steps:  16, steps per second: 143, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 0.004789, mae: 0.830954, mean_q: 1.632304, mean_eps: 2.000000\n",
            " 1167/10000: episode: 56, duration: 0.131s, episode steps:  19, steps per second: 145, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 0.008471, mae: 0.844937, mean_q: 1.630447, mean_eps: 2.000000\n",
            " 1181/10000: episode: 57, duration: 0.104s, episode steps:  14, steps per second: 135, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.214 [0.000, 1.000],  loss: 0.007427, mae: 0.850119, mean_q: 1.647921, mean_eps: 2.000000\n",
            " 1197/10000: episode: 58, duration: 0.109s, episode steps:  16, steps per second: 147, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 0.005549, mae: 0.846712, mean_q: 1.661065, mean_eps: 2.000000\n",
            " 1208/10000: episode: 59, duration: 0.082s, episode steps:  11, steps per second: 134, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.010224, mae: 0.858101, mean_q: 1.679612, mean_eps: 2.000000\n",
            " 1218/10000: episode: 60, duration: 0.070s, episode steps:  10, steps per second: 143, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.005791, mae: 0.861831, mean_q: 1.686675, mean_eps: 2.000000\n",
            " 1239/10000: episode: 61, duration: 0.143s, episode steps:  21, steps per second: 147, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.005964, mae: 0.860976, mean_q: 1.689825, mean_eps: 2.000000\n",
            " 1271/10000: episode: 62, duration: 0.211s, episode steps:  32, steps per second: 152, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 0.007159, mae: 0.868034, mean_q: 1.697258, mean_eps: 2.000000\n",
            " 1307/10000: episode: 63, duration: 0.241s, episode steps:  36, steps per second: 149, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 0.007481, mae: 0.881588, mean_q: 1.724638, mean_eps: 2.000000\n",
            " 1349/10000: episode: 64, duration: 0.287s, episode steps:  42, steps per second: 146, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.006424, mae: 0.891000, mean_q: 1.745654, mean_eps: 2.000000\n",
            " 1366/10000: episode: 65, duration: 0.115s, episode steps:  17, steps per second: 147, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 0.006305, mae: 0.899106, mean_q: 1.768752, mean_eps: 2.000000\n",
            " 1380/10000: episode: 66, duration: 0.096s, episode steps:  14, steps per second: 146, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 0.009344, mae: 0.906287, mean_q: 1.759445, mean_eps: 2.000000\n",
            " 1410/10000: episode: 67, duration: 0.260s, episode steps:  30, steps per second: 115, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.008368, mae: 0.918175, mean_q: 1.785136, mean_eps: 2.000000\n",
            " 1441/10000: episode: 68, duration: 0.292s, episode steps:  31, steps per second: 106, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  loss: 0.008311, mae: 0.924357, mean_q: 1.806658, mean_eps: 2.000000\n",
            " 1466/10000: episode: 69, duration: 0.248s, episode steps:  25, steps per second: 101, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 0.005774, mae: 0.931967, mean_q: 1.835122, mean_eps: 2.000000\n",
            " 1479/10000: episode: 70, duration: 0.144s, episode steps:  13, steps per second:  91, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.007643, mae: 0.940889, mean_q: 1.845769, mean_eps: 2.000000\n",
            " 1497/10000: episode: 71, duration: 0.178s, episode steps:  18, steps per second: 101, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.007981, mae: 0.939309, mean_q: 1.842268, mean_eps: 2.000000\n",
            " 1506/10000: episode: 72, duration: 0.088s, episode steps:   9, steps per second: 102, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.010731, mae: 0.958265, mean_q: 1.863802, mean_eps: 2.000000\n",
            " 1522/10000: episode: 73, duration: 0.166s, episode steps:  16, steps per second:  97, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 0.006848, mae: 0.957067, mean_q: 1.878328, mean_eps: 2.000000\n",
            " 1596/10000: episode: 74, duration: 0.722s, episode steps:  74, steps per second: 103, episode reward: 74.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  loss: 0.007726, mae: 0.966818, mean_q: 1.900911, mean_eps: 2.000000\n",
            " 1608/10000: episode: 75, duration: 0.119s, episode steps:  12, steps per second: 101, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.008659, mae: 0.984455, mean_q: 1.937917, mean_eps: 2.000000\n",
            " 1618/10000: episode: 76, duration: 0.103s, episode steps:  10, steps per second:  97, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.008798, mae: 1.004977, mean_q: 1.960826, mean_eps: 2.000000\n",
            " 1648/10000: episode: 77, duration: 0.286s, episode steps:  30, steps per second: 105, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.007431, mae: 0.999962, mean_q: 1.967264, mean_eps: 2.000000\n",
            " 1666/10000: episode: 78, duration: 0.136s, episode steps:  18, steps per second: 132, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 0.007184, mae: 1.005734, mean_q: 1.984283, mean_eps: 2.000000\n",
            " 1680/10000: episode: 79, duration: 0.104s, episode steps:  14, steps per second: 134, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.786 [0.000, 1.000],  loss: 0.006805, mae: 1.018847, mean_q: 2.011019, mean_eps: 2.000000\n",
            " 1688/10000: episode: 80, duration: 0.057s, episode steps:   8, steps per second: 142, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.125 [0.000, 1.000],  loss: 0.008252, mae: 1.016043, mean_q: 2.006148, mean_eps: 2.000000\n",
            " 1705/10000: episode: 81, duration: 0.116s, episode steps:  17, steps per second: 146, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.706 [0.000, 1.000],  loss: 0.008944, mae: 1.021810, mean_q: 2.012561, mean_eps: 2.000000\n",
            " 1727/10000: episode: 82, duration: 0.144s, episode steps:  22, steps per second: 153, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.318 [0.000, 1.000],  loss: 0.008786, mae: 1.026387, mean_q: 2.016898, mean_eps: 2.000000\n",
            " 1754/10000: episode: 83, duration: 0.179s, episode steps:  27, steps per second: 151, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.593 [0.000, 1.000],  loss: 0.007548, mae: 1.044180, mean_q: 2.054717, mean_eps: 2.000000\n",
            " 1767/10000: episode: 84, duration: 0.091s, episode steps:  13, steps per second: 143, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: 0.008027, mae: 1.044834, mean_q: 2.067869, mean_eps: 2.000000\n",
            " 1776/10000: episode: 85, duration: 0.069s, episode steps:   9, steps per second: 131, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.011772, mae: 1.048582, mean_q: 2.061362, mean_eps: 2.000000\n",
            " 1797/10000: episode: 86, duration: 0.139s, episode steps:  21, steps per second: 151, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 0.005231, mae: 1.060683, mean_q: 2.100557, mean_eps: 2.000000\n",
            " 1831/10000: episode: 87, duration: 0.239s, episode steps:  34, steps per second: 142, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.007563, mae: 1.068370, mean_q: 2.109434, mean_eps: 2.000000\n",
            " 1855/10000: episode: 88, duration: 0.155s, episode steps:  24, steps per second: 154, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 0.011529, mae: 1.080788, mean_q: 2.110420, mean_eps: 2.000000\n",
            " 1876/10000: episode: 89, duration: 0.148s, episode steps:  21, steps per second: 142, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 0.011422, mae: 1.080750, mean_q: 2.128612, mean_eps: 2.000000\n",
            " 1910/10000: episode: 90, duration: 0.230s, episode steps:  34, steps per second: 148, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 0.008867, mae: 1.090574, mean_q: 2.154831, mean_eps: 2.000000\n",
            " 1924/10000: episode: 91, duration: 0.093s, episode steps:  14, steps per second: 150, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.008226, mae: 1.097213, mean_q: 2.170628, mean_eps: 2.000000\n",
            " 1947/10000: episode: 92, duration: 0.159s, episode steps:  23, steps per second: 144, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  loss: 0.010614, mae: 1.118431, mean_q: 2.205902, mean_eps: 2.000000\n",
            " 1988/10000: episode: 93, duration: 0.275s, episode steps:  41, steps per second: 149, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 0.012119, mae: 1.129037, mean_q: 2.216590, mean_eps: 2.000000\n",
            " 2007/10000: episode: 94, duration: 0.129s, episode steps:  19, steps per second: 148, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.684 [0.000, 1.000],  loss: 0.011586, mae: 1.136569, mean_q: 2.248726, mean_eps: 2.000000\n",
            " 2024/10000: episode: 95, duration: 0.118s, episode steps:  17, steps per second: 144, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 0.007376, mae: 1.145195, mean_q: 2.271318, mean_eps: 2.000000\n",
            " 2079/10000: episode: 96, duration: 0.358s, episode steps:  55, steps per second: 154, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 0.011533, mae: 1.155029, mean_q: 2.272455, mean_eps: 2.000000\n",
            " 2104/10000: episode: 97, duration: 0.172s, episode steps:  25, steps per second: 145, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  loss: 0.007239, mae: 1.171184, mean_q: 2.326784, mean_eps: 2.000000\n",
            " 2120/10000: episode: 98, duration: 0.120s, episode steps:  16, steps per second: 134, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  loss: 0.013698, mae: 1.172152, mean_q: 2.321194, mean_eps: 2.000000\n",
            " 2138/10000: episode: 99, duration: 0.125s, episode steps:  18, steps per second: 144, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.722 [0.000, 1.000],  loss: 0.010050, mae: 1.173767, mean_q: 2.328775, mean_eps: 2.000000\n",
            " 2156/10000: episode: 100, duration: 0.122s, episode steps:  18, steps per second: 148, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.389 [0.000, 1.000],  loss: 0.009513, mae: 1.194898, mean_q: 2.364639, mean_eps: 2.000000\n",
            " 2177/10000: episode: 101, duration: 0.137s, episode steps:  21, steps per second: 153, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.007925, mae: 1.204969, mean_q: 2.384359, mean_eps: 2.000000\n",
            " 2201/10000: episode: 102, duration: 0.167s, episode steps:  24, steps per second: 144, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 0.010330, mae: 1.214755, mean_q: 2.402637, mean_eps: 2.000000\n",
            " 2227/10000: episode: 103, duration: 0.170s, episode steps:  26, steps per second: 153, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.346 [0.000, 1.000],  loss: 0.012623, mae: 1.214064, mean_q: 2.402458, mean_eps: 2.000000\n",
            " 2244/10000: episode: 104, duration: 0.120s, episode steps:  17, steps per second: 142, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.294 [0.000, 1.000],  loss: 0.010710, mae: 1.230240, mean_q: 2.428532, mean_eps: 2.000000\n",
            " 2256/10000: episode: 105, duration: 0.081s, episode steps:  12, steps per second: 148, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.006330, mae: 1.247559, mean_q: 2.478052, mean_eps: 2.000000\n",
            " 2292/10000: episode: 106, duration: 0.247s, episode steps:  36, steps per second: 146, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 0.011083, mae: 1.234030, mean_q: 2.437875, mean_eps: 2.000000\n",
            " 2311/10000: episode: 107, duration: 0.130s, episode steps:  19, steps per second: 147, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.316 [0.000, 1.000],  loss: 0.011865, mae: 1.260356, mean_q: 2.482596, mean_eps: 2.000000\n",
            " 2322/10000: episode: 108, duration: 0.099s, episode steps:  11, steps per second: 111, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.011341, mae: 1.258204, mean_q: 2.499865, mean_eps: 2.000000\n",
            " 2346/10000: episode: 109, duration: 0.159s, episode steps:  24, steps per second: 151, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 0.008185, mae: 1.264821, mean_q: 2.503886, mean_eps: 2.000000\n",
            " 2366/10000: episode: 110, duration: 0.135s, episode steps:  20, steps per second: 148, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.011023, mae: 1.274671, mean_q: 2.514123, mean_eps: 2.000000\n",
            " 2379/10000: episode: 111, duration: 0.086s, episode steps:  13, steps per second: 151, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.154 [0.000, 1.000],  loss: 0.007656, mae: 1.279676, mean_q: 2.534088, mean_eps: 2.000000\n",
            " 2395/10000: episode: 112, duration: 0.108s, episode steps:  16, steps per second: 149, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.011277, mae: 1.282439, mean_q: 2.530103, mean_eps: 2.000000\n",
            " 2421/10000: episode: 113, duration: 0.183s, episode steps:  26, steps per second: 142, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.008751, mae: 1.296665, mean_q: 2.570590, mean_eps: 2.000000\n",
            " 2449/10000: episode: 114, duration: 0.186s, episode steps:  28, steps per second: 151, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.013063, mae: 1.312616, mean_q: 2.588007, mean_eps: 2.000000\n",
            " 2462/10000: episode: 115, duration: 0.090s, episode steps:  13, steps per second: 144, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 0.010532, mae: 1.325107, mean_q: 2.612396, mean_eps: 2.000000\n",
            " 2490/10000: episode: 116, duration: 0.186s, episode steps:  28, steps per second: 150, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.393 [0.000, 1.000],  loss: 0.015775, mae: 1.337363, mean_q: 2.626236, mean_eps: 2.000000\n",
            " 2513/10000: episode: 117, duration: 0.151s, episode steps:  23, steps per second: 153, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 0.010427, mae: 1.343618, mean_q: 2.658229, mean_eps: 2.000000\n",
            " 2541/10000: episode: 118, duration: 0.182s, episode steps:  28, steps per second: 154, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 0.011948, mae: 1.339808, mean_q: 2.644070, mean_eps: 2.000000\n",
            " 2564/10000: episode: 119, duration: 0.178s, episode steps:  23, steps per second: 129, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.391 [0.000, 1.000],  loss: 0.016280, mae: 1.355359, mean_q: 2.662315, mean_eps: 2.000000\n",
            " 2573/10000: episode: 120, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.010614, mae: 1.360388, mean_q: 2.702821, mean_eps: 2.000000\n",
            " 2589/10000: episode: 121, duration: 0.106s, episode steps:  16, steps per second: 151, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 0.011782, mae: 1.372772, mean_q: 2.712439, mean_eps: 2.000000\n",
            " 2606/10000: episode: 122, duration: 0.113s, episode steps:  17, steps per second: 150, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 0.012841, mae: 1.370170, mean_q: 2.701666, mean_eps: 2.000000\n",
            " 2631/10000: episode: 123, duration: 0.165s, episode steps:  25, steps per second: 152, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 0.013460, mae: 1.375551, mean_q: 2.724050, mean_eps: 2.000000\n",
            " 2674/10000: episode: 124, duration: 0.274s, episode steps:  43, steps per second: 157, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 0.013650, mae: 1.392736, mean_q: 2.746740, mean_eps: 2.000000\n",
            " 2694/10000: episode: 125, duration: 0.135s, episode steps:  20, steps per second: 148, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.011737, mae: 1.415084, mean_q: 2.805438, mean_eps: 2.000000\n",
            " 2709/10000: episode: 126, duration: 0.113s, episode steps:  15, steps per second: 133, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.018025, mae: 1.440218, mean_q: 2.817207, mean_eps: 2.000000\n",
            " 2723/10000: episode: 127, duration: 0.111s, episode steps:  14, steps per second: 126, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.015220, mae: 1.416881, mean_q: 2.790624, mean_eps: 2.000000\n",
            " 2739/10000: episode: 128, duration: 0.116s, episode steps:  16, steps per second: 138, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.312 [0.000, 1.000],  loss: 0.016951, mae: 1.425856, mean_q: 2.809252, mean_eps: 2.000000\n",
            " 2764/10000: episode: 129, duration: 0.169s, episode steps:  25, steps per second: 148, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.360 [0.000, 1.000],  loss: 0.018137, mae: 1.445579, mean_q: 2.855809, mean_eps: 2.000000\n",
            " 2798/10000: episode: 130, duration: 0.226s, episode steps:  34, steps per second: 150, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.676 [0.000, 1.000],  loss: 0.012584, mae: 1.453288, mean_q: 2.872049, mean_eps: 2.000000\n",
            " 2814/10000: episode: 131, duration: 0.104s, episode steps:  16, steps per second: 154, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 0.018638, mae: 1.455360, mean_q: 2.866409, mean_eps: 2.000000\n",
            " 2829/10000: episode: 132, duration: 0.105s, episode steps:  15, steps per second: 143, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.733 [0.000, 1.000],  loss: 0.012991, mae: 1.451360, mean_q: 2.870729, mean_eps: 2.000000\n",
            " 2851/10000: episode: 133, duration: 0.145s, episode steps:  22, steps per second: 152, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.591 [0.000, 1.000],  loss: 0.013630, mae: 1.479505, mean_q: 2.926326, mean_eps: 2.000000\n",
            " 2878/10000: episode: 134, duration: 0.204s, episode steps:  27, steps per second: 132, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.630 [0.000, 1.000],  loss: 0.010554, mae: 1.479840, mean_q: 2.931806, mean_eps: 2.000000\n",
            " 2900/10000: episode: 135, duration: 0.143s, episode steps:  22, steps per second: 154, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 0.009763, mae: 1.484614, mean_q: 2.951769, mean_eps: 2.000000\n",
            " 2926/10000: episode: 136, duration: 0.167s, episode steps:  26, steps per second: 155, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.577 [0.000, 1.000],  loss: 0.009050, mae: 1.500467, mean_q: 2.977915, mean_eps: 2.000000\n",
            " 2945/10000: episode: 137, duration: 0.123s, episode steps:  19, steps per second: 154, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.632 [0.000, 1.000],  loss: 0.015006, mae: 1.497823, mean_q: 2.973007, mean_eps: 2.000000\n",
            " 2977/10000: episode: 138, duration: 0.217s, episode steps:  32, steps per second: 147, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.007228, mae: 1.525424, mean_q: 3.042171, mean_eps: 2.000000\n",
            " 2999/10000: episode: 139, duration: 0.143s, episode steps:  22, steps per second: 154, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.409 [0.000, 1.000],  loss: 0.012963, mae: 1.541256, mean_q: 3.058280, mean_eps: 2.000000\n",
            " 3011/10000: episode: 140, duration: 0.090s, episode steps:  12, steps per second: 133, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.020030, mae: 1.532351, mean_q: 3.007269, mean_eps: 2.000000\n",
            " 3060/10000: episode: 141, duration: 0.339s, episode steps:  49, steps per second: 145, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 0.018303, mae: 1.550874, mean_q: 3.044345, mean_eps: 2.000000\n",
            " 3083/10000: episode: 142, duration: 0.149s, episode steps:  23, steps per second: 155, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  loss: 0.012050, mae: 1.585086, mean_q: 3.139397, mean_eps: 2.000000\n",
            " 3101/10000: episode: 143, duration: 0.120s, episode steps:  18, steps per second: 149, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 0.014347, mae: 1.586707, mean_q: 3.139244, mean_eps: 2.000000\n",
            " 3123/10000: episode: 144, duration: 0.192s, episode steps:  22, steps per second: 115, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 0.014107, mae: 1.596919, mean_q: 3.163791, mean_eps: 2.000000\n",
            " 3143/10000: episode: 145, duration: 0.194s, episode steps:  20, steps per second: 103, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 0.016970, mae: 1.594224, mean_q: 3.144052, mean_eps: 2.000000\n",
            " 3198/10000: episode: 146, duration: 0.534s, episode steps:  55, steps per second: 103, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  loss: 0.011064, mae: 1.612874, mean_q: 3.213637, mean_eps: 2.000000\n",
            " 3225/10000: episode: 147, duration: 0.259s, episode steps:  27, steps per second: 104, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 0.014601, mae: 1.623015, mean_q: 3.220388, mean_eps: 2.000000\n",
            " 3242/10000: episode: 148, duration: 0.163s, episode steps:  17, steps per second: 104, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 0.016657, mae: 1.638905, mean_q: 3.246068, mean_eps: 2.000000\n",
            " 3268/10000: episode: 149, duration: 0.266s, episode steps:  26, steps per second:  98, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 0.013969, mae: 1.633018, mean_q: 3.247351, mean_eps: 2.000000\n",
            " 3302/10000: episode: 150, duration: 0.320s, episode steps:  34, steps per second: 106, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 0.014561, mae: 1.656588, mean_q: 3.284757, mean_eps: 2.000000\n",
            " 3351/10000: episode: 151, duration: 0.472s, episode steps:  49, steps per second: 104, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 0.012351, mae: 1.693143, mean_q: 3.347354, mean_eps: 2.000000\n",
            " 3393/10000: episode: 152, duration: 0.391s, episode steps:  42, steps per second: 107, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 0.014618, mae: 1.694016, mean_q: 3.359434, mean_eps: 2.000000\n",
            " 3418/10000: episode: 153, duration: 0.163s, episode steps:  25, steps per second: 154, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.012194, mae: 1.723790, mean_q: 3.414311, mean_eps: 2.000000\n",
            " 3434/10000: episode: 154, duration: 0.109s, episode steps:  16, steps per second: 147, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 0.008908, mae: 1.701727, mean_q: 3.393347, mean_eps: 2.000000\n",
            " 3448/10000: episode: 155, duration: 0.102s, episode steps:  14, steps per second: 137, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.020775, mae: 1.721216, mean_q: 3.400214, mean_eps: 2.000000\n",
            " 3475/10000: episode: 156, duration: 0.175s, episode steps:  27, steps per second: 155, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 0.009652, mae: 1.739959, mean_q: 3.466176, mean_eps: 2.000000\n",
            " 3493/10000: episode: 157, duration: 0.117s, episode steps:  18, steps per second: 153, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.389 [0.000, 1.000],  loss: 0.010465, mae: 1.755996, mean_q: 3.494913, mean_eps: 2.000000\n",
            " 3508/10000: episode: 158, duration: 0.109s, episode steps:  15, steps per second: 138, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.007939, mae: 1.751569, mean_q: 3.499314, mean_eps: 2.000000\n",
            " 3516/10000: episode: 159, duration: 0.057s, episode steps:   8, steps per second: 140, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.013598, mae: 1.702090, mean_q: 3.370433, mean_eps: 2.000000\n",
            " 3530/10000: episode: 160, duration: 0.101s, episode steps:  14, steps per second: 138, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 0.011071, mae: 1.787833, mean_q: 3.560120, mean_eps: 2.000000\n",
            " 3563/10000: episode: 161, duration: 0.213s, episode steps:  33, steps per second: 155, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.010394, mae: 1.769657, mean_q: 3.521827, mean_eps: 2.000000\n",
            " 3578/10000: episode: 162, duration: 0.099s, episode steps:  15, steps per second: 152, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.010465, mae: 1.770279, mean_q: 3.529003, mean_eps: 2.000000\n",
            " 3611/10000: episode: 163, duration: 0.224s, episode steps:  33, steps per second: 148, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 0.011055, mae: 1.788453, mean_q: 3.563152, mean_eps: 2.000000\n",
            " 3630/10000: episode: 164, duration: 0.124s, episode steps:  19, steps per second: 154, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 0.009233, mae: 1.805324, mean_q: 3.606849, mean_eps: 2.000000\n",
            " 3642/10000: episode: 165, duration: 0.082s, episode steps:  12, steps per second: 146, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.010405, mae: 1.802375, mean_q: 3.602471, mean_eps: 2.000000\n",
            " 3658/10000: episode: 166, duration: 0.116s, episode steps:  16, steps per second: 138, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 0.014712, mae: 1.824648, mean_q: 3.637460, mean_eps: 2.000000\n",
            " 3670/10000: episode: 167, duration: 0.080s, episode steps:  12, steps per second: 149, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 0.016712, mae: 1.825858, mean_q: 3.640122, mean_eps: 2.000000\n",
            " 3685/10000: episode: 168, duration: 0.107s, episode steps:  15, steps per second: 140, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.020181, mae: 1.840952, mean_q: 3.664701, mean_eps: 2.000000\n",
            " 3696/10000: episode: 169, duration: 0.102s, episode steps:  11, steps per second: 107, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.014276, mae: 1.815336, mean_q: 3.589720, mean_eps: 2.000000\n",
            " 3733/10000: episode: 170, duration: 0.238s, episode steps:  37, steps per second: 155, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 0.014570, mae: 1.839436, mean_q: 3.666043, mean_eps: 2.000000\n",
            " 3746/10000: episode: 171, duration: 0.089s, episode steps:  13, steps per second: 146, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 0.015700, mae: 1.870248, mean_q: 3.726260, mean_eps: 2.000000\n",
            " 3759/10000: episode: 172, duration: 0.092s, episode steps:  13, steps per second: 141, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.014568, mae: 1.859820, mean_q: 3.715454, mean_eps: 2.000000\n",
            " 3780/10000: episode: 173, duration: 0.141s, episode steps:  21, steps per second: 149, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 0.014991, mae: 1.860076, mean_q: 3.711834, mean_eps: 2.000000\n",
            " 3804/10000: episode: 174, duration: 0.161s, episode steps:  24, steps per second: 149, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 0.015352, mae: 1.876184, mean_q: 3.743588, mean_eps: 2.000000\n",
            " 3821/10000: episode: 175, duration: 0.116s, episode steps:  17, steps per second: 146, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.353 [0.000, 1.000],  loss: 0.015907, mae: 1.864641, mean_q: 3.720751, mean_eps: 2.000000\n",
            " 3834/10000: episode: 176, duration: 0.095s, episode steps:  13, steps per second: 137, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.013221, mae: 1.887786, mean_q: 3.781844, mean_eps: 2.000000\n",
            " 3864/10000: episode: 177, duration: 0.197s, episode steps:  30, steps per second: 153, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.018519, mae: 1.904957, mean_q: 3.786648, mean_eps: 2.000000\n",
            " 3894/10000: episode: 178, duration: 0.209s, episode steps:  30, steps per second: 144, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.018779, mae: 1.921969, mean_q: 3.811756, mean_eps: 2.000000\n",
            " 3918/10000: episode: 179, duration: 0.159s, episode steps:  24, steps per second: 151, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 0.015226, mae: 1.920194, mean_q: 3.826809, mean_eps: 2.000000\n",
            " 3931/10000: episode: 180, duration: 0.092s, episode steps:  13, steps per second: 142, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 0.017264, mae: 1.964362, mean_q: 3.899561, mean_eps: 2.000000\n",
            " 3948/10000: episode: 181, duration: 0.116s, episode steps:  17, steps per second: 146, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 0.022795, mae: 1.933350, mean_q: 3.817421, mean_eps: 2.000000\n",
            " 3962/10000: episode: 182, duration: 0.098s, episode steps:  14, steps per second: 143, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.022012, mae: 1.928891, mean_q: 3.843765, mean_eps: 2.000000\n",
            " 3974/10000: episode: 183, duration: 0.080s, episode steps:  12, steps per second: 149, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 0.019669, mae: 1.967914, mean_q: 3.908321, mean_eps: 2.000000\n",
            " 3986/10000: episode: 184, duration: 0.085s, episode steps:  12, steps per second: 141, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.016794, mae: 1.922555, mean_q: 3.838522, mean_eps: 2.000000\n",
            " 4001/10000: episode: 185, duration: 0.102s, episode steps:  15, steps per second: 147, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.023324, mae: 1.950244, mean_q: 3.892795, mean_eps: 2.000000\n",
            " 4010/10000: episode: 186, duration: 0.061s, episode steps:   9, steps per second: 148, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.027275, mae: 1.969354, mean_q: 3.878144, mean_eps: 2.000000\n",
            " 4036/10000: episode: 187, duration: 0.178s, episode steps:  26, steps per second: 146, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.015473, mae: 1.949267, mean_q: 3.884145, mean_eps: 2.000000\n",
            " 4060/10000: episode: 188, duration: 0.167s, episode steps:  24, steps per second: 144, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 0.018637, mae: 1.948800, mean_q: 3.868155, mean_eps: 2.000000\n",
            " 4085/10000: episode: 189, duration: 0.168s, episode steps:  25, steps per second: 149, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.017032, mae: 1.985239, mean_q: 3.952776, mean_eps: 2.000000\n",
            " 4101/10000: episode: 190, duration: 0.109s, episode steps:  16, steps per second: 147, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 0.010666, mae: 2.005320, mean_q: 4.017479, mean_eps: 2.000000\n",
            " 4116/10000: episode: 191, duration: 0.110s, episode steps:  15, steps per second: 136, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.021948, mae: 1.956326, mean_q: 3.915509, mean_eps: 2.000000\n",
            " 4134/10000: episode: 192, duration: 0.117s, episode steps:  18, steps per second: 154, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.021023, mae: 2.023996, mean_q: 4.014998, mean_eps: 2.000000\n",
            " 4149/10000: episode: 193, duration: 0.102s, episode steps:  15, steps per second: 147, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.016044, mae: 2.009319, mean_q: 4.025186, mean_eps: 2.000000\n",
            " 4168/10000: episode: 194, duration: 0.125s, episode steps:  19, steps per second: 153, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 0.024266, mae: 2.030730, mean_q: 4.053160, mean_eps: 2.000000\n",
            " 4184/10000: episode: 195, duration: 0.115s, episode steps:  16, steps per second: 139, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 0.016877, mae: 2.045333, mean_q: 4.080217, mean_eps: 2.000000\n",
            " 4201/10000: episode: 196, duration: 0.111s, episode steps:  17, steps per second: 153, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 0.019341, mae: 2.008478, mean_q: 4.013271, mean_eps: 2.000000\n",
            " 4223/10000: episode: 197, duration: 0.145s, episode steps:  22, steps per second: 152, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 0.021978, mae: 2.053291, mean_q: 4.097130, mean_eps: 2.000000\n",
            " 4239/10000: episode: 198, duration: 0.103s, episode steps:  16, steps per second: 155, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 0.023960, mae: 2.036194, mean_q: 4.067100, mean_eps: 2.000000\n",
            " 4276/10000: episode: 199, duration: 0.247s, episode steps:  37, steps per second: 150, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.568 [0.000, 1.000],  loss: 0.025442, mae: 2.054373, mean_q: 4.095763, mean_eps: 2.000000\n",
            " 4299/10000: episode: 200, duration: 0.145s, episode steps:  23, steps per second: 158, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 0.016542, mae: 2.041529, mean_q: 4.078769, mean_eps: 2.000000\n",
            " 4309/10000: episode: 201, duration: 0.072s, episode steps:  10, steps per second: 139, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.036658, mae: 2.076191, mean_q: 4.108157, mean_eps: 2.000000\n",
            " 4337/10000: episode: 202, duration: 0.181s, episode steps:  28, steps per second: 155, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.022512, mae: 2.079637, mean_q: 4.146427, mean_eps: 2.000000\n",
            " 4364/10000: episode: 203, duration: 0.180s, episode steps:  27, steps per second: 150, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 0.027278, mae: 2.071738, mean_q: 4.118609, mean_eps: 2.000000\n",
            " 4380/10000: episode: 204, duration: 0.114s, episode steps:  16, steps per second: 141, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.018386, mae: 2.109408, mean_q: 4.212920, mean_eps: 2.000000\n",
            " 4398/10000: episode: 205, duration: 0.140s, episode steps:  18, steps per second: 128, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.722 [0.000, 1.000],  loss: 0.028077, mae: 2.064166, mean_q: 4.124990, mean_eps: 2.000000\n",
            " 4426/10000: episode: 206, duration: 0.189s, episode steps:  28, steps per second: 148, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.024213, mae: 2.106583, mean_q: 4.195048, mean_eps: 2.000000\n",
            " 4445/10000: episode: 207, duration: 0.125s, episode steps:  19, steps per second: 152, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.368 [0.000, 1.000],  loss: 0.030924, mae: 2.087705, mean_q: 4.184996, mean_eps: 2.000000\n",
            " 4466/10000: episode: 208, duration: 0.137s, episode steps:  21, steps per second: 154, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 0.040371, mae: 2.113809, mean_q: 4.218327, mean_eps: 2.000000\n",
            " 4479/10000: episode: 209, duration: 0.091s, episode steps:  13, steps per second: 143, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 0.030415, mae: 2.153557, mean_q: 4.271849, mean_eps: 2.000000\n",
            " 4502/10000: episode: 210, duration: 0.148s, episode steps:  23, steps per second: 155, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 0.021356, mae: 2.122346, mean_q: 4.226599, mean_eps: 2.000000\n",
            " 4514/10000: episode: 211, duration: 0.083s, episode steps:  12, steps per second: 144, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.016899, mae: 2.163936, mean_q: 4.306285, mean_eps: 2.000000\n",
            " 4528/10000: episode: 212, duration: 0.106s, episode steps:  14, steps per second: 133, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.027789, mae: 2.155039, mean_q: 4.282060, mean_eps: 2.000000\n",
            " 4545/10000: episode: 213, duration: 0.131s, episode steps:  17, steps per second: 130, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 0.023208, mae: 2.122243, mean_q: 4.219126, mean_eps: 2.000000\n",
            " 4562/10000: episode: 214, duration: 0.120s, episode steps:  17, steps per second: 142, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 0.021538, mae: 2.145583, mean_q: 4.272276, mean_eps: 2.000000\n",
            " 4575/10000: episode: 215, duration: 0.100s, episode steps:  13, steps per second: 130, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.017771, mae: 2.141672, mean_q: 4.287114, mean_eps: 2.000000\n",
            " 4588/10000: episode: 216, duration: 0.095s, episode steps:  13, steps per second: 137, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.028498, mae: 2.131209, mean_q: 4.255222, mean_eps: 2.000000\n",
            " 4600/10000: episode: 217, duration: 0.085s, episode steps:  12, steps per second: 141, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.041942, mae: 2.196556, mean_q: 4.371467, mean_eps: 2.000000\n",
            " 4619/10000: episode: 218, duration: 0.146s, episode steps:  19, steps per second: 130, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.368 [0.000, 1.000],  loss: 0.026197, mae: 2.172724, mean_q: 4.339107, mean_eps: 2.000000\n",
            " 4631/10000: episode: 219, duration: 0.087s, episode steps:  12, steps per second: 138, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.016924, mae: 2.165982, mean_q: 4.328060, mean_eps: 2.000000\n",
            " 4660/10000: episode: 220, duration: 0.193s, episode steps:  29, steps per second: 150, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 0.018771, mae: 2.187965, mean_q: 4.382698, mean_eps: 2.000000\n",
            " 4710/10000: episode: 221, duration: 0.326s, episode steps:  50, steps per second: 153, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.020352, mae: 2.196602, mean_q: 4.390762, mean_eps: 2.000000\n",
            " 4722/10000: episode: 222, duration: 0.079s, episode steps:  12, steps per second: 152, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.031985, mae: 2.244313, mean_q: 4.478707, mean_eps: 2.000000\n",
            " 4738/10000: episode: 223, duration: 0.109s, episode steps:  16, steps per second: 147, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.312 [0.000, 1.000],  loss: 0.027994, mae: 2.225795, mean_q: 4.458282, mean_eps: 2.000000\n",
            " 4751/10000: episode: 224, duration: 0.092s, episode steps:  13, steps per second: 141, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 0.035254, mae: 2.217012, mean_q: 4.411009, mean_eps: 2.000000\n",
            " 4764/10000: episode: 225, duration: 0.091s, episode steps:  13, steps per second: 142, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 0.045638, mae: 2.208000, mean_q: 4.373103, mean_eps: 2.000000\n",
            " 4775/10000: episode: 226, duration: 0.073s, episode steps:  11, steps per second: 150, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.028833, mae: 2.253205, mean_q: 4.475729, mean_eps: 2.000000\n",
            " 4792/10000: episode: 227, duration: 0.113s, episode steps:  17, steps per second: 151, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 0.028217, mae: 2.261156, mean_q: 4.506356, mean_eps: 2.000000\n",
            " 4802/10000: episode: 228, duration: 0.069s, episode steps:  10, steps per second: 145, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.013313, mae: 2.244802, mean_q: 4.506457, mean_eps: 2.000000\n",
            " 4818/10000: episode: 229, duration: 0.114s, episode steps:  16, steps per second: 140, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 0.018490, mae: 2.248656, mean_q: 4.520438, mean_eps: 2.000000\n",
            " 4837/10000: episode: 230, duration: 0.159s, episode steps:  19, steps per second: 120, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 0.026515, mae: 2.267629, mean_q: 4.540028, mean_eps: 2.000000\n",
            " 4860/10000: episode: 231, duration: 0.227s, episode steps:  23, steps per second: 101, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.391 [0.000, 1.000],  loss: 0.023228, mae: 2.249992, mean_q: 4.503791, mean_eps: 2.000000\n",
            " 4901/10000: episode: 232, duration: 0.383s, episode steps:  41, steps per second: 107, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.439 [0.000, 1.000],  loss: 0.024397, mae: 2.280825, mean_q: 4.565197, mean_eps: 2.000000\n",
            " 4916/10000: episode: 233, duration: 0.144s, episode steps:  15, steps per second: 104, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.733 [0.000, 1.000],  loss: 0.024641, mae: 2.295233, mean_q: 4.594719, mean_eps: 2.000000\n",
            " 4941/10000: episode: 234, duration: 0.243s, episode steps:  25, steps per second: 103, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.640 [0.000, 1.000],  loss: 0.017423, mae: 2.298816, mean_q: 4.617901, mean_eps: 2.000000\n",
            " 4970/10000: episode: 235, duration: 0.288s, episode steps:  29, steps per second: 101, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 0.023111, mae: 2.293572, mean_q: 4.604072, mean_eps: 2.000000\n",
            " 4979/10000: episode: 236, duration: 0.092s, episode steps:   9, steps per second:  98, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.019611, mae: 2.333608, mean_q: 4.682953, mean_eps: 2.000000\n",
            " 4991/10000: episode: 237, duration: 0.121s, episode steps:  12, steps per second:  99, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.019993, mae: 2.317514, mean_q: 4.675613, mean_eps: 2.000000\n",
            " 5010/10000: episode: 238, duration: 0.181s, episode steps:  19, steps per second: 105, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.263 [0.000, 1.000],  loss: 0.025847, mae: 2.341616, mean_q: 4.692530, mean_eps: 2.000000\n",
            " 5034/10000: episode: 239, duration: 0.248s, episode steps:  24, steps per second:  97, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 0.028046, mae: 2.358285, mean_q: 4.681585, mean_eps: 2.000000\n",
            " 5092/10000: episode: 240, duration: 0.561s, episode steps:  58, steps per second: 103, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.569 [0.000, 1.000],  loss: 0.022686, mae: 2.363284, mean_q: 4.722537, mean_eps: 2.000000\n",
            " 5109/10000: episode: 241, duration: 0.140s, episode steps:  17, steps per second: 122, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 0.015205, mae: 2.378890, mean_q: 4.771730, mean_eps: 2.000000\n",
            " 5126/10000: episode: 242, duration: 0.115s, episode steps:  17, steps per second: 148, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 0.012245, mae: 2.380640, mean_q: 4.776492, mean_eps: 2.000000\n",
            " 5158/10000: episode: 243, duration: 0.207s, episode steps:  32, steps per second: 155, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 0.018415, mae: 2.403746, mean_q: 4.820976, mean_eps: 2.000000\n",
            " 5201/10000: episode: 244, duration: 0.286s, episode steps:  43, steps per second: 150, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.442 [0.000, 1.000],  loss: 0.018283, mae: 2.424432, mean_q: 4.859857, mean_eps: 2.000000\n",
            " 5274/10000: episode: 245, duration: 0.471s, episode steps:  73, steps per second: 155, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 0.015110, mae: 2.426405, mean_q: 4.873388, mean_eps: 2.000000\n",
            " 5311/10000: episode: 246, duration: 0.239s, episode steps:  37, steps per second: 155, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 0.013777, mae: 2.517062, mean_q: 5.042250, mean_eps: 2.000000\n",
            " 5360/10000: episode: 247, duration: 0.320s, episode steps:  49, steps per second: 153, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.010837, mae: 2.469500, mean_q: 4.952267, mean_eps: 2.000000\n",
            " 5379/10000: episode: 248, duration: 0.129s, episode steps:  19, steps per second: 148, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 0.010156, mae: 2.475908, mean_q: 4.963448, mean_eps: 2.000000\n",
            " 5396/10000: episode: 249, duration: 0.121s, episode steps:  17, steps per second: 141, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 0.011662, mae: 2.506332, mean_q: 5.039523, mean_eps: 2.000000\n",
            " 5417/10000: episode: 250, duration: 0.137s, episode steps:  21, steps per second: 153, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.009091, mae: 2.530469, mean_q: 5.096597, mean_eps: 2.000000\n",
            " 5440/10000: episode: 251, duration: 0.155s, episode steps:  23, steps per second: 148, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.348 [0.000, 1.000],  loss: 0.009580, mae: 2.516370, mean_q: 5.072398, mean_eps: 2.000000\n",
            " 5451/10000: episode: 252, duration: 0.075s, episode steps:  11, steps per second: 147, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.008334, mae: 2.503636, mean_q: 5.018184, mean_eps: 2.000000\n",
            " 5464/10000: episode: 253, duration: 0.094s, episode steps:  13, steps per second: 139, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: 0.022825, mae: 2.511784, mean_q: 5.014518, mean_eps: 2.000000\n",
            " 5485/10000: episode: 254, duration: 0.148s, episode steps:  21, steps per second: 142, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 0.007895, mae: 2.547784, mean_q: 5.113332, mean_eps: 2.000000\n",
            " 5531/10000: episode: 255, duration: 0.307s, episode steps:  46, steps per second: 150, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 0.014551, mae: 2.578069, mean_q: 5.170677, mean_eps: 2.000000\n",
            " 5557/10000: episode: 256, duration: 0.168s, episode steps:  26, steps per second: 154, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 0.013409, mae: 2.617351, mean_q: 5.242262, mean_eps: 2.000000\n",
            " 5584/10000: episode: 257, duration: 0.190s, episode steps:  27, steps per second: 142, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.370 [0.000, 1.000],  loss: 0.017282, mae: 2.615150, mean_q: 5.223114, mean_eps: 2.000000\n",
            " 5594/10000: episode: 258, duration: 0.074s, episode steps:  10, steps per second: 135, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.009339, mae: 2.632082, mean_q: 5.272151, mean_eps: 2.000000\n",
            " 5634/10000: episode: 259, duration: 0.278s, episode steps:  40, steps per second: 144, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 0.011070, mae: 2.625971, mean_q: 5.267631, mean_eps: 2.000000\n",
            " 5668/10000: episode: 260, duration: 0.225s, episode steps:  34, steps per second: 151, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.441 [0.000, 1.000],  loss: 0.011345, mae: 2.662398, mean_q: 5.343243, mean_eps: 2.000000\n",
            " 5728/10000: episode: 261, duration: 0.390s, episode steps:  60, steps per second: 154, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 0.019473, mae: 2.672983, mean_q: 5.358100, mean_eps: 2.000000\n",
            " 5742/10000: episode: 262, duration: 0.095s, episode steps:  14, steps per second: 147, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.009471, mae: 2.673441, mean_q: 5.376509, mean_eps: 2.000000\n",
            " 5772/10000: episode: 263, duration: 0.205s, episode steps:  30, steps per second: 146, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.016778, mae: 2.699251, mean_q: 5.404236, mean_eps: 2.000000\n",
            " 5785/10000: episode: 264, duration: 0.099s, episode steps:  13, steps per second: 132, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 0.010894, mae: 2.747633, mean_q: 5.492479, mean_eps: 2.000000\n",
            " 5808/10000: episode: 265, duration: 0.171s, episode steps:  23, steps per second: 135, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.609 [0.000, 1.000],  loss: 0.016785, mae: 2.718919, mean_q: 5.445739, mean_eps: 2.000000\n",
            " 5824/10000: episode: 266, duration: 0.120s, episode steps:  16, steps per second: 134, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.312 [0.000, 1.000],  loss: 0.011622, mae: 2.762050, mean_q: 5.546518, mean_eps: 2.000000\n",
            " 5843/10000: episode: 267, duration: 0.138s, episode steps:  19, steps per second: 138, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.368 [0.000, 1.000],  loss: 0.015919, mae: 2.747824, mean_q: 5.526108, mean_eps: 2.000000\n",
            " 5865/10000: episode: 268, duration: 0.149s, episode steps:  22, steps per second: 148, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 0.013202, mae: 2.736181, mean_q: 5.510156, mean_eps: 2.000000\n",
            " 5897/10000: episode: 269, duration: 0.221s, episode steps:  32, steps per second: 144, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 0.012214, mae: 2.741409, mean_q: 5.507438, mean_eps: 2.000000\n",
            " 5912/10000: episode: 270, duration: 0.099s, episode steps:  15, steps per second: 151, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.267 [0.000, 1.000],  loss: 0.009442, mae: 2.801471, mean_q: 5.640913, mean_eps: 2.000000\n",
            " 5972/10000: episode: 271, duration: 0.394s, episode steps:  60, steps per second: 152, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 0.014666, mae: 2.790518, mean_q: 5.599698, mean_eps: 2.000000\n",
            " 6006/10000: episode: 272, duration: 0.221s, episode steps:  34, steps per second: 154, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 0.026822, mae: 2.824579, mean_q: 5.651301, mean_eps: 2.000000\n",
            " 6022/10000: episode: 273, duration: 0.108s, episode steps:  16, steps per second: 148, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 0.017747, mae: 2.815873, mean_q: 5.624086, mean_eps: 2.000000\n",
            " 6034/10000: episode: 274, duration: 0.084s, episode steps:  12, steps per second: 143, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.010996, mae: 2.839009, mean_q: 5.717452, mean_eps: 2.000000\n",
            " 6055/10000: episode: 275, duration: 0.143s, episode steps:  21, steps per second: 146, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.009521, mae: 2.837807, mean_q: 5.684557, mean_eps: 2.000000\n",
            " 6074/10000: episode: 276, duration: 0.130s, episode steps:  19, steps per second: 146, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 0.032127, mae: 2.832397, mean_q: 5.671425, mean_eps: 2.000000\n",
            " 6089/10000: episode: 277, duration: 0.109s, episode steps:  15, steps per second: 138, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.019705, mae: 2.837970, mean_q: 5.683405, mean_eps: 2.000000\n",
            " 6117/10000: episode: 278, duration: 0.187s, episode steps:  28, steps per second: 150, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.019461, mae: 2.869174, mean_q: 5.752073, mean_eps: 2.000000\n",
            " 6129/10000: episode: 279, duration: 0.093s, episode steps:  12, steps per second: 129, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 0.019621, mae: 2.861635, mean_q: 5.744544, mean_eps: 2.000000\n",
            " 6150/10000: episode: 280, duration: 0.148s, episode steps:  21, steps per second: 142, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.381 [0.000, 1.000],  loss: 0.017659, mae: 2.840393, mean_q: 5.708865, mean_eps: 2.000000\n",
            " 6167/10000: episode: 281, duration: 0.132s, episode steps:  17, steps per second: 129, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.235 [0.000, 1.000],  loss: 0.032732, mae: 2.910371, mean_q: 5.829831, mean_eps: 2.000000\n",
            " 6236/10000: episode: 282, duration: 0.463s, episode steps:  69, steps per second: 149, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.551 [0.000, 1.000],  loss: 0.020203, mae: 2.896829, mean_q: 5.821564, mean_eps: 2.000000\n",
            " 6249/10000: episode: 283, duration: 0.087s, episode steps:  13, steps per second: 149, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 0.031117, mae: 2.945789, mean_q: 5.902155, mean_eps: 2.000000\n",
            " 6294/10000: episode: 284, duration: 0.294s, episode steps:  45, steps per second: 153, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.015905, mae: 2.923095, mean_q: 5.871091, mean_eps: 2.000000\n",
            " 6309/10000: episode: 285, duration: 0.101s, episode steps:  15, steps per second: 148, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 0.018346, mae: 2.910964, mean_q: 5.876324, mean_eps: 2.000000\n",
            " 6319/10000: episode: 286, duration: 0.068s, episode steps:  10, steps per second: 148, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.022633, mae: 2.858510, mean_q: 5.787849, mean_eps: 2.000000\n",
            " 6329/10000: episode: 287, duration: 0.073s, episode steps:  10, steps per second: 138, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.021262, mae: 2.871481, mean_q: 5.761191, mean_eps: 2.000000\n",
            " 6346/10000: episode: 288, duration: 0.115s, episode steps:  17, steps per second: 148, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.353 [0.000, 1.000],  loss: 0.022392, mae: 2.978808, mean_q: 5.954148, mean_eps: 2.000000\n",
            " 6386/10000: episode: 289, duration: 0.271s, episode steps:  40, steps per second: 147, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.019141, mae: 2.959001, mean_q: 5.969697, mean_eps: 2.000000\n",
            " 6403/10000: episode: 290, duration: 0.121s, episode steps:  17, steps per second: 141, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 0.034012, mae: 2.987359, mean_q: 5.999239, mean_eps: 2.000000\n",
            " 6414/10000: episode: 291, duration: 0.082s, episode steps:  11, steps per second: 135, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.016933, mae: 2.977219, mean_q: 5.999999, mean_eps: 2.000000\n",
            " 6431/10000: episode: 292, duration: 0.120s, episode steps:  17, steps per second: 142, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 0.011062, mae: 2.982676, mean_q: 6.006695, mean_eps: 2.000000\n",
            " 6486/10000: episode: 293, duration: 0.365s, episode steps:  55, steps per second: 151, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.436 [0.000, 1.000],  loss: 0.016973, mae: 2.964312, mean_q: 6.010579, mean_eps: 2.000000\n",
            " 6499/10000: episode: 294, duration: 0.094s, episode steps:  13, steps per second: 138, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 0.015424, mae: 2.958871, mean_q: 5.978227, mean_eps: 2.000000\n",
            " 6515/10000: episode: 295, duration: 0.107s, episode steps:  16, steps per second: 150, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 0.049504, mae: 2.996584, mean_q: 6.037196, mean_eps: 2.000000\n",
            " 6530/10000: episode: 296, duration: 0.119s, episode steps:  15, steps per second: 126, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 0.012314, mae: 3.033257, mean_q: 6.126220, mean_eps: 2.000000\n",
            " 6542/10000: episode: 297, duration: 0.084s, episode steps:  12, steps per second: 142, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.016353, mae: 2.957535, mean_q: 5.992364, mean_eps: 2.000000\n",
            " 6564/10000: episode: 298, duration: 0.194s, episode steps:  22, steps per second: 113, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.409 [0.000, 1.000],  loss: 0.040792, mae: 3.069834, mean_q: 6.186394, mean_eps: 2.000000\n",
            " 6586/10000: episode: 299, duration: 0.209s, episode steps:  22, steps per second: 105, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.409 [0.000, 1.000],  loss: 0.027585, mae: 2.997487, mean_q: 6.059935, mean_eps: 2.000000\n",
            " 6599/10000: episode: 300, duration: 0.140s, episode steps:  13, steps per second:  93, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 0.019810, mae: 3.038174, mean_q: 6.103535, mean_eps: 2.000000\n",
            " 6619/10000: episode: 301, duration: 0.191s, episode steps:  20, steps per second: 105, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 0.030355, mae: 3.039079, mean_q: 6.108906, mean_eps: 2.000000\n",
            " 6628/10000: episode: 302, duration: 0.092s, episode steps:   9, steps per second:  97, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.016417, mae: 3.022122, mean_q: 6.091755, mean_eps: 2.000000\n",
            " 6647/10000: episode: 303, duration: 0.186s, episode steps:  19, steps per second: 102, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 0.021258, mae: 3.048475, mean_q: 6.148337, mean_eps: 2.000000\n",
            " 6677/10000: episode: 304, duration: 0.288s, episode steps:  30, steps per second: 104, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.433 [0.000, 1.000],  loss: 0.023043, mae: 3.006979, mean_q: 6.054882, mean_eps: 2.000000\n",
            " 6716/10000: episode: 305, duration: 0.409s, episode steps:  39, steps per second:  95, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 0.024742, mae: 3.032545, mean_q: 6.113324, mean_eps: 2.000000\n",
            " 6727/10000: episode: 306, duration: 0.120s, episode steps:  11, steps per second:  92, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.024156, mae: 3.059819, mean_q: 6.160446, mean_eps: 2.000000\n",
            " 6750/10000: episode: 307, duration: 0.270s, episode steps:  23, steps per second:  85, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 0.031620, mae: 3.053822, mean_q: 6.147089, mean_eps: 2.000000\n",
            " 6770/10000: episode: 308, duration: 0.211s, episode steps:  20, steps per second:  95, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.027541, mae: 3.074544, mean_q: 6.225766, mean_eps: 2.000000\n",
            " 6792/10000: episode: 309, duration: 0.215s, episode steps:  22, steps per second: 102, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 0.027772, mae: 3.054664, mean_q: 6.166702, mean_eps: 2.000000\n",
            " 6805/10000: episode: 310, duration: 0.129s, episode steps:  13, steps per second: 100, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 0.025664, mae: 3.032564, mean_q: 6.136844, mean_eps: 2.000000\n",
            " 6823/10000: episode: 311, duration: 0.133s, episode steps:  18, steps per second: 135, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.035855, mae: 3.024433, mean_q: 6.129468, mean_eps: 2.000000\n",
            " 6833/10000: episode: 312, duration: 0.071s, episode steps:  10, steps per second: 141, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.021071, mae: 3.060198, mean_q: 6.182769, mean_eps: 2.000000\n",
            " 6850/10000: episode: 313, duration: 0.120s, episode steps:  17, steps per second: 142, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 0.036840, mae: 3.122084, mean_q: 6.314256, mean_eps: 2.000000\n",
            " 6870/10000: episode: 314, duration: 0.138s, episode steps:  20, steps per second: 145, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 0.025923, mae: 3.040719, mean_q: 6.141873, mean_eps: 2.000000\n",
            " 6884/10000: episode: 315, duration: 0.097s, episode steps:  14, steps per second: 144, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.031678, mae: 3.100680, mean_q: 6.302015, mean_eps: 2.000000\n",
            " 6899/10000: episode: 316, duration: 0.110s, episode steps:  15, steps per second: 137, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.733 [0.000, 1.000],  loss: 0.032626, mae: 3.076024, mean_q: 6.217320, mean_eps: 2.000000\n",
            " 6924/10000: episode: 317, duration: 0.163s, episode steps:  25, steps per second: 153, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  loss: 0.031178, mae: 3.142762, mean_q: 6.348877, mean_eps: 2.000000\n",
            " 6938/10000: episode: 318, duration: 0.109s, episode steps:  14, steps per second: 128, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.022279, mae: 3.019575, mean_q: 6.098015, mean_eps: 2.000000\n",
            " 6957/10000: episode: 319, duration: 0.132s, episode steps:  19, steps per second: 144, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 0.023825, mae: 3.168209, mean_q: 6.405479, mean_eps: 2.000000\n",
            " 6971/10000: episode: 320, duration: 0.105s, episode steps:  14, steps per second: 133, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 0.018017, mae: 3.148340, mean_q: 6.368371, mean_eps: 2.000000\n",
            " 6986/10000: episode: 321, duration: 0.119s, episode steps:  15, steps per second: 126, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.018502, mae: 3.128608, mean_q: 6.315309, mean_eps: 2.000000\n",
            " 7022/10000: episode: 322, duration: 0.255s, episode steps:  36, steps per second: 141, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 0.011547, mae: 3.172331, mean_q: 6.457440, mean_eps: 2.000000\n",
            " 7064/10000: episode: 323, duration: 0.273s, episode steps:  42, steps per second: 154, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 0.020866, mae: 3.140219, mean_q: 6.388355, mean_eps: 2.000000\n",
            " 7077/10000: episode: 324, duration: 0.089s, episode steps:  13, steps per second: 145, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 0.012889, mae: 3.214394, mean_q: 6.511477, mean_eps: 2.000000\n",
            " 7091/10000: episode: 325, duration: 0.094s, episode steps:  14, steps per second: 149, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 0.014175, mae: 3.165331, mean_q: 6.437620, mean_eps: 2.000000\n",
            " 7102/10000: episode: 326, duration: 0.081s, episode steps:  11, steps per second: 135, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.022142, mae: 3.210809, mean_q: 6.497561, mean_eps: 2.000000\n",
            " 7116/10000: episode: 327, duration: 0.091s, episode steps:  14, steps per second: 155, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 0.013227, mae: 3.175884, mean_q: 6.446479, mean_eps: 2.000000\n",
            " 7136/10000: episode: 328, duration: 0.144s, episode steps:  20, steps per second: 139, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 0.022777, mae: 3.195107, mean_q: 6.500363, mean_eps: 2.000000\n",
            " 7190/10000: episode: 329, duration: 0.356s, episode steps:  54, steps per second: 152, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 0.014846, mae: 3.199729, mean_q: 6.503043, mean_eps: 2.000000\n",
            " 7209/10000: episode: 330, duration: 0.128s, episode steps:  19, steps per second: 148, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 0.013150, mae: 3.177577, mean_q: 6.479608, mean_eps: 2.000000\n",
            " 7225/10000: episode: 331, duration: 0.120s, episode steps:  16, steps per second: 133, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.011097, mae: 3.196213, mean_q: 6.529305, mean_eps: 2.000000\n",
            " 7243/10000: episode: 332, duration: 0.124s, episode steps:  18, steps per second: 145, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 0.021027, mae: 3.208604, mean_q: 6.546731, mean_eps: 2.000000\n",
            " 7256/10000: episode: 333, duration: 0.090s, episode steps:  13, steps per second: 145, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.231 [0.000, 1.000],  loss: 0.020857, mae: 3.175654, mean_q: 6.403280, mean_eps: 2.000000\n",
            " 7303/10000: episode: 334, duration: 0.314s, episode steps:  47, steps per second: 150, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 0.028321, mae: 3.239438, mean_q: 6.572946, mean_eps: 2.000000\n",
            " 7324/10000: episode: 335, duration: 0.143s, episode steps:  21, steps per second: 146, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.043994, mae: 3.232125, mean_q: 6.546601, mean_eps: 2.000000\n",
            " 7338/10000: episode: 336, duration: 0.096s, episode steps:  14, steps per second: 146, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 0.031957, mae: 3.284100, mean_q: 6.633048, mean_eps: 2.000000\n",
            " 7358/10000: episode: 337, duration: 0.130s, episode steps:  20, steps per second: 154, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.350 [0.000, 1.000],  loss: 0.029296, mae: 3.293777, mean_q: 6.636259, mean_eps: 2.000000\n",
            " 7376/10000: episode: 338, duration: 0.126s, episode steps:  18, steps per second: 143, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.040564, mae: 3.299081, mean_q: 6.711924, mean_eps: 2.000000\n",
            " 7385/10000: episode: 339, duration: 0.061s, episode steps:   9, steps per second: 147, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.037198, mae: 3.271666, mean_q: 6.611336, mean_eps: 2.000000\n",
            " 7397/10000: episode: 340, duration: 0.082s, episode steps:  12, steps per second: 147, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 0.032589, mae: 3.342850, mean_q: 6.753730, mean_eps: 2.000000\n",
            " 7445/10000: episode: 341, duration: 0.334s, episode steps:  48, steps per second: 144, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 0.032988, mae: 3.332533, mean_q: 6.749463, mean_eps: 2.000000\n",
            " 7463/10000: episode: 342, duration: 0.136s, episode steps:  18, steps per second: 132, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.278 [0.000, 1.000],  loss: 0.023260, mae: 3.288516, mean_q: 6.647003, mean_eps: 2.000000\n",
            " 7490/10000: episode: 343, duration: 0.183s, episode steps:  27, steps per second: 148, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 0.017384, mae: 3.365853, mean_q: 6.810297, mean_eps: 2.000000\n",
            " 7510/10000: episode: 344, duration: 0.130s, episode steps:  20, steps per second: 154, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 0.035473, mae: 3.339316, mean_q: 6.778277, mean_eps: 2.000000\n",
            " 7529/10000: episode: 345, duration: 0.136s, episode steps:  19, steps per second: 140, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.737 [0.000, 1.000],  loss: 0.026321, mae: 3.408194, mean_q: 6.867181, mean_eps: 2.000000\n",
            " 7553/10000: episode: 346, duration: 0.163s, episode steps:  24, steps per second: 147, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 0.023192, mae: 3.323932, mean_q: 6.746122, mean_eps: 2.000000\n",
            " 7605/10000: episode: 347, duration: 0.366s, episode steps:  52, steps per second: 142, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.442 [0.000, 1.000],  loss: 0.018740, mae: 3.358621, mean_q: 6.804972, mean_eps: 2.000000\n",
            " 7619/10000: episode: 348, duration: 0.108s, episode steps:  14, steps per second: 130, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 0.041280, mae: 3.382657, mean_q: 6.858915, mean_eps: 2.000000\n",
            " 7633/10000: episode: 349, duration: 0.108s, episode steps:  14, steps per second: 130, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 0.022385, mae: 3.337224, mean_q: 6.782517, mean_eps: 2.000000\n",
            " 7654/10000: episode: 350, duration: 0.147s, episode steps:  21, steps per second: 142, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.619 [0.000, 1.000],  loss: 0.024237, mae: 3.374000, mean_q: 6.901126, mean_eps: 2.000000\n",
            " 7671/10000: episode: 351, duration: 0.120s, episode steps:  17, steps per second: 142, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 0.018537, mae: 3.387832, mean_q: 6.889280, mean_eps: 2.000000\n",
            " 7682/10000: episode: 352, duration: 0.076s, episode steps:  11, steps per second: 145, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.041067, mae: 3.444500, mean_q: 6.954610, mean_eps: 2.000000\n",
            " 7694/10000: episode: 353, duration: 0.080s, episode steps:  12, steps per second: 149, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.025516, mae: 3.433904, mean_q: 6.959201, mean_eps: 2.000000\n",
            " 7722/10000: episode: 354, duration: 0.190s, episode steps:  28, steps per second: 147, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 0.023436, mae: 3.438217, mean_q: 6.923593, mean_eps: 2.000000\n",
            " 7741/10000: episode: 355, duration: 0.129s, episode steps:  19, steps per second: 148, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 0.019289, mae: 3.432212, mean_q: 6.963220, mean_eps: 2.000000\n",
            " 7761/10000: episode: 356, duration: 0.132s, episode steps:  20, steps per second: 152, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.041957, mae: 3.416623, mean_q: 6.912229, mean_eps: 2.000000\n",
            " 7776/10000: episode: 357, duration: 0.098s, episode steps:  15, steps per second: 154, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.023660, mae: 3.522723, mean_q: 7.091098, mean_eps: 2.000000\n",
            " 7789/10000: episode: 358, duration: 0.094s, episode steps:  13, steps per second: 138, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.846 [0.000, 1.000],  loss: 0.018288, mae: 3.451861, mean_q: 7.007097, mean_eps: 2.000000\n",
            " 7798/10000: episode: 359, duration: 0.065s, episode steps:   9, steps per second: 138, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.025013, mae: 3.521013, mean_q: 7.129645, mean_eps: 2.000000\n",
            " 7809/10000: episode: 360, duration: 0.074s, episode steps:  11, steps per second: 149, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.021128, mae: 3.381462, mean_q: 6.846494, mean_eps: 2.000000\n",
            " 7821/10000: episode: 361, duration: 0.083s, episode steps:  12, steps per second: 145, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.035873, mae: 3.439087, mean_q: 6.974903, mean_eps: 2.000000\n",
            " 7854/10000: episode: 362, duration: 0.213s, episode steps:  33, steps per second: 155, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 0.034187, mae: 3.381496, mean_q: 6.874857, mean_eps: 2.000000\n",
            " 7876/10000: episode: 363, duration: 0.150s, episode steps:  22, steps per second: 147, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.039114, mae: 3.518035, mean_q: 7.121544, mean_eps: 2.000000\n",
            " 7903/10000: episode: 364, duration: 0.189s, episode steps:  27, steps per second: 143, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.593 [0.000, 1.000],  loss: 0.024465, mae: 3.475049, mean_q: 7.028622, mean_eps: 2.000000\n",
            " 7922/10000: episode: 365, duration: 0.133s, episode steps:  19, steps per second: 143, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 0.033425, mae: 3.547825, mean_q: 7.150191, mean_eps: 2.000000\n",
            " 7944/10000: episode: 366, duration: 0.154s, episode steps:  22, steps per second: 143, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 0.028575, mae: 3.538544, mean_q: 7.135443, mean_eps: 2.000000\n",
            " 7955/10000: episode: 367, duration: 0.095s, episode steps:  11, steps per second: 116, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.017287, mae: 3.560179, mean_q: 7.259647, mean_eps: 2.000000\n",
            " 7966/10000: episode: 368, duration: 0.079s, episode steps:  11, steps per second: 140, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.014333, mae: 3.511889, mean_q: 7.109072, mean_eps: 2.000000\n",
            " 7997/10000: episode: 369, duration: 0.208s, episode steps:  31, steps per second: 149, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 0.047166, mae: 3.532267, mean_q: 7.187731, mean_eps: 2.000000\n",
            " 8026/10000: episode: 370, duration: 0.205s, episode steps:  29, steps per second: 141, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 0.069410, mae: 3.607870, mean_q: 7.252358, mean_eps: 2.000000\n",
            " 8054/10000: episode: 371, duration: 0.182s, episode steps:  28, steps per second: 154, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.041973, mae: 3.550314, mean_q: 7.186301, mean_eps: 2.000000\n",
            " 8095/10000: episode: 372, duration: 0.277s, episode steps:  41, steps per second: 148, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.439 [0.000, 1.000],  loss: 0.038236, mae: 3.601579, mean_q: 7.276857, mean_eps: 2.000000\n",
            " 8125/10000: episode: 373, duration: 0.190s, episode steps:  30, steps per second: 158, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 0.024196, mae: 3.658265, mean_q: 7.433807, mean_eps: 2.000000\n",
            " 8141/10000: episode: 374, duration: 0.116s, episode steps:  16, steps per second: 137, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 0.039472, mae: 3.653067, mean_q: 7.384104, mean_eps: 2.000000\n",
            " 8157/10000: episode: 375, duration: 0.106s, episode steps:  16, steps per second: 151, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.012715, mae: 3.633415, mean_q: 7.344903, mean_eps: 2.000000\n",
            " 8190/10000: episode: 376, duration: 0.228s, episode steps:  33, steps per second: 144, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 0.032039, mae: 3.608154, mean_q: 7.333600, mean_eps: 2.000000\n",
            " 8203/10000: episode: 377, duration: 0.098s, episode steps:  13, steps per second: 133, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.032238, mae: 3.651818, mean_q: 7.417076, mean_eps: 2.000000\n",
            " 8213/10000: episode: 378, duration: 0.073s, episode steps:  10, steps per second: 137, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.065626, mae: 3.646472, mean_q: 7.349768, mean_eps: 2.000000\n",
            " 8232/10000: episode: 379, duration: 0.139s, episode steps:  19, steps per second: 137, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.368 [0.000, 1.000],  loss: 0.029983, mae: 3.605922, mean_q: 7.307423, mean_eps: 2.000000\n",
            " 8247/10000: episode: 380, duration: 0.108s, episode steps:  15, steps per second: 138, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.064115, mae: 3.621128, mean_q: 7.392777, mean_eps: 2.000000\n",
            " 8298/10000: episode: 381, duration: 0.488s, episode steps:  51, steps per second: 104, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 0.040707, mae: 3.672276, mean_q: 7.454297, mean_eps: 2.000000\n",
            " 8311/10000: episode: 382, duration: 0.123s, episode steps:  13, steps per second: 106, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 0.029558, mae: 3.703176, mean_q: 7.489566, mean_eps: 2.000000\n",
            " 8343/10000: episode: 383, duration: 0.297s, episode steps:  32, steps per second: 108, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 0.037228, mae: 3.698393, mean_q: 7.497382, mean_eps: 2.000000\n",
            " 8354/10000: episode: 384, duration: 0.106s, episode steps:  11, steps per second: 104, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.048733, mae: 3.756966, mean_q: 7.591765, mean_eps: 2.000000\n",
            " 8367/10000: episode: 385, duration: 0.126s, episode steps:  13, steps per second: 103, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 0.049472, mae: 3.748929, mean_q: 7.566415, mean_eps: 2.000000\n",
            " 8386/10000: episode: 386, duration: 0.186s, episode steps:  19, steps per second: 102, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 0.076932, mae: 3.640244, mean_q: 7.345029, mean_eps: 2.000000\n",
            " 8406/10000: episode: 387, duration: 0.222s, episode steps:  20, steps per second:  90, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.039473, mae: 3.831100, mean_q: 7.692113, mean_eps: 2.000000\n",
            " 8423/10000: episode: 388, duration: 0.177s, episode steps:  17, steps per second:  96, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 0.036310, mae: 3.715826, mean_q: 7.487006, mean_eps: 2.000000\n",
            " 8434/10000: episode: 389, duration: 0.127s, episode steps:  11, steps per second:  86, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.029875, mae: 3.771233, mean_q: 7.548242, mean_eps: 2.000000\n",
            " 8448/10000: episode: 390, duration: 0.153s, episode steps:  14, steps per second:  91, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 0.043429, mae: 3.747536, mean_q: 7.530044, mean_eps: 2.000000\n",
            " 8501/10000: episode: 391, duration: 0.563s, episode steps:  53, steps per second:  94, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 0.034608, mae: 3.705297, mean_q: 7.496694, mean_eps: 2.000000\n",
            " 8529/10000: episode: 392, duration: 0.179s, episode steps:  28, steps per second: 156, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 0.039081, mae: 3.727927, mean_q: 7.526706, mean_eps: 2.000000\n",
            " 8558/10000: episode: 393, duration: 0.192s, episode steps:  29, steps per second: 151, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 0.041779, mae: 3.702970, mean_q: 7.488153, mean_eps: 2.000000\n",
            " 8577/10000: episode: 394, duration: 0.133s, episode steps:  19, steps per second: 143, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 0.039808, mae: 3.832604, mean_q: 7.756170, mean_eps: 2.000000\n",
            " 8609/10000: episode: 395, duration: 0.214s, episode steps:  32, steps per second: 149, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.594 [0.000, 1.000],  loss: 0.043800, mae: 3.752237, mean_q: 7.602865, mean_eps: 2.000000\n",
            " 8650/10000: episode: 396, duration: 0.275s, episode steps:  41, steps per second: 149, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 0.032411, mae: 3.819500, mean_q: 7.750807, mean_eps: 2.000000\n",
            " 8662/10000: episode: 397, duration: 0.079s, episode steps:  12, steps per second: 152, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.022643, mae: 3.767956, mean_q: 7.611508, mean_eps: 2.000000\n",
            " 8693/10000: episode: 398, duration: 0.211s, episode steps:  31, steps per second: 147, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  loss: 0.043834, mae: 3.814633, mean_q: 7.713569, mean_eps: 2.000000\n",
            " 8733/10000: episode: 399, duration: 0.264s, episode steps:  40, steps per second: 151, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 0.034649, mae: 3.883557, mean_q: 7.856366, mean_eps: 2.000000\n",
            " 8752/10000: episode: 400, duration: 0.123s, episode steps:  19, steps per second: 155, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 0.023976, mae: 3.889545, mean_q: 7.849592, mean_eps: 2.000000\n",
            " 8769/10000: episode: 401, duration: 0.117s, episode steps:  17, steps per second: 145, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.353 [0.000, 1.000],  loss: 0.016407, mae: 3.896437, mean_q: 7.918239, mean_eps: 2.000000\n",
            " 8790/10000: episode: 402, duration: 0.135s, episode steps:  21, steps per second: 156, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 0.020493, mae: 3.951377, mean_q: 8.027929, mean_eps: 2.000000\n",
            " 8805/10000: episode: 403, duration: 0.113s, episode steps:  15, steps per second: 133, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.038536, mae: 3.822189, mean_q: 7.793153, mean_eps: 2.000000\n",
            " 8821/10000: episode: 404, duration: 0.111s, episode steps:  16, steps per second: 145, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 0.042642, mae: 3.904943, mean_q: 7.910156, mean_eps: 2.000000\n",
            " 8835/10000: episode: 405, duration: 0.105s, episode steps:  14, steps per second: 133, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.024538, mae: 3.873187, mean_q: 7.814512, mean_eps: 2.000000\n",
            " 8863/10000: episode: 406, duration: 0.185s, episode steps:  28, steps per second: 151, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 0.028027, mae: 3.919591, mean_q: 7.954998, mean_eps: 2.000000\n",
            " 8877/10000: episode: 407, duration: 0.095s, episode steps:  14, steps per second: 148, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 0.048428, mae: 3.873612, mean_q: 7.852228, mean_eps: 2.000000\n",
            " 8889/10000: episode: 408, duration: 0.084s, episode steps:  12, steps per second: 143, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 0.033325, mae: 3.855564, mean_q: 7.776131, mean_eps: 2.000000\n",
            " 8908/10000: episode: 409, duration: 0.131s, episode steps:  19, steps per second: 145, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.368 [0.000, 1.000],  loss: 0.031129, mae: 3.877212, mean_q: 7.898396, mean_eps: 2.000000\n",
            " 8933/10000: episode: 410, duration: 0.160s, episode steps:  25, steps per second: 156, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.035364, mae: 3.975425, mean_q: 8.036946, mean_eps: 2.000000\n",
            " 8955/10000: episode: 411, duration: 0.154s, episode steps:  22, steps per second: 143, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.409 [0.000, 1.000],  loss: 0.020938, mae: 3.998043, mean_q: 8.116489, mean_eps: 2.000000\n",
            " 8967/10000: episode: 412, duration: 0.089s, episode steps:  12, steps per second: 136, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 0.030373, mae: 3.901506, mean_q: 7.871253, mean_eps: 2.000000\n",
            " 9004/10000: episode: 413, duration: 0.240s, episode steps:  37, steps per second: 154, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.378 [0.000, 1.000],  loss: 0.028518, mae: 4.025123, mean_q: 8.142031, mean_eps: 2.000000\n",
            " 9049/10000: episode: 414, duration: 0.296s, episode steps:  45, steps per second: 152, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 0.021844, mae: 3.944425, mean_q: 7.993837, mean_eps: 2.000000\n",
            " 9062/10000: episode: 415, duration: 0.091s, episode steps:  13, steps per second: 142, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 0.029518, mae: 4.015658, mean_q: 8.136433, mean_eps: 2.000000\n",
            " 9084/10000: episode: 416, duration: 0.163s, episode steps:  22, steps per second: 135, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.014426, mae: 4.012249, mean_q: 8.168263, mean_eps: 2.000000\n",
            " 9120/10000: episode: 417, duration: 0.246s, episode steps:  36, steps per second: 146, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 0.018349, mae: 4.016997, mean_q: 8.148590, mean_eps: 2.000000\n",
            " 9145/10000: episode: 418, duration: 0.166s, episode steps:  25, steps per second: 150, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  loss: 0.015525, mae: 4.037799, mean_q: 8.167340, mean_eps: 2.000000\n",
            " 9173/10000: episode: 419, duration: 0.181s, episode steps:  28, steps per second: 154, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.607 [0.000, 1.000],  loss: 0.023642, mae: 4.057216, mean_q: 8.250399, mean_eps: 2.000000\n",
            " 9186/10000: episode: 420, duration: 0.095s, episode steps:  13, steps per second: 136, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 0.025235, mae: 4.112413, mean_q: 8.313968, mean_eps: 2.000000\n",
            " 9199/10000: episode: 421, duration: 0.086s, episode steps:  13, steps per second: 152, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 0.016142, mae: 4.079306, mean_q: 8.292687, mean_eps: 2.000000\n",
            " 9216/10000: episode: 422, duration: 0.114s, episode steps:  17, steps per second: 150, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 0.020370, mae: 4.034371, mean_q: 8.220292, mean_eps: 2.000000\n",
            " 9236/10000: episode: 423, duration: 0.134s, episode steps:  20, steps per second: 149, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.012062, mae: 4.045976, mean_q: 8.266550, mean_eps: 2.000000\n",
            " 9255/10000: episode: 424, duration: 0.134s, episode steps:  19, steps per second: 142, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.368 [0.000, 1.000],  loss: 0.009213, mae: 4.067397, mean_q: 8.296904, mean_eps: 2.000000\n",
            " 9268/10000: episode: 425, duration: 0.085s, episode steps:  13, steps per second: 154, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: 0.013412, mae: 4.071320, mean_q: 8.303200, mean_eps: 2.000000\n",
            " 9285/10000: episode: 426, duration: 0.120s, episode steps:  17, steps per second: 142, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.353 [0.000, 1.000],  loss: 0.011790, mae: 4.096085, mean_q: 8.326612, mean_eps: 2.000000\n",
            " 9303/10000: episode: 427, duration: 0.121s, episode steps:  18, steps per second: 149, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.389 [0.000, 1.000],  loss: 0.017470, mae: 4.093923, mean_q: 8.321970, mean_eps: 2.000000\n",
            " 9318/10000: episode: 428, duration: 0.110s, episode steps:  15, steps per second: 137, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.017160, mae: 4.089762, mean_q: 8.273040, mean_eps: 2.000000\n",
            " 9359/10000: episode: 429, duration: 0.289s, episode steps:  41, steps per second: 142, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  loss: 0.018005, mae: 4.093513, mean_q: 8.295155, mean_eps: 2.000000\n",
            " 9370/10000: episode: 430, duration: 0.076s, episode steps:  11, steps per second: 145, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.010857, mae: 4.052675, mean_q: 8.319880, mean_eps: 2.000000\n",
            " 9392/10000: episode: 431, duration: 0.156s, episode steps:  22, steps per second: 141, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.318 [0.000, 1.000],  loss: 0.017984, mae: 4.084776, mean_q: 8.301902, mean_eps: 2.000000\n",
            " 9427/10000: episode: 432, duration: 0.237s, episode steps:  35, steps per second: 148, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  loss: 0.021002, mae: 4.102920, mean_q: 8.328855, mean_eps: 2.000000\n",
            " 9443/10000: episode: 433, duration: 0.106s, episode steps:  16, steps per second: 151, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 0.019984, mae: 4.089290, mean_q: 8.314920, mean_eps: 2.000000\n",
            " 9462/10000: episode: 434, duration: 0.132s, episode steps:  19, steps per second: 144, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 0.015173, mae: 4.221221, mean_q: 8.600409, mean_eps: 2.000000\n",
            " 9485/10000: episode: 435, duration: 0.149s, episode steps:  23, steps per second: 154, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.391 [0.000, 1.000],  loss: 0.027067, mae: 4.101033, mean_q: 8.317799, mean_eps: 2.000000\n",
            " 9523/10000: episode: 436, duration: 0.252s, episode steps:  38, steps per second: 151, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.632 [0.000, 1.000],  loss: 0.027340, mae: 4.166481, mean_q: 8.472235, mean_eps: 2.000000\n",
            " 9547/10000: episode: 437, duration: 0.175s, episode steps:  24, steps per second: 137, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 0.027934, mae: 4.193274, mean_q: 8.566845, mean_eps: 2.000000\n",
            " 9556/10000: episode: 438, duration: 0.073s, episode steps:   9, steps per second: 124, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.013902, mae: 4.150023, mean_q: 8.483415, mean_eps: 2.000000\n",
            " 9580/10000: episode: 439, duration: 0.162s, episode steps:  24, steps per second: 148, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 0.025390, mae: 4.256248, mean_q: 8.622596, mean_eps: 2.000000\n",
            " 9614/10000: episode: 440, duration: 0.231s, episode steps:  34, steps per second: 147, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 0.022889, mae: 4.173734, mean_q: 8.519745, mean_eps: 2.000000\n",
            " 9631/10000: episode: 441, duration: 0.117s, episode steps:  17, steps per second: 145, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.353 [0.000, 1.000],  loss: 0.011988, mae: 4.287095, mean_q: 8.710600, mean_eps: 2.000000\n",
            " 9661/10000: episode: 442, duration: 0.211s, episode steps:  30, steps per second: 142, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.367 [0.000, 1.000],  loss: 0.021740, mae: 4.160704, mean_q: 8.464774, mean_eps: 2.000000\n",
            " 9691/10000: episode: 443, duration: 0.213s, episode steps:  30, steps per second: 141, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 0.023664, mae: 4.171096, mean_q: 8.537168, mean_eps: 2.000000\n",
            " 9702/10000: episode: 444, duration: 0.078s, episode steps:  11, steps per second: 141, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.012789, mae: 4.370771, mean_q: 8.899077, mean_eps: 2.000000\n",
            " 9717/10000: episode: 445, duration: 0.106s, episode steps:  15, steps per second: 142, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.014028, mae: 4.290680, mean_q: 8.760448, mean_eps: 2.000000\n",
            " 9729/10000: episode: 446, duration: 0.085s, episode steps:  12, steps per second: 142, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.033778, mae: 4.147063, mean_q: 8.492894, mean_eps: 2.000000\n",
            " 9751/10000: episode: 447, duration: 0.145s, episode steps:  22, steps per second: 152, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 0.030343, mae: 4.143605, mean_q: 8.378558, mean_eps: 2.000000\n",
            " 9787/10000: episode: 448, duration: 0.243s, episode steps:  36, steps per second: 148, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.028974, mae: 4.270409, mean_q: 8.671264, mean_eps: 2.000000\n",
            " 9806/10000: episode: 449, duration: 0.137s, episode steps:  19, steps per second: 139, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.632 [0.000, 1.000],  loss: 0.039138, mae: 4.278747, mean_q: 8.690063, mean_eps: 2.000000\n",
            " 9820/10000: episode: 450, duration: 0.107s, episode steps:  14, steps per second: 131, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.030021, mae: 4.342788, mean_q: 8.817500, mean_eps: 2.000000\n",
            " 9839/10000: episode: 451, duration: 0.142s, episode steps:  19, steps per second: 134, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 0.055754, mae: 4.214732, mean_q: 8.539500, mean_eps: 2.000000\n",
            " 9853/10000: episode: 452, duration: 0.109s, episode steps:  14, steps per second: 129, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  loss: 0.039517, mae: 4.248410, mean_q: 8.674000, mean_eps: 2.000000\n",
            " 9864/10000: episode: 453, duration: 0.080s, episode steps:  11, steps per second: 138, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.044715, mae: 4.203542, mean_q: 8.565611, mean_eps: 2.000000\n",
            " 9888/10000: episode: 454, duration: 0.175s, episode steps:  24, steps per second: 137, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 0.051921, mae: 4.288131, mean_q: 8.674554, mean_eps: 2.000000\n",
            " 9934/10000: episode: 455, duration: 0.320s, episode steps:  46, steps per second: 144, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.435 [0.000, 1.000],  loss: 0.024082, mae: 4.330440, mean_q: 8.869273, mean_eps: 2.000000\n",
            " 9962/10000: episode: 456, duration: 0.236s, episode steps:  28, steps per second: 119, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 0.027026, mae: 4.278045, mean_q: 8.792822, mean_eps: 2.000000\n",
            " 9976/10000: episode: 457, duration: 0.151s, episode steps:  14, steps per second:  92, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 0.024306, mae: 4.347926, mean_q: 8.906586, mean_eps: 2.000000\n",
            " 9992/10000: episode: 458, duration: 0.161s, episode steps:  16, steps per second: 100, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 0.028660, mae: 4.367497, mean_q: 8.901655, mean_eps: 2.000000\n",
            "done, took 74.776 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize the history for number  of episode steps\n",
        "plt.plot(history.history['nb_episode_steps'])\n",
        "plt.ylabel('nb_episode_steps')\n",
        "plt.xlabel('episodes')\n",
        "plt.show()\n",
        "\n",
        "dqn.test(env, nb_episodes=20, visualize=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 672
        },
        "id": "Vfw_AHVIbed5",
        "outputId": "c272197d-4ce7-4f16-f889-c7b9226a3a34"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEGCAYAAACNaZVuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAABuoElEQVR4nO19d9wdRbn/99lT3jeNdEIgQOi9hyYI0hEQUBEFVBQUCyJef9eC7Xrv9Sper3otWLiCxAKCSFNQulQFAgkEkkCAhISQBul52zlnn98fu7M7MztbTj9vznzzyec9Z8/uzuzszDPPfJ8yxMywsLCwsOguOO2ugIWFhYVF62GFv4WFhUUXwgp/CwsLiy6EFf4WFhYWXQgr/C0sLCy6EPl2VyArJk2axNOnT293NSwsLCyGFZ5++uk3mXmyfnzYCP/p06dj1qxZ7a6GhYWFxbACEb1mOm5pHwsLC4suhBX+FhYWFl0IK/wtLCwsuhBW+FtYWFh0Iazwt7CwsOhCWOFvYWFh0YWwwt/CwsKiC2GFf4eDmXHz069jsFxpd1UsLCy2IFjh3+G4d95K/Osfn8UP7nmp3VWxsLDYgmCFf4djw0AZALB602Cba2JhYbElwQp/C4sWYKjs4qanlsLunGfRKRg2uX0sLIYzfvrAQvz4gZcxopjDuw7Ytt3VsbCwmr+FRSsgaLsNA6U218TCwoMV/hYWLQSB2l0FCwsAVvhbWLQEluq36DRY4W9hYWHRhbDC38LCwqIL0VThT0R7ENEc6f8GIvocEU0gonuJaKH/d3wz6zGcYV0DLSwsmoGmCn9mfpGZD2TmAwEcAqAPwK0AvgzgfmbeDcD9/neLBFhD4ZYBsq/RokPQStrnBACvMPNrAM4CMNM/PhPA2S2sh4VFy2EXcBadhlYK/w8AuMH/PIWZl/ufVwCYYrqAiC4hollENGv16tWtqKOFRVNhFX+LTkFLhD8RFQGcCeCP+m/skdpGvYiZr2bmGcw8Y/LkyU2upYWFhUX3oFWa/zsBPMPMK/3vK4loKgD4f1e1qB4WFm0Bm/WbhmLBig14/OU3m16OxZaBVgn/8xBSPgBwB4AL/c8XAri9RfWwsGgrmmnwPfV/H8H5v3qieQVYbFFouvAnolEATgJwi3T4SgAnEdFCACf63y0stlhYg69Fp6HpWT2ZeTOAidqxt+B5/1hYWFhYtAE2wtfCwsKiC2GFv4WFhUUXwgp/C4sWwkZqW3QKrPC3sGgBrL3XotNghX+HwwqNLQxW8bfoEFjhP0xgE4INb1hXT4tOgxX+FhYthJ3DLToFVvhbWFhYdCGs8LewsLDoQljhb2FhYdGFsMLfwqIFaEVWTwuLamCFv4VFC0HWbcuiQ2CFv4VFC8HW59OiQ2CFv4VFK8DKHwuLtsMKfwuLVsJKf4sOgRX+FhYtgJD5rqV9LDoEVvhbWLQQVvRbdAqs8LewaCGs5m/RKbDC38KihbCy36JTYIW/hUULYV09LToFXSP8H1m4Gs8sWdvualh0KYTQd63st+gQNF34E9E4IrqZiBYQ0XwiOpKIJhDRvUS00P87vtn1+NA1T+I9P3u82cVYWCTCav4WnYJWaP4/AvA3Zt4TwAEA5gP4MoD7mXk3APf73y0stliErp5trYaFRYCmCn8iGgvgGADXAAAzDzHzOgBnAZjpnzYTwNnNrIeFRacgi+x/ftl63D9/pfG322Yvw2tvbW5spSy6Es3W/HcCsBrAr4loNhH9iohGAZjCzMv9c1YAmGK6mIguIaJZRDRr9erVTa6qhUXzkYX2OeMnj+LimbOMv33uxjk44yePNrpaFl2IZgv/PICDAfycmQ8CsBkaxcPeaDCOCGa+mplnMPOMyZMnN7mqFhbNg5D5jfDz3zhQrvseFhbNFv6vA3idmZ/wv98MbzJYSURTAcD/u6rJ9bCw6AhYe69Fp6Cpwp+ZVwBYSkR7+IdOADAPwB0ALvSPXQjg9mbWw8KiU2ANvhadgnwLyrgMwO+JqAjgVQAfhTfp3EREFwN4DcC5LaiHhUXbUc+OXtZN1KKRaLrwZ+Y5AGYYfjqh2WVvEbDjfdji+WXrsWxdP07ZZ5vgNdYjv+2qwaKRaIXmb9EA2M3/hh+EV87iK08PjtWjvVvN36KR6Jr0DhYWnYB6tHcr+i0aCSv8LSxagDC3Tz2af6NqY2Fhhb+FRUvQGM7fSn+LxsEK/w5HPd4hFh0EsYG7FeAWHQIr/DscVlZsGRBaez2v0/YFi0bCCv8Ohx3vWwbcRnD+GXuDXV1YZIEV/h0OO463DIS5fWq/R9ZrbZ+xyAIr/Dsc9XD+r6zehJuffr2BtakepYqLqx58GQOlSlvr0W64Aedf+z2yavSmszYOlPDzv78C10aKWfiwQV4djnqExan/+zBKFcY5h0xrXIWqxPVPLMH37n4R5Qrj8hN3a1s92g0huOsK8qqqLDUs8L/unI8/PLUUO08ehVP22abmOlhsOcis+RPR5US0FXm4hoieIaKTm1k5i/o4/1Kl/Vpe31DF/9vdaYhbafA1av6DXvsPld06amCxJaEa2uciZt4A4GQA4wF8CMCVTamVRQhL4G4REGxLPbRLZtrHdFqTu9H6Po9Wssbm4YNqhL9YR54G4LfM/AJsypmmY7gPJbI9BIDs7VP7PbJr/q3vNV+//Xl8928L8OjLb7a8bIvaUI3wf5qI7oEn/O8mojEA7BqyyRADvh4h2gnaWPtr0F6IV1BXSucqy2olNg6UAHgGfovhgWqE/8XwtmA8lJn7ABTh5ebfonDvvJV4ctGadlcjQCMEdztlv1X8PQScf5vTO3T7Suy6xxZh+fr+dlejI5BZ+DOzC2A6gG8Q0fcBHMPMzzWrYu3Cx38zC+f+8h/trkaARshtmxOm/XAb4e1j/fzrwvL1/fjmn+fhoutmtbsqHYFqvH1+BuCTAOYCeB7AJ4joqmZVzMJDIwZyJ7h2dwL11E40Isgrc4RvG0i24fB2Rduv7xtqb0U6BNX4+R8PYC/2RzERzYS3H69FE9GIQdXO5HCCZuhy2d8Qzj/rpcOhrdduHsINTy3Bp47dBdQiLkqUMgyapyWohvN/GcAO0vftASxsbHUsdAx/zt8bct0+4Brh7ZM5vUPtRdSMasX3FbfMxX//7UX889XW29eGw+TYClSj+Y8BMJ+InoTXvw4DMIuI7gAAZj6zCfWzaADayfl3u4FRoCGcfx2J3Zq9+qv27hsHPe+gsts676BgFdr1qoiHaoT/N5pWC4tYbCmcf7ejMbl9Mp5XexEtBzXBH+zxl9/EQLmC4/ecYizLav4eMgt/Zn6IiHYEsBsz30dEIwDkmXlj0nVEtBjARgAVAGVmnkFEEwDcCM97aDGAc5l5bW2PsGWjEVpKJ3j7dEAV2oqGbOOYuayai6gZ1YrwZtbx/F89AQBYfOXpynG7ClVRjbfPxwHcDOCX/qFpAG7LePlxzHwgM8/wv38ZwP3MvBuA+/3vFgY0YpCwjbtpOxqh+WdODZFwWjM0bbnIau/fSoEcGt0tgOoMvpcCOArABgBg5oUAtq6x3LMAzPQ/zwRwdo332eJh/fw7A/1DFfzw3pdqimBl5poMvrXaB9rJaWe3SzS5IqYyUX+g3ZaEaoT/IDMHDrJElEc22cQA7iGip4noEv/YFGZe7n9eAWCK6UIiuoSIZhHRrNWrV1dR1S0HjeH829/bh7uR7ccPLMSP7l+Im2YtrfpaZlnzz94O+qlbYpBXK5mYsF2GUQM1EdUYfB8ioq8AGEFEJwH4NIA/Z7juaGZeRkRbA7iXiBbIPzIzE5HxbTDz1QCuBoAZM2Z05RtrDOffgIrUiFb5cDcbfX5K5FINKZEZUj7/Kq9Tv2cN8jIca1EfyEr7BM/Swu7RiBQbWxKq0fy/DGA1vAjfTwC4i5m/mnYRMy/z/64CcCs8F9GVRDQVAPy/q6qsd9cgSOxWxyhpdXTtvDc24KanVA1ZVOGWZ17Hc6+va2l9GoGA065hMmPm4Plvnb0ML65I9JFQrpORfRvHzqd9BJplgzDBCn0V1Qj/y5j5/5j5fcx8DjP/HxFdnnQBEY3ys3+CiEbB2wvgeQB3ALjQP+1CALfXUHeLjGh1nz/tx4/gi3/y0j7pQ/vzNz2LM3/6WItrVD/qya7qskq9nfqjh7OVGalD7Zp/p6EtnL81+CqoRvhfaDj2kZRrpgB4lIieBfAkgDuZ+W/wNoE5iYgWAjgRdlOYWDRCi+sEzn+4Q7RhLXoqg5V3UCt33whXz2azcNlpH//8Vnr7BAZfOx6ADJw/EZ0H4HwAO4loXh9bAUiMzWbmVwEcYDj+FoATqqtqd8IGeXUGgiasifap7T3qk3Ynb+aShjWbh/C7f76Gzxy3KxyH2qJ+u1bzV5DF4Ps4gOUAJgH4vnR8I4AtLqVzp6Ehrp5tlP5biL1Xsr3Udm0jVl+ZNdYOlG5fuWUu/vbCChyy43gcteuk4HhrvX2swVdGqvBn5tcAvEZEJwLoZ2aXiHYHsCc8469FE9GQIC/b2RsArxGdWjR/cE2rr5ppn+qLajo2D/neUn6cRDtWJ7W4227JqIbzfxhALxFtB+AeeBu4X9eMSlmE2HLSO7S/DvVA5B9rhME3K/R3P5z8/K95dBEWvbk5+B7nJdVaV+AOaJgOQlUbuPvbN74HwM+Y+X0A9mlOtSwEhnuQlzy0h/MEIARxbbQP18j5m+uQWp7hvFY2ff9QBf/5l3mJO+K1oytYzl9FVcKfiI4EcAGAO/1jucZXqfEYzkJHoJ4VQLsof7nduY31aATqcfX0nj18eCfjPSJ+/hnjy9rZ3dn/B4Sbuqu/q2hHbh8r/T1UI/wvB3AFgFuZ+QUi2hnAg82pVmMxrIWO+FvHM7Rr8nNZXda3Mnd7o1FXkJervr+cJv1nLV6D22Yviy0z/N75fv7M5tTJeqs1ol9XiyDCt3VF1ozX1/bh539/paljt5qUzg/D4/3F91cBfFZ8J6KfMPNlja1eYzCsNf8GdNh2TX463TSMZX993j6an79uND7nFx49cvZB26mrJZ32aUCEb7MVbeaUSSryTK3rnEGQ1zCQBx+bOQsLVmzEuw6YimnjRzaljGo0/zQc1cB7NRSd/6rj0RBXz7Zp/qogG96av8/516D5ewbf8Luu+evnBmXW6uffxg7vcjbPpnB/gyZXSEInOD5kxSY/l1Qzh0wjhX/HYhi98whCbaX+e7QaermVYcy/1efnz4ogzyVMIEmRwFlpn/+550WUY1JPN/sNyPYNY1nao7fD5bPWEh9YsBL3zVvZ0LrEoRVbTnaH8B/Gun8Qkl6Xwbc9z19xVaFXdmXBNrzeiahvQwy+CZq/PEFGOP+MTXb7nDfw1+dXaHXgqu5RKxTPJlNZmsdNK7tBvYrURdfNwsd+M6txFUpAKxLeNVL4d2ws5zCTMwoa4aHQrueXKQAGK4KtVBleLyU0+FZ/rU6FJHn7yO8qQvtUUWbcKqvZipDL5ok9rt3aY/AdPn2vme1TtfAnojjrw4/qrEvTMKyFv/a3FrSP81frLQukwXKl9RWqA6IJa4nwBavvIJnzZ+lz/G+1otnMG0v2DWO8gbYCaWXfbMdqo1a0wgW2mj1830ZE8wAs8L8fQEQ/E78z83WNr15jMJxmeh2N8FBol/CXuW5mXfgPL+NvvRuvy5cnTSAK519jhK/p2vAezdb82az5x5zfyp45nFw9BZpZ12o0/x8COAXAWwDAzM8COKYZlWo0hsNMH4eQ868d7XP1lCYvqJz/cBP+Ie1Ti7cPV6H5GwqNP1A1mjUWZE0+k7cPwvNbheEkB0QPaaaTRFW0DzPrG5gOi7V71ubrSCNkQ7x96n+um2YtxdOvJWbwjqDiqkKvIvmtDZaGRdcJUZe3T3bNX4+K1u9TL1qxCjZlzxSTZuQZWmrw1azNwwDNnByr2cN3KRG9DQATUQFexO/85lSrscjagB0s+9uu+X/xZi979+IrT898DcsGX1Y1/4HScNP8a1+Buawau5M0f/m8SJBcI4R/k/q4mM9c5mx9NvDzbwPnPwykv5gsy010jKhG8/8kgEsBbAdgGYAD/e8dj6z9qxODQEItavhx/p7BV9b8h7/Bt5b3wAyUpFVP9iAv/T6da/CV3Sir6W8t9fZxoyuSToXoIc0cu5mFPzO/ycwXMPMUZt6amT/o78jV+cjYfs3sE+WKiyv/ugDr+oaMv9/yzOt4/JU3o3VqwEq1nRG+YdE8rA2+9fiIe/EO4fcks0Ei7VN90dJ9o/evB29tGsR3/7YgwknL0cxyWXG5fdqh+Q8nNJPzz7KN40+Q0G7M/Nm43zoFWZd5zeyI981fiV889ApWbRjAD95/YOT3z9/0LIAorcKRDzWgbQZfVnYRk2mfoWEm/OvxER/Som3jInxlmkx8V3+PL6PVm7t/5da5uPuFlThy54k4ZvfJCu2TtHNcNGq5dajH26flu+H57Vlps+Y/C8DTAHoBHAxgof//QADFptWsgcjafs1UQsT4H6iS7gg1/3pon5ovrQvM8X7+zezUzUCgqdYwZ+kTXRzto2/6Ug3tk72PN6bdN/R7uWfEs1QbjNjolUi2QmsvU5/AW4VmTjpZtnGcCQBE9CkARzNz2f/+CwCPNK1mDUTW5mtmPww0oyr7UC1Cf94bG/DPV0NGrp3pHQJti1XjVTv3Fa4FafTbdY8twozpE7DvdmMjv+mCI87bp+Kq1FiSlszMittpq/u4sNn05FX9UXZrzVJUPfV5YMFK9A+5OH3/qZnO1zdzuW/eSpRdF6fum359NTTlPS94qTVO3mebzNfEoa20j4TxALYCIPz9RvvHUkFEOXgriGXMfAYR7QTgDwAmwltVfIiZzWR4A5BV+HWmwVf9mwWn/VidkzuB89cNgcMvyVuyd8o3/zwPgNkbqqQJDidmva3aSKITv/Ibq7aDVtM+YkIrRoS/zPmHx8NEZeIvB+fXiouu8/LsnL5/Ng80PbpY5OnJ4sFWDU15yW+fznzfOHSan/+VAGYT0XVENBPAMwC+nfHay6G6hX4XwA+ZeVcAawFcXEU9qkbmJXET6+AEnT9aSpYXXJ+ff+3X1gM5zwuDFc5/uAl/N5RaVSMr56/HRUQ1/3jaLLLlY0w9G6UIDPquujqFxTBH+Ib10ia0FrL+9Tx6q2kfsaprN+cPAGDmXwM4HMCtAP4E4EhBCSWBiKYBOB3Ar/zvBOB4ADf7p8wEcHZVta4SnWDwFXO5SeYNJAQ8ycKzVrQzvYN4XpfVIK9hx/nX4Zeua41xWT0rzPi/R14Nvif5+eu/xfWPqx9+BQtWbGh4XhshDPX76XsX6NDr0Qn5/O+dtxJ/nbs88dpWOyh0muYPAIcBeDu8tA6HZrzmfwF8EYBovYkA1gnbAYDX4cUOREBElxDRLCKatXr16iqrKiGr5t/MjROE5m+oS3+S8Bd/6+gD9fafWo1ysp+/y6xw/sNN809S/NPap5TV28cFfv/EkkiZpnKixmBzvb591wKc+ZPHYu9ZK4TmHymX4zR/YRjWqawWav4xxz/+m1n41O+fSby2Xd5pHeHnT0RXwqNv5vn/P0tEibQPEZ0BYBUzP11L5Zj5amaewcwzJk+eXMstvPtkPq95DR0a56JlJGv+cVdlR70dqFZB7dEY3mdmzdtnuAn/QFON1jvtUXRjYZLmbyoz+K6UmZ0qHKq4gSbZKGErNH/T6iSpBN0e0FJnnzoKa5fwbybbVI3mfxqAk5j5Wma+FsCpAM5IueYoAGcS0WJ4Bt7j4aV+HkdEwtg8DV7EcNOQ3eDbzFp4MFUlUfhrRqrayqxT+Nes+bMiNOX7DDvhL/4aqp32LPreBXExXtH7xEv/LBy/6b3Fvco1m4fw7bvmx+4ApkPkZooKfzaON4pZ+Q4Xzr/VEemivTqJ9hknfR6bdjIzX8HM05h5OoAPAHiAmS8A8CCAc/zTLgRwe5X1qAqql0R2Y1Qj4WjeDjKS8tyEVaqH86/5Uu/6GrUPZpkrT85b0+lISrNhehb5PF1rzGqMjfLp8ZOnSYiaVynmwh9ZuBpXP/wqXn1zs7lyGkLNXz3OnNxfdDfQVm7rXM84aD3nL2yEzRsn1bh6fgeet8+D8JSXYwB8ucZyvwTgD0T0LQCzAVxT430yQV0uA7kY1auZyqgcAamj2Zx/vf2nHs3fDYS/yvmXh5vmn0C/mbQz+dCQpjXGDWjdNpCk3WeJ/pUFa1o/EquTrK86PD+q+Sdp81FDdetQj3I32HJvH+9vM8dJZuHPzDcQ0d8RGnq/xMwrEi7Rr/87gL/7n1+FZzxuOr71l3nYPFQOvnsdwGvZl1dtxF/nrsBlJ+wm/dYcUGDwiv4maB+THXA4c/6qn78awDTsgryE4dpQb9MAlfuSTvvEvQ89g2PEz1+5h16/KIwrEmPJ4cRTbV8xTVBJt5D7Qy3lVQM9EK5dmj8z43/ueRHnHbYDpo2P2wjRjGaOk2oMvkcB2MDMd8AL9voiEe3YtJo1CE8sWoNZi9cG3+WmPPeX/8T3730Jff7k0ExxlBT12D/kCf9izvQ64umGasuuFbUL/3DAua4qJLckzd80QBXNP0WjFyi7yfSQusWjrvlHb2rm/OMmHq/sat+1yeU0qb9FfmtiN4g+Su2F1SP8X1y5EVc9+AouTfEoMqFTOP+fA+gjogMAfB7AKwB+05RaNRCOQ7Eh833SigBorhYi6mAafAN+x9JD5b3z/b91lF037VOH5i/TPoqf/zAV/qZqm4Ss3JeinH+MANZ5/ATaJ40+YTa7Lsf1haEqaZ+4esi7t8kg6fek62tBXHtGKanay6hH+ItuX0sm244I8gJQZq81zwJwFTNfBWBMc6rVOORIHVTyUlocFoKomfIo6d6C9inmc5HfGuESV+8Aq/V6WSOu1eDbN1TGN+94ITJR14prH12E2UvWpp+oIcxRZBD0KR0ns+afSg/JtJn2i0GuGDX/GDWiVtrHNEFVo/k3YsjFKRJJNpOsXk0CW2Jit2qE/0YiugLABwHcSUQOgEJzqtU45BI0/4B3dNXvzYCsAesQwt+o+TdgeNTbf+qjfUKhWQvtc+2ji3Dd44vxq0cW1VQHHT+89yXcNrt6z2JR26wulSbN/9Ljdon8JiOtTZI1/+Q6hMfM9y7XzPlHJyxTGaGrp7rCaITmH9duepvIZel2mNQyujm9A4D3AxgEcLFv6J0G4HtNqVUD4RApXhSq8Pf+VrQO2Qy4CWWEmn9zaJ92cf7Mmp9/DQZfMbAbZSOoaJNQZgTvIXqtaas9uQjR/845ZHuctPeUVAEcFBmhSOTPKRSRdk7aClLQPtU2TVS7ZiT1VlfqD0n1qQZxWnmSeaFaTb6e7pe0eU9quZ2g+TPzCmb+ATM/4n9fwswdz/nnc5RA+3ifQz6+efVIKqN/yM+QaDD4hhpn7ZUzXXv9E0uMO4eZUDPtw/KKB1pitypv1qCXo09CmYtPyEKZ1c8/73g+X3HvUtdGo94+5hWs91u0fJMPfRrtk6WfyeeYJqGk5s1iqK4WcfvcRimp6DvJCrned81djjufS84FVNW9XcaVf12A19f2BcfEfNFMx4hU4U9Ej/p/NxLRBv1v02rWIDhEikalLp3F33hKplFI2gmqr+Tx2UmunvWVHT32lVvn4vz/eyLT9TWnd2BW2lhN79AuDrW2AZWkOaf6+fv9r5Bz4BAl0D7Z/fzThKiu+ZvuoZQdE7Rlrqc8CenUillb1V2dA6Umvbj0+ijjO3liEtBjKtIgv+NP//4ZXHp9ds+dNLkyb/kG/OKhV3DZDbODY62I8M2ymcvR/t+ON+6aEOH8DeeEBt8mCv/ArhD9beNAObb8RqR3aKefPxC2rbhPzqHMXCbFJkOoDZWaNX8PWaNmTX7++RzBcZIEcLpAF4hG+EbLNz1n2qojS19R6Tu1/OQQL8Mz1dg1lfaNs+np10B+J9UJ/3qGUBqzINrcVKdOifAFER0M4Gh47fooM89OuaTtyBElaiqA9HKaWI8ku8KmQPgbLgw0pdprV7fBt8YOKFMPruu1c84hf0Ku8l411SCKistVD3wgeRCabiefLiJ8CzkHlKj5pwv0sD7x5YnfTcXE9YW4RG0mmOIN5DQeyd4+0M6vcVUpOw9ILyApFkJeWNVD+2TFX557Aw4Rthnbm3ieuLWs6ISaf9XFZkY1QV7fgJd7fyKASQCuI6KvNatijYLu55/E2TbT2ycpL/+mQV/4mzS14Pq6Cq/j4jo0f1el1MpC+BO1hfYR76AmzV/QV4ZrzbRPeEwImkKO4BClUi96mabvaZui6In04u6pl52lq6iGZ/3+nHiPNEN1VsgTpeLQkXBv+WsrDL6fuX42Pv37Z1KNtuJXE+3bKZr/BQAOYOYBIEjxPAfAt5pQr4ZB1/xNKmRrDL6ILWPjQAlAMn1QH+1T+7VA1Kc8e7msTF4V10WOatP8G0H+iPdcE+ev/TXdV4aJ8887DhyKH9AlXfNPEPCRIg2afxKNGCm7RtpH1+A5ZsUR5rZSq1urcFOFv1nbN+UdMl2TBfUI4XQXXu93uY+LVUCnRPi+AUBev/SgyamYG4GclsUtyR+6mUFeAe1j+G1jAu0TCp34ys19fT2uevBlzH19PX7295cjv9fN+Ruun/n44lRvoUhiN5eRD2ifKjnXqs42QzxHTQMqgaZIC6Yqlb3PQvOPK15vkyjtE35Oj/BlTeNM7uNxWTpN0IP3vGNhvZL6W6NW1xU5SaD0WV0d6YWHH6umfeoQDqK/EQGL39yMK/+6QGmH4JNB9e+IxG4A1gN4gYjuhVffkwA8SUQ/BgBm/mwT6lc39F2TzN4a3t9mJ5mS/8oIaJ+EZXpS1d7100cBAN+7+0Xj780I8vrpgy/j2N0n4227TIq9zmUEPVtk9cznCERVGHwbaO8VRdbG+av3kGE2rIafhyouco733JSk+UcMvuY6mMqM8ttqsFXwc5y9oV7OX5pcMkX4sva9SsieUSXpc5LBV9X8W+fnLwQ4s7dp/MurNuEDh26P6ZNGKefJXT1QmjpE+N/q/xf4e2Or0hxEN5iOopV+/qZ3mYnzr6PsujdzMdSrVHFTO6aq+XvaVjHvoOJWb8hqxLsRz1GL5p+Y3iHl2FDZRd7vh8mcv/fD2Qdui9vmvGHwjAm/R7x9DBOFKZ1G3JPHpWg2oWIQ/iGdY16j6rRPXL2zQokWj6F9klZH1XL+9UTayis6EdDpSFpNkkLRzAjfalI6zySiEQB2YGazitmBcCKaf/xAba6fv1++dpyZk2kfjrmwqrKbIPzLbmrH9LRA7zOz52VTyDkgcITi6Bsq49t3zcfWY3px+E4TcPjOE+uqswmivrUspZM8wrJw/iKAL4nzF9qs2OYxIijlMlNoH51+Yem4CUFunwwyUdknQHq/4nvSBKJPQmmv4o11/fjVI4vw1dP3UhQ53eD7339bgPccPE3xrIlOiNI1VdI+9ShQFUN7yWJJ1Es+Js7riAhfInoXPAPv3/zvBxLRHU2qV8OgB82a2rIVmr8bSkHl+EDJTYwzaESV6jb4Guo1VHFTNWiWDL4uextiFPOO0eB73eOL8bt/LsEP7n0J77/6n5F7NYL+EcnP6tH8jVp+ij99qeIinws1/1jh72uwgqqM6NDSV73MtBVJEm0FhBx4JoOvifaRJgHTBBK3n0WaC/MXb34O1z62CE8uWqPWQdL2l63tx8/+/gouvPbJRIOvTsVVg3oUKFnRMQl6U4RyXfapjKjG4PtNeBuwrAMAZp4DYOeG16jBiNI+8Q2ths83ttHjDL4bB0vBZ2MHk5bTtaLRmr+nxScb9kS5stAcKnsacM6JCsBUd7gGvI5A828w5y+0UJPmBvi0j6+FeJy/uQyh+edi9vyU2yyd9tE5//iVi/wM1Rp8Aw1Von+yGHyzerHFKUYyzy922aq4rGQ3Na2yg+tNAVUJD98Izh+QnkcqPjAIG+rSKYndSsy8XjvWnhj9KqALf1Pvdw0dv9FtHpfISgR4jenNx3j71L8qqfdZ9A4o+GFdAD392hr84qFXwutc2RXQG3A9vuavUy8Uo9o3Mr63HldPUx8J7suGwasJmoLfD0nj/GWBI9pV9Fm5qDWbh/D125+PlBlbX9ZdMqP1klFNbh8Tr+5K9zfeIZbzTy5PtIXe1+KCvOTS//3PL6gTlXS98MCSkWioboC3j7fRjX9MKktM+uquY2KS6Azh/wIRnQ8gR0S7EdFPADzepHo1DBFvH8M5po1WGt3kbowGI/bvHVXMJ3v7NKDsRl0/FGha6nl/fnY5fnz/QmW/YpkOGCq7fn6b5nbqOIj2rWUpbVodBvcNNH+zEW+o7AYuxw6p/Ux+52VN+Mu/feeu+ao/u57P36D5q/05uSOFtI/5dxmmSYWl70aBrk0+4oy08uL2vi7HuHrK97tr7grMXrou/E360bQnb9KEmpXuM8EUF5G2sVE99qmsqEb4XwZgH3hpna+H5/r5uSbUqaFwdNonQXOT27nRxt842kAM6J6CE8Mdi7+116feJ9E7ZymGHy67ruZzbaB98g7yjlO1AG7Mvga1D6gku5Bx2a5o/oyCIwy+pGnO4TWBwZei/LjOUUcNvtp3TfPXc/DoKMcoJyaYxolM5yStYKv19glWQdp5squnaBui5HGiav4m2ie+HqZnKmWMVVFon0D4R39XaR9RbgcIf2buY+avMvOh/v+viWhfAPBXAh0HXfP/778tiHQQ02YujW7zOG8RsWTtyTtw2dvP9yu3zsXqjYP++ebrqkG1/LqOiPD366wL0YrLipByWaYbfK+XvAPHQPu0AvW4eiZNwmZXz/Bz2XUDIaYHeSmav6tq/vJd9SpHAsJ0zd/VOH8hfGOevZqdvEyTl0z/GBV/rf3SaCgBMRHK7+yuuctxw5NLgu/y9ohJrzbN4JspPkGCKUo4KXcYIHH5BjuAyQOoUwy+aThKP0BEvUT0JBE9S0QvENG/+8d3IqIniOhlIrqRiIoNrIcCnfO/ZfYyDJTMWpTczI3QNGXEcXiiAxXzDlxm/OYfi3H9E0tw3eOLlPPqmYz0/lOtNqFrmWKwxT2LPLBl4SBon5zTXI0mDkJe1hLklRSnIW6nuhWrmn8u4PzNnDkQKgKhtms+Ty4zWpo4X7/efF5Qx3L23D7G+IHg/mZHAN1bKqtSEwh/6Z6f/v0zuOHJpcH3QZ86JSSP2zRXz0Tax/DiTY4Dpv4hB3mJIhSbhYn2CSaJ2CrVjUYKfxMGARzPzAcAOBDAqUR0BIDvAvghM+8KYC2Ai5tVAZ32AaLLtYCPN3CZjUJIOahli+/FnCf85y7zbOrbjB2h1KOe6ujaSLUeBHGaf5IRDvDaU9b0hgJXz+y0TyMjfBvC+RsHt/9OFc1NPScXE+SlrhA4OEcvSxc+qRG+rCV2S9G0S8Hklt425tw+CP4m2a6C8wyrbROEq3bSeUIZ0Y3p3jHzNUbNv0pvH5Pmb+TvpbIC2keqqFjFyVk99RVVM9BU4c8eNvlfC/5/BnA8gJv94zMBnN2sOuQNwl/3qzVpdY2nfdSy9LoU8w5cF5i/XN0fR5w9WPLooLc2DVZddiTSscpni7jZVaId2Dse3YyEg8+e5t+Tc5Cj+pazjy58Exde+yQeWbi6quvqMaIlpSAOfLcNxwTCCN8kzd/7LATeV26dG0SEmoS7Wr9ofc3cfKT6AGTax/x7XFmhLSv5/rowy+rqKSbCpHoN+imz5fub6xD+Zsrtk1SG6b66Ihd3XsXwHj5/0xx86ebn4LocyqNhTPsY51giyhHRHACrANwL4BUA65i57J/yOoDtYq69hIhmEdGs1aurG+gCeoQvEF2uGf38G0z7BFZ+rXMMBZx/Di4z1vd7zTIwVPGv885bsGIjrn9iCb5914Kqy66b9lEiFDn0DMmgjcrLfRHhq2+wkwVylf/6/HI89NJq3DW3uq30THxrViRdI9onzs8fgMb5S8JfWf77tI9/o+XrB/DnZ9/wztPuF60PR77FuTmaEGfENyE1yMt4D3UJm1WzNXH+OmQaN+l2aZx/UhlG4W/k/KPXqkFe3t9XV2/GjbOWYl1/KTEleEcJfyLaiohMu3r9yHQ+M1eY+UB4G74fBmDPrGUx89XMPIOZZ0yePLnaqgIw+PkjmjpXDJK45XgjEHB4WocRHajH5/wLvkugcAHVh60pG2YaNRLli6t7OJ0OC1w9tfvqGrVHAYh7hLl9ahH+MsS1uu0m9Tq/vvVx/oaBmjB4BfKOCPLSDb7hZ7GikqnKYOWUQvPo1dLTOyRFKAMS7ZPhvZj2x5Dvb3KCMZ0HZOD8DW6v+phO0vzV1Vj42az5Jwh/wzNl3XlLDgI0KUwmb596nBOyopr0DocS0VwAzwF43jfiHiJ+Z+brkq5n5nUAHgRwJIBxRCTyCk1DE1NDm4R/rOYvtXOzInx1ARlw/r63j6itWO5HB3X03mm0eJZ7JEF3VYvj/KObkcicP6vCv8r2lSe4UPhXYs42w42h3rIg1Gyjv4VBXoQf378QT7+2NnKeL/sj7ogmg6+JqkzycwfMBl+jP76h/iy90yxNY0qhIK8ATLeIBoNF72WCaAr5vCljepRzZEGeqPlLNbte8hYSqFrzzzDpA6HCxxxjmzHdRwj/DuH8rwHwaWaezsw7ArgUwK+TLiCiyUQ0zv88Al4a6PnwJoFz/NMuBHB7lfXODBPtoxtqTFpdo5tc3DpqPBWaf8777v8uNP/ooI7WLC46NrhGK7PaiU1f5ovBFhH+Bu1UNvSVKly1wTess1QfrY2yoi4//wTNWXbV+8G9L+G9P388IoTyjpzYLVonIHz3JieFCO2TSfOXfxeatrn+4XtKbxs1n7+qwacbfENlQLkwBoICk8fsxNGq8B/MKvzZ/Fkg2V4QPWZcPRhWCLI3oYm+ExOv6uop7tc84V9NSucKMz8ivjDzo0RUTroAwFQAM4koB2+iuYmZ/0JE8wD8gYi+BWA2vImlKdATuwGetm3qwIrwb7CLVVxqAaHtFfNeRUVHEJRG0ibeAmmafzpfnHK93FZudm+fihsKmwp7MQCFKg2+polNDKZqNf96ltKBu17CfVV/c/VM1dtHbc+gDOHqKT2z+JQWq2EK8jIpM6ZHT9vjWoeqDITlieuTYiH0SSBtshHv38SvC4TePibNOvyc9mzJQV51aP4JlGHFDTV/2dvH5BXUaKQKf3/TdgB4iIh+CeAGeH3p/UjJ6c/MzwE4yHD8VXj8f9ORc6LSv1xhlcowRD+2zM/fFZq/E9QNkGgf7T6mDpzG+Uf3dzWf99TiNbhv/kqs21zCpDFFHDp9At6xx9YRv+64gCB9gMr8r/DF7qmR85fPDjX/2jIzVlzGj+5biEN3Go99th2Lb985H994194Y1RM/HEK7UPzgFn8LuWjiunzg5x8f5CWuN1GVEbuN//3NTYP4/j0v4gOH7hCpr4nzN8kSPTAvDbKQNHH4SfJKp3/kc++auxwbB0p4v/QsQnmTPWv0vjMkCX+9aFNAWhwq0hj91p3zccERO2CXyaMj9xH42m1zccelRysrtaRJIs6wa04MWLuikhVZNP/va9+/4f/1Yio6HDmDYCxVXKUzmQZ2oyfcOMpBeFkIzX8o0PzNnL+pM1DKqzBRAia87xf/0I68gsVXnq4KKOZA04rSPlHOX1w7IJ4zJqtn3ASWtGH6YI20DwD88L6XAACfPHYX3DhrKXaePAqfOHaX2GvlQJ2k+wLAiEIu8jZykqundx/2J4LwTN3PXynD1b975377rvm45ZllGNNb0Opk5vxN/aSUsGIxwTWME1mjN8krfZLQJwHAC94CoAh/x0D7uMzYbevR2H/aOPzluTcU+iXJuSHt0cS1r63pw7WPLcKDL67Cg//6jkg9BZ5ftgGvrenDTtKOXKbzglV/TFyAKcK3FQbfVOHPzMcBXrQugPcCmC5d1/nC32TwdVnpTOGMH57T6OCKOGOjEJg9Gu0Tz/lH712tt0/1rp7SAHLlzb7V80zePuKIGKCFHBmzesYhaTBVy/mbnHyyeJ2YKEJTfQRG9eQjKwSZ9vHu4ykm8ml6YjeljBihZkpKKOopHwo55MitNcEa/T2pLnrCQpNRUy43QhOliJCQ9lE1/123Ho3vn3sA7pu/MvD2IUSDvEyrn/cctB1umR31MdGVwDiXXBli3AqYVoZxNKkoQ1cs5BiNTknsdhuAdwEoAdgk/e9oGCN8tY1ITHl3Gt3kgZuhNvpKkqsnEHaAUPNXayL//qWbn8ObmwZrEP611V3cK8zt42LzYBlf+OOzWNc3ZKR99PoX8znP191l/PPVt/DTBxYqz5VWd/lY1d4+hnsteasvsXxAfX7TebpgHlHMRdpY1/xNftyPvvymd46hz0ajtNX66PViZk3rFX08XjiZyjHBtIG7rPmb7iCOXv/EEsx7Y0MiDSVD0D7f+esCLFjhBUBWmIM2yjuk0FZxebu8OngomAyBkLy2TCuvmIpG7FyG84TiY07jELpvm5IHtpv2EZjGzKc2rSZNgp7YDfB9ayvRpaJpOdsoyLEErht2XjnCV0Ycny069/zlG3DjrKU4bs/0+Ae9/1Sd2E1bJYVBXsANTy7BH59+HeNGFmLSO6j38rJ6eq6eH/B37PrM8bvFlm3SwsQg6h+q1tUz+twrN3q5CZM0UJNfu/K7NumNKuYN3j4h5+/dJ/pcAqY+GzXaJ9s7dK+buEkCUIV/piCvBGcJZvMEItf//Vf/I+r1EwOZAvvItU/hn185Aa7LQRvlHFJdPfW6Km3AIAIKebO2pDdpUsR2cH/DaldH6B1nDizTM6pWDH29GahG83+ciPZrWk2aBJMWVa6wEuhlmnEb7eevuvSFnaDsuiAKXQEFBmP9/FXBV3bZyBHLMFEC+m9iE3kT4jT/iqtu0B7JmWSgAAo58rJ6Jnhv6Pfw6inVxz84YHC1S4JJK8uiWSkadMJOcAKe5q8ey0kpnQFZU46WJ/v5i8nC5Enl1ccMPcFaXEploHrax+RBI9M4alBg8INUdznhX1pZ5olENFHeoRRvH3UCJMRr/kkTUlw9I1Sn4cShSrzm7zIHyoPJKyhtkq8H1Qj/owE8TUQvEtFzRDSXiJ5rVsUaBVPATKniRjhEQH3p375rfuoE8OzSdfivO+dlOu+2OW8E32XBJ3K96zxvyPmr9xbfNvvCP4vwkvvP1Q+/gvvmrwq+i+tXrO9PuF4SDq45wveaRxfh1dWb1es4avzryTvIGfaxjVuNmOgRcWyoHN1H+Cf3L8RjPn2iw9RWpuRcketSVoR63Ucahb/3V6d9TMItC+2Tpjl7E6/6HUinfao1+IZC3H8eV52Q9N/F5yQaSi0r/Cz2Qa5IK+dcTtX84wzjoiyHKJ72MQhf033U+mnj03DaYCme85c1/6B8xf23eZp/NbTPO5tWiyYizuC7eTAaEi6/m9vmvIH/PHvfiBeFjPP/75/YPFTBpcftinEj47NSv/tnj6nlK8Lf9bRhrZppEb59Q+XgXmkD1lUmNTU3UIUZeSDIKWRCXJBXGn0kG3wFinkH+Zzq6lmuuLH+zKbgKrn9BssVjCyG3fj793pePIuvPD1yLyNf77qxvwXnaBvUxNVRYGQxF3nwXJDewftuEorhudE6RGkfITzN0HPsJCVSUzn/mBsayvbqpY4dfcJ3mZEDRSaEzJq/dIJQ5FwOaZ+844SaP6KTiU7ZESFIoRJ5Lv9ZsvrvA2b3Zh1x+1+IMgPjvYH26Yj0Dsz8mul/02rWIJgjfF2s6xsKvodLaLWh03LH9PkCevn6gcTz9Pen0D4Vb3Nv3cjUPxQj/DW+27NfZNOexISh3i+sRxzkn+SIxLQAFFPATzHnGXzlTl2qcOxEEtAjMR431eT3MQ2kLJyqKahJva/6XfflB+Ssnhrn71/7kbdND841unpyjFCLqb7ralqvON0onOSVXW2av6gIa2UYVzis2giSIL8zeT9f8VmOGTGldFYpKu+cOM1f1Ns0nuKaJZPBN8nbR9H8RVkdJvyHK8y5fRhr+0rBd5PmD2Q3KK5IEf76WFY0f5d9zT88ySEpwjcSoOV9F7RPyXVTBZi4xjRJCVfTOPpDDxZiVjdwT6JNXI6uSkyunkMJmr9p43R5QMS5e/7k/oWRY6Yysiyr01w9TRRWlPZRDb4h5+99kA3+cp810V7y8TjaRNfAWfsrQ3V+MN4OK9YP4Is3P4uhsmtcUYS2GdXIb7JtVKRzjPx6jIFdCG2XVW8fGVHhr9Y1ifM3pV1/dfUmfPXWubHJAE2xLTri4mLEsdDbJ7qi7hSD77CEMaWz62KtovkLLURt6L5ScvYKcfobCXw5EE2/IHekcsVF3nGUpf7IYh5DFTcykOQy+30t3pRfRId4vuXrosI/bXergXJFKUP29qm4nDhBVtzoYDTt4etx9+Z7mOgR+do4d09B/8gwDUwh+JLsNkr6A9PvhlxRuvCX8/nL5YnzZCpC9vYxOSMUctE4Cb1eLifTMzKGMnD+X7/9edw063X8/cVVyrsyefsY31XM+zMVF1cfOb2z7O0jIzHIy79HLO3jnyu37f/747P4/RNL8Nzr6xOvCcs3PE/CGK2wgfOPUXQajS1e+BtTOldYo33MS9C+BMEma0smoSpDn4BU4c/I50ihfYTPf8WN6nWic4u6DWYQ/qK85YZJKtT8zffZPFiJDEZ5GZvURiaDbzHvoJh3lHuWKm5k0OrCUREo0udq3D1Nj6gvuc3XRTVdGaY4Cv0sU5CX/Ff29pINviYjpIiT8OoD5a9cT7OrZ7T+pRSbhn48zc9f5/zl83QkceSA2vaywTdn0PzJcD89UMvj/JNpH7nMgv9eNmuU6ZdO3TNybpbn0eG60YA9xR7WRG+fagy+wxLmnbxcrO0rYWQxh/5SJdbzom8wXrCslnbUStX8ddpH5rtdRiHnKBNEbyEXnhej+QuhmyXQKRT+0UnqFw+9giN2nhhL3/QPVTRvClY2/jDZEcK6mjh/B0XNQ0MPuvPu7UXAytz4owvfxDNL1qJcYYwoeO9Ofv40rysT7TMYBOAkDFCDEFXuaxAA8RG+6j2NtI9B85frLvPcOn0k18EU3WpeuaTTPuJWekoK/TlcTfMPTRMxk4rhWFy6BjGWPe7eO6Yrd3HKkvjNIUI+JchL7gu9xVxQpgxZQVPKM3SjzJq/1pZURQLEWrDFa/5GP3+XsbZvCONHFpGj6EASSBJs6/tDm8GGBE8ZQM3WB6iaQKns+jx4+Luc6iGO8xd1y2LwFALuTcMWkP/3yCJcPHNWvOY/VNYGI6rQ/M20j655mVw2xQCUA18+eM0T+MG9L8Fl9jxqoA4s/R7R6OjoQBI2g6QBKk/WJs3OlHpBv19ckJeok6yk5HIG4S8bPincD0H0D9MWmrqxUy5PRhbaR4bJ9TX4q50brOBiab3klYgq/J2gfNnbJwBFn0+nqAhAMYX2kT2pRhTMIrLHP25KY67DtGtYcL4b+vnrKWCKOSeTK3Kt2OKFvyla0qN9Shg3sgBH2lhEf3FJuWPkwS3vJGREgsG37Hqcv6z5F2XaR3v3usE3tWyEgkE2cuuI66B9muYve/u4nDxBmgy+gvbRy47jauM4f7E6UoL1/HNG+L9FA6MMA9N/trQBKmCSjaZdtvRJMS7ISxQrT4hynxXPLfcZx4nSPqbNXYzpHQz1zxLkJQ4TzAZwedMeIy1kvq3xhzjaRzSL7u0T/I547zggXDGkBXnJAl30JR3FXIzmbxL+SZq/G2r+ZTdUqgCPAbCafx0wZHTG06+twQMLVgWav2kbRwBKLIAOVfiHn695dBHuePYN5Vx98aFo/hXP24cMtE+pYuL8vb/9Ae2ThfP3LpLtHDrivF76hspRzr8cnrtxIFn4M1RjZiEX1fxLlehuRmGyq+i7KbsspcCOBvj0FtQMqQJJA6lUcfH3F1fhqgdfjvxWNgizpPuaVkR6kNfKDQP4lxvnYLMfWV2I8fb59l3z8fRra5RnMe2EFo2uVqkn8ckk/OU2/MVDr2D2krXRk7TnA9SIWnmSNnH+8cFo0eODirIRHh8KlA7J20fT4vX3E/H2SaB9As1bGgtikyUdPTEKRi0G39Dbxzsm5ENvwWkq57/FC3+T5v/gi95m8GcduK3Pn3rHdVGbpNWKF9pbcJTOev0Tr+Hqh19Rzo3SPprmn3OUCSIQbK4bS12IumXR/EVd1yYIf502OHT6+OBanYOVBVFSWohB3y1QHkDFvKMIOlFGRPOPWQoDnp2jR5ogBcRAEdpaqawPzAThX2Z85NdP4Xt3vxj5TR3g0XvoaSY84a+2i675f/OOF3Dr7GW4Z95KAEBB6gCOovkD7/35P5T3o/ZZUx29fqLHZ3j3M9Esav3f/bPHI+fICI3UFFmZMavjSM/6qcN0NC7RnOxlltnbR5sAiRJoH4PmH0eHhmM0vY/pxmLlfNeQ28dq/o2BMT2u36DvPXiav62eqr0IJPHZg36nGNNbUPLKV1zG/OUblcGvV0GezUsVRt4hZZISgq2coPkH3j6ZNH9f+G+Op330Ti68Gcoaf+2ymv89SfPvH6qAWTVmCoOvXnac5m8yxvcNVZQJMqib0PyFPcDAg8chkfYxGDBlbBpQ29VE+4Scv/d95QbP/jJ+pBdBLmujujYLqCszz04lXFSjvwNeW5hcLk1NMJSBV5a9r8R9844jpWmA9LtUD0H7xBRhak+F9tGEv0h3HPr5h+1mDPLSKKqk9A7BvrmyoiEpV/IKNs7ga1rhJK3OKxwGaYoVmOj7PXnH+vnXA5PwBzyB7DgUeE4Mliu44pa5yjlJwl8IxDG9eUXzFxTGc6+vx+wla/Gtv8yLRO9G0zs4RlfPsoHzF9NBn8b5b9Ub77glBFsS7aMbloS2LtpGQOb8gWTN/955K7Hozc1KzvNizsz567I3EvIuDQJZ+JsEhdD8ZYG+rm8o8n71OsQhbZtDfQL0bCE67aMafFf52USFli8LFlNsSoT2Cb6qfHFYB9XeotNoV9zyHD428yn0D1WM2u01jy5Svos7ld2Qoss7FEy4Mj2XGuEr3zeFI5ff+8JVm/Dbf3pJBUKDrx7kFaXgwt88u0As7SMmUqktZcHtKGM0FznXK89460ifl+un27eEfOjJ57y4iSZNAF0r/IXGIPjTh1+KJgPrz0D7mDR/AHhmyVq8+2eP41ePLop0SN3PX8/tE7h6Vtyo5u9f2jeoevuMHxWfW0hQN5szTGYCRWkCGiy7gUBlA+2z69ajjfc07UzlOFHNy0T7lDXNdp1krFYMvgbaJ/hNeqZfPPSqsY4CpQReNi7iVECfAAXt01uIavPiPYt6i4m1mDNz/gLy+3EclWYBTDlmYjR/9j7f8ORS3Dd/FRa/tdmY2uM//zIv+qAQMRnhM+nZQj3hL9cjfsUh11951oRcQ9+4/QUAYTvKnlFkKEfxeIJI75BC+2i5owTkiUa82ywGXwAYN8KcI8xlDuw1YZBZSCl7363wrwlx6Y4dJ/zdddk4ALIIy600zV+8uGdeWycd04W/qvnnc2pWz1Cr5UjvD1w9S6rmH5dYziE1l9HomH1qN2jURehx5GKo4mJEMVwJyIJoqOzi7btNwkePmm68LwBs6FfvHTX4RmkfnaN+a7PqphoMDCUvjfc34PyrcGFM0vz1KFEdUc3fo33kthb9MBrw591RbpMYfSWA4p7sH4tE/GqcfzCZQnc4cDPRPsF9pESCOceRhH6UFvKOq39lFPNOTJZRddI6dPp4fPCIHZRzTOkdGOZ4h7Au7HP+VdA+kuYvj1F5PMSVJ2PcSLPwr7hhH9ZdTeXVdzOwxQv/TJq/y8bB3zdUxrWPLsLtc6Jbvg0Gwr+gCX/vRckeE3oUrrxULLvR3D5JEb6ib/Vp3j4TYzT/nnwOJSmX0dZjeoznrd2sUkJiUJUrnrDvzYu8KlH7QE8+ZwymE9ioacb6ErhkyO0Tcv7e9zVa/UzLbnEPofnL7Z62dP677wRgOjctwteo+Q9WlA3hdc5fQNQxn0L7yHAcwor1A/jcH2YH3kIPvbRaOUf39pEnU/n9DZXdILNsEsSt5Ila3qhedumUmy8udQrgKSJL1vTh/930bEShCK73efpiTvW6Mbl6es+slqHTPk6Cq+eNTy3F9U8sUSZSOYhQvi7OnThOxxg3wjw+XTeaKFHf3a9ZHj9NjfAlou0B/AbAFHj94mpm/hERTQBwI7z9gBcDOJeZ18bdpx7ECX853L7CjA0Gw+WazUP4D3/5e9aB2ym/DQmufURe5cQrjFHFHN7anI1fF7l95PEe+rC7Rj//ciX0wBGxCB84dHvkHMK9vveIQDHvYKgc5jKaPKYHr76p5t0HgDWaPUAOpR8quxjt2xQ84aFWqifvIIH6j0DXvErlaFZPXfPX34/Q/BUtscLab6qhOitKroseJxQ2Sfn8mRmbBstBxLGoe19JTTWtp3cQEMKlV/InNxl8ZeSI8OTiNYnn6LSPvE+s3GZDFW9vi0LOQakSv9IVV5Rkzj9HRo5fVlnCYL3oPUf15PD8sg14ftkGnHHA1OC48t5cNsaGmDh/U0I93e2UQLHt+49X38I/Xn0L/3nWPgA8GkkW/jmF9pGi8JXyzB1tbJzmz2q6FHn7zd6YCaZRaLbmXwbw/5h5bwBHALiUiPYG8GUA9zPzbgDu9783BXFaVF7SHFyXsc4grJNSNYsXNqa3oPipl1wXh0yfkFinsrLs9nL7mDR/z9tHE4rMAeUDhLTP6J48vvve/SNl9fh5dARnPmWrXmOddM1fdPSyvyrq9TVtz89f0/wLTkTzT6IudM1r0OTtE7h6en8jZeaj1I5u8I2LFE2DKVEb4PcV7T79pQoqLmOrEaGgd5nRP1TG6J6oQNf7o9Dc5WAik3uyjDiFRgZDNT4G9AuicSZerEk2UVCuhO7HYlMeuUl0zl+sTE3tP7IQtplsN9PpOocoQssGm7lI3j4Vjg+KFHVzEmif4BmlviivHuU+Lj5n8fMH4jn/iqumZHc5jNmIcydtFJoq/Jl5OTM/43/eCGA+gO0AnAVgpn/aTABnN6sOcXSEkg+czdGvsvDXl62Bwddf2ss+yHtNHRPLrQNqVGqp4qIY4fwlg69hGSsnMxODS48VCO5VcBTOP472iWj+Tkg9DZVVzr9UcRUPnp58LpJGY1TC80donwSDb5zQNgV5iYEo6pqUUCsJccI/70RdCTf5K5Kx0uCuuN5mQXIbhH7+6vWCvhPpKgBzShIZmYQ/mzf5YY32+dptc/HEojWxtM+ydf343B9mB8K5XGEvvYLjKSz6CkOP8P3Sn57DnKXrjHSI/Bwyt375H+aE44m99tDtb6K6quYfjdWpuIw/PLkEv/nHYrjsGXzT2i+W88/Jwj8UzNc/sQS//cdivw7Vcf46DVdxw3QPPTFG5UahZZw/EU0HcBCAJwBMYebl/k8r4NFCpmsuIaJZRDRr9erVplNSoQcUCciJtlyXjW6QsrapTw7iNzHAB0oVf2AxenIOdpo0Sjl/z23G4PvvOwCA6llSdkVWz/BcOW+IPmjKFQ60RSDUmPL+3rg6hLuYyOuz9VZm4a/nJ1I0/7IbTGZDZc8ALNMUPfmo5i9Pfj/6wIHKb7qgSfbzN1Y3aCPdOAiEy+W4BGFp0KNlQ47bidxH2DO2knZ8Y/ZWBKNk2odUV08BEQ/S22DN35RaQ9RNbrOla/oxf/mGWM3/a7fOxW1z3sATizyaqeR6brk5oiDCV08PIZf6whsb8O6fPWbk/OXH1FOpvLxqk3c/l5GjaMClifOvuBzdxpEZX75lLr5x+wtgeAZf/R3oCCgyqCsSOaZATASuy/jKrXPxdd8LKd7ga+b8dddpOd1DaNcaxsKfiEYD+BOAzzHzBvk39nqF8emY+WpmnsHMMyZPnlxT2T0pwl8YfJOiXwHgjXVq5s7Bioti3lGMi6ELnBOZ6S86eiecsu82APQgr2hun17J0KM3TKniKj7kIrpUDxQTEM+/csMginkndltK3WgphPlQ2dssRgQj9Ze8XD8jNOGvCyShyV5w+A54x+5bK7/py+6hsgvd4STJUAh4mnTeIdXgqwt/zaslK2I1/xxF3ofQ/LeSNX/2XD1HSbSPntVTQLzLEcXouTLk1VKaQRiIBnkJMNjYFrLwP2DaWO85XA6EsugPpbLHSzuOVw9mVjOrIjrpMJsHuPwcmw1GcyCkffRUK2Ti/DnqILFqY+glxuxNOGlzZ1yQl/xe8pJyJLB8fX887RPr7cMRA7fo04ESWOMKNg1NF/5EVIAn+H/PzLf4h1cS0VT/96kAVsVdXy/ihL+8rV6FOTHpGRDl/4fKLnpyTnD/wXIleGk5hzBem+ll7fjbdy0IJhPh56/QPnJuH4MXjCz8hwLhHxXAolzACyoaP7IQy3dWXFYGhbiXiHUQmsvmQS8oSBZWvYVc5L6yN1Uhr9bL5OqpL5fl5HEm5B3PcFc28Pqyq+fazUO49PpnUrfaNJUNAG9tGsSnr38meCb9fQg3T5niEd4+ssFX38ZRQIT+K5y/4T2Ol4RHVs3fJDNc1zwRyquxEVLG1H6Ns//hfS/h6dfWwiGf9nFVrb3ixiS/S9H812njT2j6FdfL41PRVPpAedMyoF5xy3PKec+8tjb4zP5EkjZ5yq6X8qpGybwacP5hvZ55bV2Cn3+Mtw97mr643+dvnIOXVmwEgMDONiw1f/Km52sAzGfmH0g/3QHgQv/zhQBub1Yd4hIzicYu5h1fm052V3lLS4c8VI5q/qLT5B1SBquohyz0RA4ZU26fuAASwNMCTHXVjcZ56fkATwMaP7IYG2kIqAJIXC8mGqG5bBoswWVonL+DC47YAecfvgOO2NkzdgsdzCFS0+4iyvkPmRK7VULNz4ScQyg4asrbcqD5+4ndyi5+9eiruPO55Yorp47txo1QvsvC8aoHXwmEWSEX5fxFam/ZuOu6jIFyRdXmfSE1WovEFntGxG3jKCArE2m0EBCNtA2Ow7z1ptw3xaQ1VHED2kN+PbNeW6vQPrLm3z9UNpYrX3/V+QfjO+/ZT6Ff1muxIH1BuV4en2+9ez/ld5O3z1ubh/DmJnUF//racMXeN1QBAZg2fgQ+cczO+N45noNEZCVaUffKMO0dIMqXDcIrNgzErlTHj4r38y9VXIzy+8o981bixw94yQWHu7fPUQA+BOB4Iprj/z8NwJUATiKihQBO9L83BXGGLPEiews5DJQqqTtimTj/nryk+ZfCvXQ92ked6Yt5Vbv3BInvaeHoO3mFmmuE85cMvrIAzjvq/YVXj7jXqg2DGDeykOjVYaIexCAUBk2hoSmcfyGHkcU8vv3u/TBxtGdTEPU2bZtnzOevPWjSvqfivvkcGdP/ypp/kqAUAvfYPSbjqF0nBsfjNK18LurtI+hCmfMv+Rpjbz7K408dq3pbbR4qwyFtRypDlWXawJSpVofL0QkVEJx/Mu0jG8zj0po7ksFXnLPNVr1Y21cyrtbkld3p+0/FeYftoKQ7FO0oNrIXk6LLXl/cbtwIXH7Cbkr5gOrtY4KcVG3N5iE45I21K07bK4hM1/unoJgEFSqcOuTx5Tje5CdPNgOlSmx6hzE9SQZfNrqCyl5/zUBT/fyZ+VFEt7AVOKGZZQvEGXdEp+kt5LC+vxTZEUveLQkI8+IMlCr4ws3PYcX6fhTzTsDLDZYrilfIGE3D0wVeTz4XCBl9Jy/x0i//w5xIzp5yhQPPh61GFLDa5zQ9u0F43tSxvVi2rj+417J1/dh/2lgU8/HCUF4lCa8IMdGMKuZRzDlY52toOucfPKdmiCvmncg7MGlaOu0jXFjj7LQe7aOmvA1oH194/e35FUEGVxN6/RiIgkNKneLsA3nHiXDKol/I7zuYnAvq5AxEXW0HSi56C2ob6SslQNX8Tb/r0BOsCcQL/7D8kf67/eRvn45o5AIOeZOQzPlvM7YXc5auC/LvyDDFvcj9NVxBee0oVrdi60W9juJ1JQUXAmr/WdM3pEgjMeb0/rlJS50ypreADQPliCDLO6QwAhsGSviXm+YY6xEXWzAoKVdLodoVh7vm37EQnWZEwcGgr/kfttMEHLD9OP+4ShcJzeSBBavw52ffwFOL13rCPy/TPmG0pm5r0IV/Me+ENFHOUbN6SkJYD24aqrhBFklZ4Oj7AE/xNUy537z34GmRSEkZvdquRTmHgkFYzDsYUcwFg1ReJSjC33/O8w7zaKBPvWOXSDk67bN5sBLV/A3+4fIYdRxCwSEtYE71kogT/E4gTLx6jB1RUOoka1qyCVFOYSwgtgOVPa1Em/UYqJzeQrT99cnQtFqRV5JprqBANMhLII72ySu0j1fHWa+tjXDxQR0DzZ8DIbntOHMMicDUsb3473PCWBQT7SP6dBAwx9J+vUoKjKi3TxrWbB5SBHgg/KF6p23Sxpyok95qOYeU3fEeenF1bO7+uElKOGyMNcQBNDvCt2uFvzwY+0sVDJZcHLj9OHzh5D2C4zIE7SO/3GJeN/iGmr9ua9AFXt6hwKWwEOPqGQfhwSB77uidaxtfwxSTFhFw4t5TEsP4ZYEu7tknUUwjizmsD2gf1c9fQLjWjurxaCBTp9brsLZvyKD5R4W/IkzJK0vdf1bl/OMgNGchcKaOG6E8Q9xgy+ecyFJEbAdqSv4m3zMpareo9RWTYq8YfDPIO4ZZ+LtsTmJXVGifdEJA0CcuhyudbbYakXjNyXtPwbkztpfuEf4mJhnRpwX1IufuNxlc0zR/Gev6SsoKO/hIqiKl598XE4PenjkihfZZ4BtqTTBNUkShXcEo/LcUP/9OQ6j559A/VMFg2UsTLDqE3qmWre3Hh699Ep+7cU5wrJgLaZ+BUmjwzTlORJDq2t1AqaIYiFVXz3jtHAhzwY+Rc8do9xfcsnChmzjK4+Lj4h6A6GonJwn/oi/81/V7nV2eHGVhK2ifpCGpD4S1faWI5j9gMDTKwjSX87ynbpvzBh580XMW0zn/tPLFZL3N2F5lcr5jzhs4+6rH8OAC1QnNy2XjfV69cRCf+O0svL6mH+NGFox2AnmyMnmYiD6mrxJTDb4Zg7zMnH+cq2d4z1HF5PYTdXCEwden6NI0f51ekTc5EvTZqJ4ciEIvM889k/w61qf5e3UIP8ua/8TRYfvqifrExKDrBDmN9kmCiarLEQUrXJPwH9bePp0MR9L8Nw2WAw8W0Tf0TjVv+QY8rCXPKuYdTPW1nVdXbwo0xkKOcMo+22B/31/aO6ZRHUMViSZylAjPNM3/zU2D6Mk7yoDVNcsPHrEjPnTEjvjM8bsCACb5nTtpYtFXO3mH8NJKT5vZZmwvRhbzgbasG3z159T7+jfO2Bs3fPwIAFEhsK5vKGIoCzR/qeMX86FdIydtyvHRXz8FIBT+8nkm6BP71LG9yvl/fm455ixdh/vmr4xcJ2ignzywEHe/sBJPLl4T0fwFegoOLj1uF7zrgG2x+5QxwfGZFx2G/zhrn+CdRfLWGCqvGHxj7Fh7bhOW4eWAMtE+aoR58GwGg28ShMsky5r/2DThH/9dBMvlHQcjC7lA6fD26/XOMQn/LJr/pNFhYKOaXlzUg/DzCw7B7lM8A7AeczDaX43omn8+5wT2t8kxkfOijvJ4+PoZe+On5x8Ex6Fg4tzKav6tQ14S/iG1kQsEk9wx4zShYj6HsSML2GXyKDyzZJ2SA6aYd/BtyT1NH+D9Q5VgEBZyalxAmua/ZvOQnxIialAM7lHI4T/P3jfwPxcDYGTCwI4I/5yDjQNljCrmsMeUMRhRzIXePvkYzj8v0hio9bno6J1w5C4ToaMn72DtZhPtE7r6CRRzTqDV55zoJCK3f5JXkz5RTh07QtFCRQbRSOBbzgm0P7le40YWIisXwGujL5yyJ35y3kFKHMCxu0/Gh4+cjoL//rJw/lk0/++8J+xvrmve4jMb7ZNB+DuQNH/vflPHJtM+ep+Qv4rmyzmEEcV8IFRVzt9A+2TISRQXYOVIY337CSPxX/541d97XKoW+T3IE4yOrUYUFM3/PQdthzP23xY5okTaJy55XKPQtcI/5Pwl7roQ0j5yR41LhiYGzME7jMfsJWsDQ5qcNE5A57nvnLsctzz9uv+bo3COaZr/ms1DKGgpFeI8QMQSVjzDyJ7qNH8AOGD7ccjnHIwq5oL7jSiqfv4ChSqX4ZNG92DDQFmhIoo5B/9730LcP3+lRvs4AR+dc5yIcBMCWBgj46C7B27Vmze6V+qGv7xDmLd8Ax5ZuFqh/sePLJppnzTbQ4zmb6q77CceJ/xlWsxlNrovP7t0HR59ObpxkUr7pHP+uYDzZwz4glp3Y9WhV9v0nDmHMLKYk2gfNu52ltXbB/AUnh6DUiIuFUeEQqMHBAqPO31+F2WP7sknKlUTRxXVGIFcKB8GEwy+QoG58NonI4kXG4GuFf4y5y8g0z5ynzrzwG1x4l5bRzYsERPHHtuMwdq+UmD5F4JY7qxiorhS0s6+f+9L3vk5R/HgiItKFljb52v+SqIp8yA4ee8p+NARO+Krp+8FAErUqQ7drVR02B0mjIxcu8c2W0n1NdA+GQKRgHC5LCeWE89/8cxZStCM53EkIoej+xfLmr+eWO4rp+2JP1xyBP7tXXsHbfWDcw/A18/YG0RkdAneOFhWqBOhZX7omieVSWl0bz5IxiUL8rgAQwExCenvW6/Kew+ehn22DSlEMUnrGq1cNrNnNzFpvbfOju5PURvt460khPY6eUwPLjxyx9hrIpy/oYvkfeG/WaJ9QoonSvscuctEnLLPFEwbH7/q6M3ngjaTywxX+b4sKJrHXZzBV9RhdE8+1s70iWN2xjUXHmrMCOokGHzzDinX6MGBjUDXCn/Zz1+gtxDSPrIwPniH8fjVhYfigsPVji1e+LZ+hOjStX3evXPRzioG5gcO2wFH7zpJuY+uLcctZYVw7huqoJAnpRPFuf8J+meCv9mLqZOKSWqCtiGMTI0BqlA4ZMfxkeuBkPZJk/0iu6hYLssuhbJRWhayxbwTpAF2pCWzQCD8ibDTpJHKb5ccswuO2HkiPnrUTsGkdtSuk3Dx0TvF1nfjQFkpQ00xEFasN58LVh0yfZI2iRdiNH9dSH7nPfsp/VQIowmGFCICQvM3aZTmukRdPZMggryYvfw/eZ9q++rpe8deo7exSUFwAs1f5vxNmr/3eceJo/DLD83A+YfvELmXQG8xF/R7VfMPDb5A/GQt72UBhONCrNxG9+ZjI+evOG0v7DBxpKKoycZq4SarU735nLrvQNaU29Wgi4W/91fX/IUMlTuJaHg9ZYMYkMLQJULJBZcb9/I2alsm6sI+TouXowALUhroLEE/AibKQAhNPSpZnCuEirB9OARMkQxcZGirNM1faPwmQ5nsvjl32frgczEXelHlHUcRzG9uGsRlN8wO6r2ntDLRYcqtb6rtpsGSQp3IqbRlJXBE0QlsBkraixppHx06ZSgoQn3fZrnsii/84/LI6yjKQV4ZhD8BQYSvF6jmXZP0LJToA+bB0/zzgbulyzBq/rpdJCmSuzfvBKt01dsHyrG49yXcT8Urz0u0jfd73miUlYeaSfNnAHOWrlPuGZ4TzZTbaDQ1wreTITpSj+avHnL+4bmiQ+talBBE2/qGrqVrfM3fIJRl4a9HTeovPo7THd1TAFE/mD1BqGsgtUL0W31y04X/tPGeNu2lVnBw8yePxFOL1yrXCEGVViMh9GWqaedJo3DBETvif/y8RzqKeQdC3OWcMEAGAL5/T3hNziH8qx+vMXF0MbLSMtlkTMbITQNlxa7QFyP8ews5XH7ibqi43o5nVz34CoB02ifO4KtDTK6/vfgwvL62P1AemBn/evLu+J97PPpQFrzChrLtuGQjrIBC+xTSxcL4kcUgt09/qWIMXtORhfN3iDB+VDFYRXtZPb3f5BWhvjqS73XwDuPwzJJ1wfcRRZn2Mb1z7+/k0T346FHT8evHFiv3Fist8c7FuJY5f+Hp9+l37IIFKzbigQWr1MnKEKMgr3YjRn+HUlNX1Isu1vzNnL/oCHJnEi9G19BF6uXJY3qQcwhLfc1fCGNV8w8/68I/i7cH4HU20RGL+dDbp1pf5zjomUiDYCW/jQ7ecRyA0PtgxvQJkQheMcml5UwXtI8cUHPo9Am4+OidYtMriFgDD2r6DdlI5xBh7MgC/vPsffG5E3fHDG1ntSDjqCngx8cOE0Zi02BZ2cxDEf4y7VPIYeyIAv79rH0Vu0ga7SOnwMiCt+82GecdtgNG+3liBssuPnN8mO9Gnmz6Br3AxWbRPntOHRNo/oOlSmpgHRDv6rmVFqm+7dheLF8/EMQqBLSPQYAKyLTnN8/cR/lN4fwN5Yd/Cf/2LvVaAEFq7oD2CTR/75nH9OYDZ499th2LK965p1+n8B7yRGAaG3r8TU7i/HVbXKPQtcJf57MBbxVgpH1i8uH0BG6HhCljevC6r/nng86a/ML1ugjE8feOQ0ECsYKk+TeKD9SNg0K4CiEmGx3jEAr/5PO28VdLstFWPHeca5tH+3gDQff0Wb4uFP6pe+AKzV86T79i+/EjUaqwQtHJ2VRdTfMPnkEO1kvRhoUykVX4CwgOWvfmke/TN1TxOf9sgkOmfbIYfPee6tFqzy5dh4cXvplR81dbWbThWC2GYZuxvRgqu1izecjT/A1unXqXl1+5ToOOkDj/uCCvJIhJNeT8Q6cDwEvaJqjKMb35YKznlFVGchmmZIfi/YoVd6PRtcJfdCg1R41s8A3PlTVzOTeJvGqYMLoYJK/KGzh/Gb/72OHKd9Gp/3LZ0fjCKXtEzpcNu6K+xZwT4R4Bzzj4u4sPj9wjC3TNP9xUwiuzt5DDN87YG7//WPz9CwY+3YRPHLMzLjxyR1zkG1wBVQCZ0FPIYYSvYfZp2/ot8SdeID3lcdBuiuavXiN2YpPD92X/b3lSUHPxS/XNyOWbzvuyrz2aICK79QlQ7gf9pUpkN7EkyIJVNz7+68m74zvv2Q+//sih+MIpe+CjR03Huw/eDovf2gzAs7foiQwB4JoLZ+A/zgo1ab2Nzzt0e5yyzxScvt+2YT0cCuIFlq8f8Dl/Ucd4mi7JrbqnEHL+8qpG1/wFrjr/YJy0d7i5oGiacsSV2/thtMT5j+oJ3YZlJS5tJZx3CD8+7yAcs7u3aVV/qYKdJ43CJcfsjKs/fEjitbWiKzh/b8cnjhwDosFKZV+omAy+AHDujO3xv/e+hDfWDyjajpyy1SSUZeyz7Vh89vhdg7zdorPuu91Y7LtdVLsWUcg5okDQyH7+8nL4vMPivR7SoAv/YC9RSTjJwtqEQPNPKWtUTx7/fta+WLUh1Nh7UzTOYs4JaBVd+Muph7PugStP8LowEdGecuIumQJaIdFMMuVhys4ah2DPBcPK7ZPH7oIr/7rAeJ0QtLqrqw6RdjwL5D6ur3RP3XdqkP74uD3DXdleeyuccPW+AwAn7DUFg+UKvuFvcai/lXfuNxXv3G8q7n5hRXAs51AQL7B8/YCS26cQw6ED6jvX6dkRhVygNMlODYLD1w3Rp+8/FZPH9ODeeV6Et3inQhMXZYurRveEtI/nNCJsftnp2GLewZkHbIuCQ3j4pdXBJjZfOW2vzPeoFl2h+Zu0koDzV4KVcoFfuZe4yjuuL8uFW598reyHm4WOGSnn5Ukx7Ajh4jgUfC7mQoNQlijHLNDbqexGhX8a4tI7xEHmqdMim4t5JwiD14PCZKQNuoKB89ffsaCl9ElG4NnX1wef43bhSnsv4r3XSvsMxSWPl9CTgY4BNHfdSPrx9PrFRdHK7yZuNai331Q/R9Ay3+grBLs8KUU0f0VZU3/rLeSCviU7NQh10FQtOWBNvCcRGS1WD2JlPKY3H3D2BWljpmoMtqLN4/b6bQa6QvP/4yePxJ+fXY7+UgXFnIOfPvhyICC2kULSewpOwEM6FG7xqAsXcY4srOQka6bNpXXIy09Tps3/fu/++OKfnvPq5ZeTI0I+HwoMk/taLbj+Y4dj7rL1cBzCf717X+w8ydPy9I2ks0CsetKWuQKyt1US13zKPlNw5gHbYv9pY7Ghv4QPH7kjZuw4Hs8vW49rHluEpWvCXOhZNX/5/Vx63C4YKFVw3eOLAXhxDKOkYKMkxHH+aRDCI27A//5jhwceZDKE0V+PcwCAL526J97aNIhfPboIgCq4f/HBQ7ChvxT0q5kXHYYLr30SgKb55xzFiyjOBfLXHzkUH73Oy6ska/6//sihUjbZMBgsrmnk9ss7FCQhFGk2wqye8Zq/3He2GzcCnz1hN/z4/oUAvHgI8QzjFc3f69+maslR/aLY7caPwIl7hZlJP3b0zrj7hRU4bs+tcco+2+D6J5dg9ymjA5fvanQyIY/idvxqBrpC89916zH4l5N2x1dO2yvQKgK+ThLavflcYNQREYxA1BIv8tDINIWs+WcxwMrajun8cw/dPhi4QsjnHAoGSiHnBHVPW/6n4eAdx+MTx3peOxccvmOQg0cYsdL81WUUM9I+ArJw6o3RMHvyDn75oRk4cpeJGNWTxzfP3Acji3nMmD4BHzlqp0hOmSycv0PqBDWmt6B4iUwYVcRPLzjYeP1XtaW4IvyrWOoLe0JcWoSjdp2EDxhovDGSt4+OT71jFyUAT9b8T957Cs49NEypfOzuk3HmAR7frvdB2Ysozph73J5b4x17eBy1rFEft+fWeM/B04LvYgUU1zQjtPYTKR6EV1yg+Sdw/rJQJyJ8/qTdg++eATkayxLQPob+Iq/GxKRTcRn/esoe2GGiZ4A9+6Dt8PMPHoJdJo/G9hNG4kun7qncK8t2mwJhLFHrNP+uEP4yCpqlXkYx7wSC3SEK+GWdRgjyxksa8WiD5p+EUT2qe5sJYlkqtBpHE/76xhe1Iq6+woiVRsfIyOrqKSCfF6f5p22xqQvPtPbPOZTpHR28/Xjj8ala6mKZ8xcDPsvji13Y0rJh6hCKRiGGVpApRSWttOGZRR9L2uchifYRVyXRFUGa75hGMbXf6J58sGtcwKEnePskCc2pY3uDVZI8SaUFdwmI8ZnVqy7YwrQKRUDIlDj6rBnoCtpHhu6jCwB3f+4Y/PX55RhRzAWUDhFw8yePxN0vrIhoPkHeeElYJW2sYsKIYrLmD4TLUpECYWCogm228j4X805QZrXC/8+fORr/fPUt/Ndd8xPrG9A+VWj++08bi48eNV3RPrNCtPOfPvU2PLHoLSx+czNumvV66nXvP3R7DJQquPsFz0CXJti9LS/N53zt9L1wsF/3sSML+Mppe4LZE7i7TB6Nl1dtikw2Jm+fkRm4dvHetk3Jhqkj5xC+dOqeOGZ3L3jtdxcfjjfWh7SXkh7cILhnXnRYYGgXfSxJsKUFoQHJwlcYQ+Py38jja2u/f4/uzWOub1cR+wTIjg36+0sSmlPHjQjaWh6ngh5678HbGa/7+QUHo8KMnSeNwmXH74r3HbK98TwdcoLBLLjs+F2DPlUNxVovulD4+0ZS6cXssc0Y7OHnQnelF7fblDHYTcrDLiBmdlljUQy+GQaL7IIXJ3zFRDR94igAq7Fy4wD2nOrVpyjRPnFbx8Vhv2ljsd+0sYHwj9PIdD//LOgt5IyBMlmvBTy+/ZAdx8N1OZPwf9suk/C2XSZh+pfvBJCB9knQ/D/29p2V75ccowaxHbHzRCxbZ95rFQiFkik/exyq1fwBKMF1R++mRjCrgYtRYXKs704oI6nJsqzikvqIMEzH8dly+4nV9piefGBUP3gHbzIuKJq/RvuMip98RvfkA81fHrM6PaTjnftNDT7/v5OjLthxEDmgsgr/au7dSHQh7ZNsjJU5/zgEG4Urrp7V0T6ydhY3WQitbLrPMa7aMBi6euaoKZn+ZNRi8K0H+gqrmmVzNdflclQVH6tjay0fUa/B22er3uzCP0uAVDWQKcVqVm21QAjrXEqMBhBPDYlVsEw9CQ190uieIGNnkp9/2u5j4r1kiV6uF0I5bEVZ9aCp0oOIrgVwBoBVzLyvf2wCgBsBTAewGMC5zLw27h6NhknzlyHTPnEIvH0MmRaBZP5UYDsp50rc+aKcHf2Ao/5SJTDg5XOOMuHUgj9/5mg8seit1POq0fzrgYkW+I+z9skUWVwNvJ2Vahf+hZyDr52+F751p7dyMuVt2SpDZO31Hz9c8ZVvFHTa57cXH6bEJcgI3R0Jv7nosMAOkRX/dube2HZcL96u5U8yIY4aGlXM4fITdsMZ+4eathhPu08ZHaw8Rvfk8fG374S+oUokZ5FpdXL1hw4J8j99973747f/fA0HxdhxGoldtx6Ny47fFe8/NJkm+tkFBxuVzJ+ef1Amqq1eNJv2uQ7ATwH8Rjr2ZQD3M/OVRPRl//uXmlyPAEFGxxTNP0l7rxgMvrKPfBbNX16mphmSdpwQhncLQewyK/xlLRD0TxqarT0KmPLDfPjI6Q0vR86bUis+9vadA+EvQwihLDl1PLqqrmoYIQv/3kIuoE3ScIyBDkrD1mN6E9M4y9ATBwoQEf5Fo1/Eqlb25CKizGUBwMn7bBN83nbcCHzp1Pio6UaCiDJROadJtJKMM/bf1ni80WjqqGbmhwGs0Q6fBWCm/3kmgLObWQcdBSdZ8xdHkzxcxODqieP8q8zGlyaIRhsmFtflptM+Aq2ifdI2Xm8UCo5Tl+afBGF/qYb2aTTk5HKpGmQQ5dp8VBPAFO6VUb09xCIb2mHwncLMy/3PKwBMiTuRiC4BcAkA7LBD7WkLZKSlXnj7bpPxyWN3wcfevlPsPf74iSNx7/yVCu2z29Zj8IFDt8fE0eqWbT849wBsPcbcge/87NF4+KU3Uw1qPfkc/vu9+2Pa+BGYt3wDAKDiZvMoaQSyZGxsTDmteZ73zZiG/bevn0q68ZIj8Oqbm5VjG3z3xGoMvgI3fPyIIJVxPcg5hMuO3xWrNgxiNz9NRRzk7KQ6fnPRYUp6i3pRTXZKYaCN20LVhF99eEawIbpFOtrq7cPMTESxvY+ZrwZwNQDMmDGjIbsYC349TtvOOZSYVAuA0QuomHdw5Xv3j5wrB7vo2GfbsZn47N6CEwTnLFixEQCUbIfNRiv4R6B1wn/G9AmRNM+14PCdJ+LwnScqxzb4Cd9qScN75C4TcSQmpp+YAY3wIKmFBkpC1tgPINx7OmtKagA4ce9YPdLCgHZ4+6wkoqkA4P9d1crC8w3Ogd9MmBJ/iXqbdg5qFqoZtPWgVSuMZqIUuDW2LlKzHgh6M4uTQishulw1wt+iOrRD878DwIUArvT/3t7Kwk1BXp2KP192NB57WaWFhLYvjM7fO2f/puX7/stlR+OpxbrJpnmoV/P/06fehgUrNjSoNrXhk8fuglKF68qu2kp8/Yy9MXmrHpy4V/O05us+emhkA6M0fPPMfTB90igclcGLaLjgFx88JDWvfyvRbFfPGwC8A8AkInodwL/BE/o3EdHFAF4DcG4z66CjkOLq2UnYa+pW2Guquhet8E8XaSjeNyNb1GEtiEsx3SzUuymNCA5rJ8b0FpqahrfRGD+qiCve2dz6vmOPrdNP0jBlq96Weee0Cqfuu036SS1EU4U/M58X89MJzSw3CULADAfaxwRBjTRq9y4LC4vuRNeld9hhwkhcetwuOHaPxhqzWoV3HbAtXly5EZ9+x67trkrDcNulR2HusvXtroaFRVeBRAqBTseMGTN41qxZ7a6GhYWFxbACET3NzDP045Y7sLCwsOhCWOFvYWFh0YWwwt/CwsKiC2GFv4WFhUUXwgp/CwsLiy6EFf4WFhYWXQgr/C0sLCy6EFb4W1hYWHQhhk2QFxGthpcLqBZMAvBmA6sznGHbwoNthxC2LTxsqe2wIzNHUhoMG+FfD4holinCrRth28KDbYcQti08dFs7WNrHwsLCogthhb+FhYVFF6JbhP/V7a5AB8G2hQfbDiFsW3joqnboCs7fwsLCwkJFt2j+FhYWFhYSrPC3sLCw6EJs0cKfiE4loheJ6GUi+nK769NsENG1RLSKiJ6Xjk0gonuJaKH/d7x/nIjox37bPEdEB7ev5o0FEW1PRA8S0TwieoGILvePd2Nb9BLRk0T0rN8W/+4f34mInvCf+UYiKvrHe/zvL/u/T2/rAzQYRJQjotlE9Bf/e1e2A7AFC38iygG4CsA7AewN4Dwi2ru9tWo6rgNwqnbsywDuZ+bdANzvfwe8dtnN/38JgJ+3qI6tQBnA/2PmvQEcAeBS/913Y1sMAjiemQ8AcCCAU4noCADfBfBDZt4VwFoAF/vnXwxgrX/8h/55WxIuBzBf+t6t7QAw8xb5H8CRAO6Wvl8B4Ip216sFzz0dwPPS9xcBTPU/TwXwov/5lwDOM523pf0HcDuAk7q9LQCMBPAMgMPhRbLm/ePBWAFwN4Aj/c95/zxqd90b9PzT4E36xwP4CwDqxnYQ/7dYzR/AdgCWSt9f9491G6Yw83L/8woAU/zPXdE+/nL9IABPoEvbwqc65gBYBeBeAK8AWMfMZf8U+XmDtvB/Xw9gYksr3Dz8L4AvAnD97xPRne0AYAumfSyiYE+N6RrfXiIaDeBPAD7HzBvk37qpLZi5wswHwtN8DwOwZ3tr1HoQ0RkAVjHz0+2uS6dgSxb+ywBsL32f5h/rNqwkoqkA4P9d5R/fotuHiArwBP/vmfkW/3BXtoUAM68D8CA8emMcEeX9n+TnDdrC/30sgLdaW9Om4CgAZxLRYgB/gEf9/Ajd1w4BtmTh/xSA3XxrfhHABwDc0eY6tQN3ALjQ/3whPP5bHP+w7+lyBID1EiUyrEFEBOAaAPOZ+QfST93YFpOJaJz/eQQ828d8eJPAOf5peluINjoHwAP+KmlYg5mvYOZpzDwdnix4gJkvQJe1g4J2Gx2a+R/AaQBegsdxfrXd9WnB894AYDmAEjz+8mJ4POX9ABYCuA/ABP9cgucN9QqAuQBmtLv+DWyHo+FROs8BmOP/P61L22J/ALP9tngewDf84zsDeBLAywD+CKDHP97rf3/Z/33ndj9DE9rkHQD+0u3tYNM7WFhYWHQhtmTax8LCwsIiBlb4W1hYWHQhrPC3sLCw6EJY4W9hYWHRhbDC38LCwqILYYW/hUUCiOg/iOjEBtxnUyPqY2HRKFhXTwuLFoCINjHz6HbXw8JCwGr+Fl0HIvqgn+N+DhH90k98tomIfujnvL+fiCb7515HROf4n6/09wh4joj+xz82nYge8I/dT0Q7+Md3IqJ/ENFcIvqWVv4XiOgp/xqRX38UEd3p591/noje39pWseg2WOFv0VUgor0AvB/AUewlO6sAuADAKACzmHkfAA8B+DftuokA3g1gH2beH4AQ6D8BMNM/9nsAP/aP/wjAz5l5P3hR1+I+J8PbN+AwePn1DyGiY+Dtw/AGMx/AzPsC+FuDH93CQoEV/hbdhhMAHALgKT/N8QnwQvxdADf65/wOXooIGesBDAC4hojeA6DPP34kgOv9z7+VrjsKXroNcVzgZP//bHi59feENxnMBXASEX2XiN7OzOvre0wLi2Tk00+xsNiiQPA09SuUg0Rf185TjGHMXCaiw+BNFucA+Ay8zJBJMBnUCMB3mPmXkR+87SNPA/AtIrqfmf8j5f4WFjXDav4W3Yb7AZxDRFsDwb6+O8IbCyK74/kAHpUv8vcGGMvMdwH4FwAH+D89Di9LJODRR4/4nx/TjgvcDeAi/34gou2IaGsi2hZAHzP/DsD3AGwx+whbdCas5m/RVWDmeUT0NQD3EJEDLwPqpQA2AzjM/20VPLuAjDEAbieiXnja++f945cB+DURfQHAagAf9Y9fDuB6IvoSwjTBYOZ7fLvDP7zM09gE4IMAdgXwPSJy/Tp9qrFPbmGhwrp6WljAumJadB8s7WNhYWHRhbCav4WFhUUXwmr+FhYWFl0IK/wtLCwsuhBW+FtYWFh0Iazwt7CwsOhCWOFvYWFh0YX4/9WcbgaqpYRSAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 20 episodes ...\n",
            "Episode 1: reward: 200.000, steps: 200\n",
            "Episode 2: reward: 200.000, steps: 200\n",
            "Episode 3: reward: 200.000, steps: 200\n",
            "Episode 4: reward: 200.000, steps: 200\n",
            "Episode 5: reward: 200.000, steps: 200\n",
            "Episode 6: reward: 200.000, steps: 200\n",
            "Episode 7: reward: 200.000, steps: 200\n",
            "Episode 8: reward: 200.000, steps: 200\n",
            "Episode 9: reward: 200.000, steps: 200\n",
            "Episode 10: reward: 200.000, steps: 200\n",
            "Episode 11: reward: 200.000, steps: 200\n",
            "Episode 12: reward: 200.000, steps: 200\n",
            "Episode 13: reward: 200.000, steps: 200\n",
            "Episode 14: reward: 200.000, steps: 200\n",
            "Episode 15: reward: 200.000, steps: 200\n",
            "Episode 16: reward: 200.000, steps: 200\n",
            "Episode 17: reward: 200.000, steps: 200\n",
            "Episode 18: reward: 200.000, steps: 200\n",
            "Episode 19: reward: 200.000, steps: 200\n",
            "Episode 20: reward: 200.000, steps: 200\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fdef15fd790>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    }
  ]
}