{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harishahamed26/Car-pole-Reinforcement-Learning/blob/main/PredictionChallenge3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKK5DA390wRe"
      },
      "source": [
        "# Skeleton Code for Prediction Challenge 3\n",
        "Below is partial code to get you started on prediction challenge 3. You need to select values for the parameters that have question marks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gtr01pedYmmf",
        "outputId": "21ee5b34-40e8-4123-e2ee-4ee95e2db700"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras-rl2\n",
            "  Downloading keras_rl2-1.0.5-py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.1/52.1 KB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.9/dist-packages (from keras-rl2) (2.11.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (2.11.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (3.19.6)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (2.2.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (0.31.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (0.4.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (3.1.0)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (2.11.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.51.3)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (2.11.2)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (23.3.3)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (15.0.6.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (63.4.3)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (4.5.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (23.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.38.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2.25.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2.2.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2.16.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (3.4.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (6.0.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (1.26.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (3.2.2)\n",
            "Installing collected packages: keras-rl2\n",
            "Successfully installed keras-rl2-1.0.5\n"
          ]
        }
      ],
      "source": [
        "!pip install keras-rl2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwlNl77gYky4",
        "outputId": "d08c689e-db26-42ac-9937-8b24147a65db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.9/dist-packages (0.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.9/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from gym) (6.0.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.9/dist-packages (from gym) (1.22.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.8.0->gym) (3.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aw0JkEYOYhHL",
        "outputId": "e95a1f48-b974-40d5-d259-05b87449c7e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# load the gym module\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "# import the usual Keras modules for creating deep neural networks\n",
        "from keras import Sequential\n",
        "from keras.layers import Input, Flatten, Dense\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "ENV_NAME = 'CartPole-v0'\n",
        "env = gym.make(ENV_NAME)\n",
        "     \n",
        "\n",
        "import rl\n",
        "from rl.memory import SequentialMemory  # import the exerience replay buffer module\n",
        "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy # import the policy\n",
        "from rl.agents.dqn import DQNAgent   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeM5rpqobHPM",
        "outputId": "54d10a74-7825-4fe8-c4d9-20d0efdee96e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_2 (Flatten)         (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 16)                80        \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 32)                544       \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 64)                2112      \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 128)               8320      \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 256)               33024     \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 512)               131584    \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 2)                 1026      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 176,690\n",
            "Trainable params: 176,690\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "from keras.layers.regularization.dropout import Dropout\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(1,env.observation_space.shape[0])))  # The input is 1 observation vector, and the number of observations in that vector \n",
        "model.add(Flatten())\n",
        "# add extra layers here\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "\n",
        "\n",
        "model.add(Dense(env.action_space.n, activation='linear'))   # the output is the number of actions in the action space\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8ZiiRbxlH2D",
        "outputId": "5f4e200e-91fd-4d2b-843c-37daafe142af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/optimizers/optimizer_v2/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 20000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 10 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 11 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 12 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 13 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 14 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 15 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 16 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 17 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 18 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 19 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 20 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 21 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 22 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 23 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 24 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 25 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 26 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 27 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 28 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 29 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 30 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 31 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    65/20000: episode: 1, duration: 2.873s, episode steps:  65, steps per second:  23, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 0.075998, mae: 0.563865, mean_q: 0.933226, mean_eps: 0.998313\n",
            "    80/20000: episode: 2, duration: 0.173s, episode steps:  15, steps per second:  87, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.000998, mae: 0.530354, mean_q: 1.039621, mean_eps: 0.996760\n",
            "    91/20000: episode: 3, duration: 0.146s, episode steps:  11, steps per second:  76, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.000961, mae: 0.534018, mean_q: 1.045076, mean_eps: 0.996175\n",
            "   108/20000: episode: 4, duration: 0.190s, episode steps:  17, steps per second:  90, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.353 [0.000, 1.000],  loss: 0.000937, mae: 0.540306, mean_q: 1.062428, mean_eps: 0.995545\n",
            "   145/20000: episode: 5, duration: 0.413s, episode steps:  37, steps per second:  89, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.568 [0.000, 1.000],  loss: 0.000816, mae: 0.543686, mean_q: 1.075149, mean_eps: 0.994330\n",
            "   164/20000: episode: 6, duration: 0.220s, episode steps:  19, steps per second:  86, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 0.000564, mae: 0.548631, mean_q: 1.087171, mean_eps: 0.993070\n",
            "   176/20000: episode: 7, duration: 0.159s, episode steps:  12, steps per second:  75, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.000466, mae: 0.549703, mean_q: 1.091871, mean_eps: 0.992372\n",
            "   194/20000: episode: 8, duration: 0.211s, episode steps:  18, steps per second:  86, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.389 [0.000, 1.000],  loss: 0.000359, mae: 0.552285, mean_q: 1.099913, mean_eps: 0.991698\n",
            "   211/20000: episode: 9, duration: 0.204s, episode steps:  17, steps per second:  83, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 0.000623, mae: 0.561155, mean_q: 1.114776, mean_eps: 0.990910\n",
            "   232/20000: episode: 10, duration: 0.250s, episode steps:  21, steps per second:  84, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.000940, mae: 0.567330, mean_q: 1.119733, mean_eps: 0.990055\n",
            "   244/20000: episode: 11, duration: 0.135s, episode steps:  12, steps per second:  89, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.000742, mae: 0.571488, mean_q: 1.133154, mean_eps: 0.989313\n",
            "   280/20000: episode: 12, duration: 0.410s, episode steps:  36, steps per second:  88, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.000832, mae: 0.575469, mean_q: 1.141275, mean_eps: 0.988233\n",
            "   290/20000: episode: 13, duration: 0.123s, episode steps:  10, steps per second:  81, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.001195, mae: 0.581551, mean_q: 1.146157, mean_eps: 0.987198\n",
            "   309/20000: episode: 14, duration: 0.215s, episode steps:  19, steps per second:  88, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.316 [0.000, 1.000],  loss: 0.001034, mae: 0.586657, mean_q: 1.164579, mean_eps: 0.986545\n",
            "   333/20000: episode: 15, duration: 0.271s, episode steps:  24, steps per second:  89, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 0.001038, mae: 0.593691, mean_q: 1.177603, mean_eps: 0.985578\n",
            "   370/20000: episode: 16, duration: 0.426s, episode steps:  37, steps per second:  87, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 0.001355, mae: 0.602832, mean_q: 1.198426, mean_eps: 0.984205\n",
            "   387/20000: episode: 17, duration: 0.188s, episode steps:  17, steps per second:  91, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 0.001639, mae: 0.609176, mean_q: 1.200933, mean_eps: 0.982990\n",
            "   403/20000: episode: 18, duration: 0.190s, episode steps:  16, steps per second:  84, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.001906, mae: 0.618122, mean_q: 1.214959, mean_eps: 0.982247\n",
            "   419/20000: episode: 19, duration: 0.214s, episode steps:  16, steps per second:  75, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 0.002234, mae: 0.625788, mean_q: 1.231598, mean_eps: 0.981527\n",
            "   441/20000: episode: 20, duration: 0.261s, episode steps:  22, steps per second:  84, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 0.002334, mae: 0.629653, mean_q: 1.244394, mean_eps: 0.980673\n",
            "   451/20000: episode: 21, duration: 0.127s, episode steps:  10, steps per second:  79, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.001604, mae: 0.628880, mean_q: 1.253301, mean_eps: 0.979952\n",
            "   474/20000: episode: 22, duration: 0.283s, episode steps:  23, steps per second:  81, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  loss: 0.002537, mae: 0.638582, mean_q: 1.254082, mean_eps: 0.979210\n",
            "   505/20000: episode: 23, duration: 0.356s, episode steps:  31, steps per second:  87, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  loss: 0.002255, mae: 0.640373, mean_q: 1.266818, mean_eps: 0.977995\n",
            "   530/20000: episode: 24, duration: 0.297s, episode steps:  25, steps per second:  84, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.360 [0.000, 1.000],  loss: 0.002319, mae: 0.647959, mean_q: 1.279188, mean_eps: 0.976735\n",
            "   550/20000: episode: 25, duration: 0.237s, episode steps:  20, steps per second:  84, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.650 [0.000, 1.000],  loss: 0.002493, mae: 0.655976, mean_q: 1.292109, mean_eps: 0.975722\n",
            "   578/20000: episode: 26, duration: 0.314s, episode steps:  28, steps per second:  89, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 0.002906, mae: 0.661003, mean_q: 1.300786, mean_eps: 0.974643\n",
            "   625/20000: episode: 27, duration: 0.529s, episode steps:  47, steps per second:  89, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.404 [0.000, 1.000],  loss: 0.002662, mae: 0.670446, mean_q: 1.321394, mean_eps: 0.972955\n",
            "   638/20000: episode: 28, duration: 0.162s, episode steps:  13, steps per second:  80, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.154 [0.000, 1.000],  loss: 0.002518, mae: 0.680716, mean_q: 1.348363, mean_eps: 0.971605\n",
            "   655/20000: episode: 29, duration: 0.203s, episode steps:  17, steps per second:  84, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 0.003661, mae: 0.688663, mean_q: 1.353098, mean_eps: 0.970930\n",
            "   681/20000: episode: 30, duration: 0.318s, episode steps:  26, steps per second:  82, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.577 [0.000, 1.000],  loss: 0.004052, mae: 0.694465, mean_q: 1.361876, mean_eps: 0.969963\n",
            "   710/20000: episode: 31, duration: 0.326s, episode steps:  29, steps per second:  89, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.379 [0.000, 1.000],  loss: 0.004989, mae: 0.704468, mean_q: 1.373098, mean_eps: 0.968725\n",
            "   730/20000: episode: 32, duration: 0.245s, episode steps:  20, steps per second:  82, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.650 [0.000, 1.000],  loss: 0.004169, mae: 0.706650, mean_q: 1.389870, mean_eps: 0.967623\n",
            "   768/20000: episode: 33, duration: 0.443s, episode steps:  38, steps per second:  86, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 0.002794, mae: 0.709252, mean_q: 1.406437, mean_eps: 0.966318\n",
            "   781/20000: episode: 34, duration: 0.151s, episode steps:  13, steps per second:  86, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 0.003424, mae: 0.716696, mean_q: 1.418862, mean_eps: 0.965170\n",
            "   792/20000: episode: 35, duration: 0.142s, episode steps:  11, steps per second:  78, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 0.003207, mae: 0.725205, mean_q: 1.424467, mean_eps: 0.964630\n",
            "   814/20000: episode: 36, duration: 0.259s, episode steps:  22, steps per second:  85, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 0.006037, mae: 0.733565, mean_q: 1.426980, mean_eps: 0.963888\n",
            "   834/20000: episode: 37, duration: 0.230s, episode steps:  20, steps per second:  87, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 0.003979, mae: 0.734730, mean_q: 1.444152, mean_eps: 0.962942\n",
            "   848/20000: episode: 38, duration: 0.169s, episode steps:  14, steps per second:  83, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  loss: 0.003928, mae: 0.739456, mean_q: 1.452416, mean_eps: 0.962178\n",
            "   866/20000: episode: 39, duration: 0.219s, episode steps:  18, steps per second:  82, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.004040, mae: 0.748089, mean_q: 1.467051, mean_eps: 0.961458\n",
            "   884/20000: episode: 40, duration: 0.208s, episode steps:  18, steps per second:  87, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: 0.003521, mae: 0.753512, mean_q: 1.482585, mean_eps: 0.960647\n",
            "   905/20000: episode: 41, duration: 0.262s, episode steps:  21, steps per second:  80, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.381 [0.000, 1.000],  loss: 0.003735, mae: 0.753295, mean_q: 1.485370, mean_eps: 0.959770\n",
            "   932/20000: episode: 42, duration: 0.475s, episode steps:  27, steps per second:  57, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 0.003503, mae: 0.767159, mean_q: 1.498626, mean_eps: 0.958690\n",
            "   947/20000: episode: 43, duration: 0.264s, episode steps:  15, steps per second:  57, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.004799, mae: 0.773527, mean_q: 1.511624, mean_eps: 0.957745\n",
            "   962/20000: episode: 44, duration: 0.271s, episode steps:  15, steps per second:  55, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 0.003457, mae: 0.768891, mean_q: 1.516537, mean_eps: 0.957070\n",
            "  1011/20000: episode: 45, duration: 0.836s, episode steps:  49, steps per second:  59, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 0.004112, mae: 0.785768, mean_q: 1.535745, mean_eps: 0.955630\n",
            "  1035/20000: episode: 46, duration: 0.423s, episode steps:  24, steps per second:  57, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.003464, mae: 0.798930, mean_q: 1.572214, mean_eps: 0.953988\n",
            "  1048/20000: episode: 47, duration: 0.240s, episode steps:  13, steps per second:  54, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 0.004820, mae: 0.801004, mean_q: 1.569177, mean_eps: 0.953155\n",
            "  1060/20000: episode: 48, duration: 0.192s, episode steps:  12, steps per second:  63, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.005915, mae: 0.812374, mean_q: 1.586222, mean_eps: 0.952592\n",
            "  1073/20000: episode: 49, duration: 0.160s, episode steps:  13, steps per second:  81, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 0.004878, mae: 0.818729, mean_q: 1.606804, mean_eps: 0.952030\n",
            "  1086/20000: episode: 50, duration: 0.169s, episode steps:  13, steps per second:  77, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 0.004437, mae: 0.812988, mean_q: 1.599177, mean_eps: 0.951445\n",
            "  1104/20000: episode: 51, duration: 0.204s, episode steps:  18, steps per second:  88, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: 0.004159, mae: 0.821805, mean_q: 1.609344, mean_eps: 0.950748\n",
            "  1114/20000: episode: 52, duration: 0.123s, episode steps:  10, steps per second:  81, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.003568, mae: 0.826606, mean_q: 1.631609, mean_eps: 0.950118\n",
            "  1152/20000: episode: 53, duration: 0.430s, episode steps:  38, steps per second:  88, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 0.004152, mae: 0.828484, mean_q: 1.624466, mean_eps: 0.949038\n",
            "  1164/20000: episode: 54, duration: 0.137s, episode steps:  12, steps per second:  88, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.004430, mae: 0.835717, mean_q: 1.628887, mean_eps: 0.947913\n",
            "  1200/20000: episode: 55, duration: 0.431s, episode steps:  36, steps per second:  83, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 0.005625, mae: 0.851046, mean_q: 1.659946, mean_eps: 0.946832\n",
            "  1215/20000: episode: 56, duration: 0.178s, episode steps:  15, steps per second:  84, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.008129, mae: 0.865017, mean_q: 1.673692, mean_eps: 0.945685\n",
            "  1233/20000: episode: 57, duration: 0.212s, episode steps:  18, steps per second:  85, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.009913, mae: 0.877654, mean_q: 1.697023, mean_eps: 0.944943\n",
            "  1248/20000: episode: 58, duration: 0.171s, episode steps:  15, steps per second:  88, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.267 [0.000, 1.000],  loss: 0.007394, mae: 0.876609, mean_q: 1.700760, mean_eps: 0.944200\n",
            "  1261/20000: episode: 59, duration: 0.165s, episode steps:  13, steps per second:  79, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.006214, mae: 0.880333, mean_q: 1.728107, mean_eps: 0.943570\n",
            "  1282/20000: episode: 60, duration: 0.235s, episode steps:  21, steps per second:  89, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 0.007283, mae: 0.887089, mean_q: 1.724875, mean_eps: 0.942805\n",
            "  1306/20000: episode: 61, duration: 0.284s, episode steps:  24, steps per second:  85, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 0.008437, mae: 0.897960, mean_q: 1.732181, mean_eps: 0.941793\n",
            "  1324/20000: episode: 62, duration: 0.200s, episode steps:  18, steps per second:  90, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.009643, mae: 0.907156, mean_q: 1.761542, mean_eps: 0.940847\n",
            "  1337/20000: episode: 63, duration: 0.165s, episode steps:  13, steps per second:  79, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 0.007507, mae: 0.903359, mean_q: 1.758304, mean_eps: 0.940150\n",
            "  1370/20000: episode: 64, duration: 0.403s, episode steps:  33, steps per second:  82, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.576 [0.000, 1.000],  loss: 0.006290, mae: 0.909667, mean_q: 1.769856, mean_eps: 0.939115\n",
            "  1409/20000: episode: 65, duration: 0.459s, episode steps:  39, steps per second:  85, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 0.005303, mae: 0.921717, mean_q: 1.800899, mean_eps: 0.937495\n",
            "  1422/20000: episode: 66, duration: 0.155s, episode steps:  13, steps per second:  84, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 0.004314, mae: 0.924966, mean_q: 1.816531, mean_eps: 0.936325\n",
            "  1434/20000: episode: 67, duration: 0.159s, episode steps:  12, steps per second:  75, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.006031, mae: 0.932074, mean_q: 1.824804, mean_eps: 0.935763\n",
            "  1459/20000: episode: 68, duration: 0.297s, episode steps:  25, steps per second:  84, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.005485, mae: 0.938457, mean_q: 1.837953, mean_eps: 0.934930\n",
            "  1475/20000: episode: 69, duration: 0.191s, episode steps:  16, steps per second:  84, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  loss: 0.005451, mae: 0.939324, mean_q: 1.840767, mean_eps: 0.934007\n",
            "  1489/20000: episode: 70, duration: 0.164s, episode steps:  14, steps per second:  85, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.214 [0.000, 1.000],  loss: 0.003469, mae: 0.951407, mean_q: 1.872296, mean_eps: 0.933333\n",
            "  1505/20000: episode: 71, duration: 0.186s, episode steps:  16, steps per second:  86, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 0.005385, mae: 0.957122, mean_q: 1.883361, mean_eps: 0.932657\n",
            "  1531/20000: episode: 72, duration: 0.323s, episode steps:  26, steps per second:  81, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 0.007409, mae: 0.969518, mean_q: 1.885303, mean_eps: 0.931712\n",
            "  1552/20000: episode: 73, duration: 0.249s, episode steps:  21, steps per second:  84, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 0.006513, mae: 0.986697, mean_q: 1.924419, mean_eps: 0.930655\n",
            "  1584/20000: episode: 74, duration: 0.371s, episode steps:  32, steps per second:  86, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.005397, mae: 0.982862, mean_q: 1.923747, mean_eps: 0.929462\n",
            "  1617/20000: episode: 75, duration: 0.394s, episode steps:  33, steps per second:  84, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 0.005652, mae: 0.989341, mean_q: 1.935399, mean_eps: 0.928000\n",
            "  1654/20000: episode: 76, duration: 0.424s, episode steps:  37, steps per second:  87, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.405 [0.000, 1.000],  loss: 0.005250, mae: 1.004292, mean_q: 1.969205, mean_eps: 0.926425\n",
            "  1677/20000: episode: 77, duration: 0.288s, episode steps:  23, steps per second:  80, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 0.006290, mae: 1.009485, mean_q: 1.976906, mean_eps: 0.925075\n",
            "  1703/20000: episode: 78, duration: 0.319s, episode steps:  26, steps per second:  82, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.423 [0.000, 1.000],  loss: 0.007966, mae: 1.030288, mean_q: 2.001003, mean_eps: 0.923973\n",
            "  1747/20000: episode: 79, duration: 0.514s, episode steps:  44, steps per second:  86, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.432 [0.000, 1.000],  loss: 0.008196, mae: 1.042472, mean_q: 2.031777, mean_eps: 0.922397\n",
            "  1766/20000: episode: 80, duration: 0.227s, episode steps:  19, steps per second:  84, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 0.007403, mae: 1.050189, mean_q: 2.050475, mean_eps: 0.920980\n",
            "  1786/20000: episode: 81, duration: 0.256s, episode steps:  20, steps per second:  78, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 0.006978, mae: 1.055543, mean_q: 2.062317, mean_eps: 0.920103\n",
            "  1808/20000: episode: 82, duration: 0.289s, episode steps:  22, steps per second:  76, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.004852, mae: 1.067488, mean_q: 2.094882, mean_eps: 0.919158\n",
            "  1819/20000: episode: 83, duration: 0.147s, episode steps:  11, steps per second:  75, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.005911, mae: 1.065694, mean_q: 2.081674, mean_eps: 0.918415\n",
            "  1835/20000: episode: 84, duration: 0.214s, episode steps:  16, steps per second:  75, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 0.004940, mae: 1.071148, mean_q: 2.101061, mean_eps: 0.917807\n",
            "  1849/20000: episode: 85, duration: 0.182s, episode steps:  14, steps per second:  77, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 0.009371, mae: 1.078817, mean_q: 2.105944, mean_eps: 0.917133\n",
            "  1863/20000: episode: 86, duration: 0.176s, episode steps:  14, steps per second:  79, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.786 [0.000, 1.000],  loss: 0.006976, mae: 1.079679, mean_q: 2.117858, mean_eps: 0.916502\n",
            "  1895/20000: episode: 87, duration: 0.447s, episode steps:  32, steps per second:  72, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 0.007380, mae: 1.088415, mean_q: 2.130775, mean_eps: 0.915467\n",
            "  1909/20000: episode: 88, duration: 0.264s, episode steps:  14, steps per second:  53, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.004844, mae: 1.096928, mean_q: 2.165436, mean_eps: 0.914432\n",
            "  1922/20000: episode: 89, duration: 0.235s, episode steps:  13, steps per second:  55, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.231 [0.000, 1.000],  loss: 0.008473, mae: 1.097010, mean_q: 2.149815, mean_eps: 0.913825\n",
            "  1938/20000: episode: 90, duration: 0.278s, episode steps:  16, steps per second:  58, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.312 [0.000, 1.000],  loss: 0.006155, mae: 1.105082, mean_q: 2.170433, mean_eps: 0.913172\n",
            "  1953/20000: episode: 91, duration: 0.258s, episode steps:  15, steps per second:  58, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.005591, mae: 1.105180, mean_q: 2.174021, mean_eps: 0.912475\n",
            "  1971/20000: episode: 92, duration: 0.309s, episode steps:  18, steps per second:  58, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 0.006743, mae: 1.114348, mean_q: 2.192904, mean_eps: 0.911732\n",
            "  1987/20000: episode: 93, duration: 0.280s, episode steps:  16, steps per second:  57, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 0.006470, mae: 1.119389, mean_q: 2.209604, mean_eps: 0.910967\n",
            "  2002/20000: episode: 94, duration: 0.265s, episode steps:  15, steps per second:  57, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.005343, mae: 1.135564, mean_q: 2.227295, mean_eps: 0.910270\n",
            "  2025/20000: episode: 95, duration: 0.409s, episode steps:  23, steps per second:  56, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.348 [0.000, 1.000],  loss: 0.007824, mae: 1.134503, mean_q: 2.226844, mean_eps: 0.909415\n",
            "  2045/20000: episode: 96, duration: 0.323s, episode steps:  20, steps per second:  62, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.006855, mae: 1.141111, mean_q: 2.235808, mean_eps: 0.908448\n",
            "  2058/20000: episode: 97, duration: 0.166s, episode steps:  13, steps per second:  78, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 0.008407, mae: 1.142609, mean_q: 2.230891, mean_eps: 0.907705\n",
            "  2074/20000: episode: 98, duration: 0.196s, episode steps:  16, steps per second:  81, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 0.006419, mae: 1.168481, mean_q: 2.293577, mean_eps: 0.907053\n",
            "  2105/20000: episode: 99, duration: 0.385s, episode steps:  31, steps per second:  81, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.387 [0.000, 1.000],  loss: 0.009221, mae: 1.165828, mean_q: 2.288423, mean_eps: 0.905995\n",
            "  2190/20000: episode: 100, duration: 0.994s, episode steps:  85, steps per second:  86, episode reward: 85.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 0.008327, mae: 1.185368, mean_q: 2.329201, mean_eps: 0.903385\n",
            "  2200/20000: episode: 101, duration: 0.119s, episode steps:  10, steps per second:  84, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.006919, mae: 1.203085, mean_q: 2.375347, mean_eps: 0.901247\n",
            "  2214/20000: episode: 102, duration: 0.179s, episode steps:  14, steps per second:  78, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  loss: 0.011385, mae: 1.208477, mean_q: 2.372885, mean_eps: 0.900707\n",
            "  2222/20000: episode: 103, duration: 0.096s, episode steps:   8, steps per second:  84, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.875 [0.000, 1.000],  loss: 0.009716, mae: 1.213853, mean_q: 2.381486, mean_eps: 0.900213\n",
            "  2234/20000: episode: 104, duration: 0.148s, episode steps:  12, steps per second:  81, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 0.005556, mae: 1.223510, mean_q: 2.404403, mean_eps: 0.899762\n",
            "  2263/20000: episode: 105, duration: 0.345s, episode steps:  29, steps per second:  84, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.448 [0.000, 1.000],  loss: 0.008715, mae: 1.223498, mean_q: 2.405798, mean_eps: 0.898840\n",
            "  2284/20000: episode: 106, duration: 0.249s, episode steps:  21, steps per second:  84, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.619 [0.000, 1.000],  loss: 0.005942, mae: 1.225082, mean_q: 2.423084, mean_eps: 0.897715\n",
            "  2299/20000: episode: 107, duration: 0.183s, episode steps:  15, steps per second:  82, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.006601, mae: 1.224502, mean_q: 2.418964, mean_eps: 0.896905\n",
            "  2349/20000: episode: 108, duration: 0.581s, episode steps:  50, steps per second:  86, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.420 [0.000, 1.000],  loss: 0.007789, mae: 1.249098, mean_q: 2.456830, mean_eps: 0.895443\n",
            "  2364/20000: episode: 109, duration: 0.193s, episode steps:  15, steps per second:  78, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.006825, mae: 1.246560, mean_q: 2.468030, mean_eps: 0.893980\n",
            "  2433/20000: episode: 110, duration: 0.783s, episode steps:  69, steps per second:  88, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 0.007835, mae: 1.275501, mean_q: 2.523863, mean_eps: 0.892090\n",
            "  2481/20000: episode: 111, duration: 0.551s, episode steps:  48, steps per second:  87, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 0.010086, mae: 1.290348, mean_q: 2.542372, mean_eps: 0.889457\n",
            "  2496/20000: episode: 112, duration: 0.169s, episode steps:  15, steps per second:  89, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.267 [0.000, 1.000],  loss: 0.010757, mae: 1.322848, mean_q: 2.588136, mean_eps: 0.888040\n",
            "  2510/20000: episode: 113, duration: 0.172s, episode steps:  14, steps per second:  82, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.786 [0.000, 1.000],  loss: 0.006636, mae: 1.308640, mean_q: 2.589940, mean_eps: 0.887388\n",
            "  2527/20000: episode: 114, duration: 0.209s, episode steps:  17, steps per second:  81, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 0.006585, mae: 1.330873, mean_q: 2.632336, mean_eps: 0.886690\n",
            "  2566/20000: episode: 115, duration: 0.447s, episode steps:  39, steps per second:  87, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 0.007614, mae: 1.329257, mean_q: 2.632140, mean_eps: 0.885430\n",
            "  2582/20000: episode: 116, duration: 0.189s, episode steps:  16, steps per second:  85, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.008839, mae: 1.335266, mean_q: 2.642848, mean_eps: 0.884192\n",
            "  2607/20000: episode: 117, duration: 0.295s, episode steps:  25, steps per second:  85, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 0.011474, mae: 1.343097, mean_q: 2.652572, mean_eps: 0.883270\n",
            "  2645/20000: episode: 118, duration: 0.452s, episode steps:  38, steps per second:  84, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.447 [0.000, 1.000],  loss: 0.011133, mae: 1.363564, mean_q: 2.676182, mean_eps: 0.881852\n",
            "  2690/20000: episode: 119, duration: 0.503s, episode steps:  45, steps per second:  90, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 0.006853, mae: 1.378750, mean_q: 2.736055, mean_eps: 0.879985\n",
            "  2736/20000: episode: 120, duration: 0.523s, episode steps:  46, steps per second:  88, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 0.008746, mae: 1.399881, mean_q: 2.771748, mean_eps: 0.877938\n",
            "  2752/20000: episode: 121, duration: 0.205s, episode steps:  16, steps per second:  78, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 0.006693, mae: 1.412200, mean_q: 2.803496, mean_eps: 0.876543\n",
            "  2769/20000: episode: 122, duration: 0.206s, episode steps:  17, steps per second:  83, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 0.005772, mae: 1.404419, mean_q: 2.799823, mean_eps: 0.875800\n",
            "  2785/20000: episode: 123, duration: 0.185s, episode steps:  16, steps per second:  86, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  loss: 0.005120, mae: 1.416333, mean_q: 2.820417, mean_eps: 0.875058\n",
            "  2798/20000: episode: 124, duration: 0.163s, episode steps:  13, steps per second:  80, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 0.012797, mae: 1.420036, mean_q: 2.796609, mean_eps: 0.874405\n",
            "  2823/20000: episode: 125, duration: 0.281s, episode steps:  25, steps per second:  89, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  loss: 0.009156, mae: 1.437394, mean_q: 2.836099, mean_eps: 0.873550\n",
            "  2862/20000: episode: 126, duration: 0.436s, episode steps:  39, steps per second:  89, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.564 [0.000, 1.000],  loss: 0.013112, mae: 1.456877, mean_q: 2.873125, mean_eps: 0.872110\n",
            "  2889/20000: episode: 127, duration: 0.342s, episode steps:  27, steps per second:  79, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 0.011043, mae: 1.459386, mean_q: 2.881816, mean_eps: 0.870625\n",
            "  2913/20000: episode: 128, duration: 0.439s, episode steps:  24, steps per second:  55, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.009324, mae: 1.465874, mean_q: 2.906647, mean_eps: 0.869478\n",
            "  2936/20000: episode: 129, duration: 0.396s, episode steps:  23, steps per second:  58, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 0.010081, mae: 1.490044, mean_q: 2.937268, mean_eps: 0.868420\n",
            "  2978/20000: episode: 130, duration: 0.715s, episode steps:  42, steps per second:  59, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  loss: 0.007433, mae: 1.480377, mean_q: 2.946490, mean_eps: 0.866957\n",
            "  3069/20000: episode: 131, duration: 1.393s, episode steps:  91, steps per second:  65, episode reward: 91.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 0.010895, mae: 1.515522, mean_q: 3.001548, mean_eps: 0.863965\n",
            "  3112/20000: episode: 132, duration: 0.489s, episode steps:  43, steps per second:  88, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 0.008028, mae: 1.531871, mean_q: 3.046188, mean_eps: 0.860950\n",
            "  3123/20000: episode: 133, duration: 0.127s, episode steps:  11, steps per second:  86, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.012849, mae: 1.541810, mean_q: 3.076354, mean_eps: 0.859735\n",
            "  3145/20000: episode: 134, duration: 0.251s, episode steps:  22, steps per second:  88, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.008040, mae: 1.569698, mean_q: 3.124228, mean_eps: 0.858992\n",
            "  3172/20000: episode: 135, duration: 0.308s, episode steps:  27, steps per second:  88, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 0.009421, mae: 1.550737, mean_q: 3.092758, mean_eps: 0.857890\n",
            "  3190/20000: episode: 136, duration: 0.226s, episode steps:  18, steps per second:  80, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: 0.014202, mae: 1.564916, mean_q: 3.114473, mean_eps: 0.856878\n",
            "  3215/20000: episode: 137, duration: 0.272s, episode steps:  25, steps per second:  92, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  loss: 0.010646, mae: 1.604372, mean_q: 3.183727, mean_eps: 0.855910\n",
            "  3239/20000: episode: 138, duration: 0.284s, episode steps:  24, steps per second:  85, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 0.009387, mae: 1.592341, mean_q: 3.150304, mean_eps: 0.854808\n",
            "  3272/20000: episode: 139, duration: 0.377s, episode steps:  33, steps per second:  87, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 0.007468, mae: 1.600446, mean_q: 3.192297, mean_eps: 0.853525\n",
            "  3286/20000: episode: 140, duration: 0.164s, episode steps:  14, steps per second:  86, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 0.004662, mae: 1.615615, mean_q: 3.227211, mean_eps: 0.852467\n",
            "  3303/20000: episode: 141, duration: 0.201s, episode steps:  17, steps per second:  84, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 0.005763, mae: 1.615847, mean_q: 3.237732, mean_eps: 0.851770\n",
            "  3318/20000: episode: 142, duration: 0.177s, episode steps:  15, steps per second:  85, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.004967, mae: 1.610254, mean_q: 3.229309, mean_eps: 0.851050\n",
            "  3328/20000: episode: 143, duration: 0.125s, episode steps:  10, steps per second:  80, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.003087, mae: 1.619772, mean_q: 3.257828, mean_eps: 0.850487\n",
            "  3360/20000: episode: 144, duration: 0.390s, episode steps:  32, steps per second:  82, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 0.009199, mae: 1.629165, mean_q: 3.253353, mean_eps: 0.849542\n",
            "  3375/20000: episode: 145, duration: 0.176s, episode steps:  15, steps per second:  85, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.005354, mae: 1.647551, mean_q: 3.317333, mean_eps: 0.848485\n",
            "  3431/20000: episode: 146, duration: 0.629s, episode steps:  56, steps per second:  89, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.411 [0.000, 1.000],  loss: 0.017095, mae: 1.667662, mean_q: 3.311503, mean_eps: 0.846888\n",
            "  3441/20000: episode: 147, duration: 0.115s, episode steps:  10, steps per second:  87, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.011698, mae: 1.664807, mean_q: 3.301377, mean_eps: 0.845402\n",
            "  3459/20000: episode: 148, duration: 0.212s, episode steps:  18, steps per second:  85, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 0.013562, mae: 1.686320, mean_q: 3.327383, mean_eps: 0.844773\n",
            "  3498/20000: episode: 149, duration: 0.446s, episode steps:  39, steps per second:  87, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 0.017814, mae: 1.720631, mean_q: 3.380510, mean_eps: 0.843490\n",
            "  3577/20000: episode: 150, duration: 0.886s, episode steps:  79, steps per second:  89, episode reward: 79.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.443 [0.000, 1.000],  loss: 0.013304, mae: 1.719339, mean_q: 3.421596, mean_eps: 0.840835\n",
            "  3592/20000: episode: 151, duration: 0.168s, episode steps:  15, steps per second:  89, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.733 [0.000, 1.000],  loss: 0.011660, mae: 1.730267, mean_q: 3.440603, mean_eps: 0.838720\n",
            "  3605/20000: episode: 152, duration: 0.154s, episode steps:  13, steps per second:  84, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 0.013922, mae: 1.746085, mean_q: 3.483072, mean_eps: 0.838090\n",
            "  3622/20000: episode: 153, duration: 0.213s, episode steps:  17, steps per second:  80, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.294 [0.000, 1.000],  loss: 0.011762, mae: 1.718614, mean_q: 3.422528, mean_eps: 0.837415\n",
            "  3636/20000: episode: 154, duration: 0.169s, episode steps:  14, steps per second:  83, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.013063, mae: 1.750698, mean_q: 3.475595, mean_eps: 0.836718\n",
            "  3662/20000: episode: 155, duration: 0.308s, episode steps:  26, steps per second:  84, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.013437, mae: 1.759788, mean_q: 3.496597, mean_eps: 0.835817\n",
            "  3674/20000: episode: 156, duration: 0.143s, episode steps:  12, steps per second:  84, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.008893, mae: 1.778377, mean_q: 3.555548, mean_eps: 0.834962\n",
            "  3703/20000: episode: 157, duration: 0.320s, episode steps:  29, steps per second:  91, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.448 [0.000, 1.000],  loss: 0.014525, mae: 1.776753, mean_q: 3.533674, mean_eps: 0.834040\n",
            "  3723/20000: episode: 158, duration: 0.242s, episode steps:  20, steps per second:  82, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 0.011694, mae: 1.781154, mean_q: 3.545542, mean_eps: 0.832937\n",
            "  3744/20000: episode: 159, duration: 0.239s, episode steps:  21, steps per second:  88, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.381 [0.000, 1.000],  loss: 0.016556, mae: 1.797298, mean_q: 3.564632, mean_eps: 0.832015\n",
            "  3760/20000: episode: 160, duration: 0.193s, episode steps:  16, steps per second:  83, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 0.012095, mae: 1.810281, mean_q: 3.606620, mean_eps: 0.831182\n",
            "  3773/20000: episode: 161, duration: 0.155s, episode steps:  13, steps per second:  84, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 0.012437, mae: 1.811093, mean_q: 3.601533, mean_eps: 0.830530\n",
            "  3784/20000: episode: 162, duration: 0.127s, episode steps:  11, steps per second:  86, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.026996, mae: 1.831641, mean_q: 3.631902, mean_eps: 0.829990\n",
            "  3816/20000: episode: 163, duration: 0.381s, episode steps:  32, steps per second:  84, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.015481, mae: 1.855833, mean_q: 3.693511, mean_eps: 0.829022\n",
            "  3839/20000: episode: 164, duration: 0.264s, episode steps:  23, steps per second:  87, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 0.009170, mae: 1.823074, mean_q: 3.653043, mean_eps: 0.827785\n",
            "  3859/20000: episode: 165, duration: 0.235s, episode steps:  20, steps per second:  85, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.350 [0.000, 1.000],  loss: 0.016932, mae: 1.839518, mean_q: 3.665278, mean_eps: 0.826817\n",
            "  3885/20000: episode: 166, duration: 0.317s, episode steps:  26, steps per second:  82, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 0.015636, mae: 1.883484, mean_q: 3.751685, mean_eps: 0.825783\n",
            "  3910/20000: episode: 167, duration: 0.357s, episode steps:  25, steps per second:  70, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.008995, mae: 1.854193, mean_q: 3.703442, mean_eps: 0.824635\n",
            "  3924/20000: episode: 168, duration: 0.260s, episode steps:  14, steps per second:  54, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.009321, mae: 1.895824, mean_q: 3.804654, mean_eps: 0.823758\n",
            "  3945/20000: episode: 169, duration: 0.371s, episode steps:  21, steps per second:  57, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 0.012812, mae: 1.842688, mean_q: 3.684213, mean_eps: 0.822970\n",
            "  3991/20000: episode: 170, duration: 0.780s, episode steps:  46, steps per second:  59, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.435 [0.000, 1.000],  loss: 0.017621, mae: 1.876976, mean_q: 3.734891, mean_eps: 0.821463\n",
            "  4024/20000: episode: 171, duration: 0.576s, episode steps:  33, steps per second:  57, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 0.008664, mae: 1.896562, mean_q: 3.802100, mean_eps: 0.819685\n",
            "  4048/20000: episode: 172, duration: 0.430s, episode steps:  24, steps per second:  56, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.009492, mae: 1.919846, mean_q: 3.850726, mean_eps: 0.818403\n",
            "  4107/20000: episode: 173, duration: 0.700s, episode steps:  59, steps per second:  84, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.576 [0.000, 1.000],  loss: 0.012701, mae: 1.938234, mean_q: 3.875813, mean_eps: 0.816535\n",
            "  4118/20000: episode: 174, duration: 0.136s, episode steps:  11, steps per second:  81, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.013633, mae: 1.960512, mean_q: 3.919329, mean_eps: 0.814960\n",
            "  4227/20000: episode: 175, duration: 1.231s, episode steps: 109, steps per second:  89, episode reward: 109.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 0.014922, mae: 1.984225, mean_q: 3.959419, mean_eps: 0.812260\n",
            "  4286/20000: episode: 176, duration: 0.679s, episode steps:  59, steps per second:  87, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 0.011474, mae: 2.006860, mean_q: 4.010183, mean_eps: 0.808480\n",
            "  4311/20000: episode: 177, duration: 0.291s, episode steps:  25, steps per second:  86, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.020239, mae: 2.049728, mean_q: 4.081472, mean_eps: 0.806590\n",
            "  4340/20000: episode: 178, duration: 0.362s, episode steps:  29, steps per second:  80, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 0.013610, mae: 2.021047, mean_q: 4.041145, mean_eps: 0.805375\n",
            "  4360/20000: episode: 179, duration: 0.242s, episode steps:  20, steps per second:  83, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.013028, mae: 2.045612, mean_q: 4.101309, mean_eps: 0.804272\n",
            "  4398/20000: episode: 180, duration: 0.450s, episode steps:  38, steps per second:  84, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 0.022443, mae: 2.058693, mean_q: 4.119837, mean_eps: 0.802968\n",
            "  4418/20000: episode: 181, duration: 0.248s, episode steps:  20, steps per second:  81, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.021353, mae: 2.079515, mean_q: 4.108004, mean_eps: 0.801662\n",
            "  4436/20000: episode: 182, duration: 0.221s, episode steps:  18, steps per second:  82, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.027525, mae: 2.068456, mean_q: 4.126918, mean_eps: 0.800808\n",
            "  4471/20000: episode: 183, duration: 0.422s, episode steps:  35, steps per second:  83, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  loss: 0.018766, mae: 2.086720, mean_q: 4.173185, mean_eps: 0.799615\n",
            "  4520/20000: episode: 184, duration: 0.556s, episode steps:  49, steps per second:  88, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 0.009262, mae: 2.097350, mean_q: 4.211944, mean_eps: 0.797725\n",
            "  4535/20000: episode: 185, duration: 0.184s, episode steps:  15, steps per second:  81, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.018662, mae: 2.113372, mean_q: 4.234519, mean_eps: 0.796285\n",
            "  4605/20000: episode: 186, duration: 0.817s, episode steps:  70, steps per second:  86, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.027663, mae: 2.150811, mean_q: 4.287101, mean_eps: 0.794372\n",
            "  4640/20000: episode: 187, duration: 0.417s, episode steps:  35, steps per second:  84, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.343 [0.000, 1.000],  loss: 0.013928, mae: 2.156441, mean_q: 4.322505, mean_eps: 0.792010\n",
            "  4694/20000: episode: 188, duration: 0.611s, episode steps:  54, steps per second:  88, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 0.023126, mae: 2.167954, mean_q: 4.335127, mean_eps: 0.790007\n",
            "  4726/20000: episode: 189, duration: 0.373s, episode steps:  32, steps per second:  86, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.018751, mae: 2.192955, mean_q: 4.382997, mean_eps: 0.788072\n",
            "  4766/20000: episode: 190, duration: 0.481s, episode steps:  40, steps per second:  83, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.011153, mae: 2.199996, mean_q: 4.422084, mean_eps: 0.786452\n",
            "  4789/20000: episode: 191, duration: 0.269s, episode steps:  23, steps per second:  86, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 0.012572, mae: 2.235929, mean_q: 4.485181, mean_eps: 0.785035\n",
            "  4811/20000: episode: 192, duration: 0.259s, episode steps:  22, steps per second:  85, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 0.007328, mae: 2.206742, mean_q: 4.435391, mean_eps: 0.784022\n",
            "  4927/20000: episode: 193, duration: 1.416s, episode steps: 116, steps per second:  82, episode reward: 116.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 0.021055, mae: 2.261729, mean_q: 4.519766, mean_eps: 0.780917\n",
            "  4951/20000: episode: 194, duration: 0.413s, episode steps:  24, steps per second:  58, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 0.009710, mae: 2.281613, mean_q: 4.572004, mean_eps: 0.777767\n",
            "  4980/20000: episode: 195, duration: 0.490s, episode steps:  29, steps per second:  59, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.448 [0.000, 1.000],  loss: 0.018532, mae: 2.303288, mean_q: 4.633256, mean_eps: 0.776575\n",
            "  4990/20000: episode: 196, duration: 0.190s, episode steps:  10, steps per second:  53, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.029792, mae: 2.273726, mean_q: 4.560312, mean_eps: 0.775698\n",
            "  5034/20000: episode: 197, duration: 0.770s, episode steps:  44, steps per second:  57, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 0.014713, mae: 2.319613, mean_q: 4.651517, mean_eps: 0.774483\n",
            "  5049/20000: episode: 198, duration: 0.294s, episode steps:  15, steps per second:  51, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.019854, mae: 2.322451, mean_q: 4.670027, mean_eps: 0.773155\n",
            "  5071/20000: episode: 199, duration: 0.326s, episode steps:  22, steps per second:  67, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.010234, mae: 2.310662, mean_q: 4.654146, mean_eps: 0.772323\n",
            "  5095/20000: episode: 200, duration: 0.285s, episode steps:  24, steps per second:  84, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.015996, mae: 2.293170, mean_q: 4.613412, mean_eps: 0.771288\n",
            "  5156/20000: episode: 201, duration: 0.706s, episode steps:  61, steps per second:  86, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 0.027514, mae: 2.366375, mean_q: 4.718818, mean_eps: 0.769375\n",
            "  5168/20000: episode: 202, duration: 0.149s, episode steps:  12, steps per second:  81, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 0.015728, mae: 2.365435, mean_q: 4.746369, mean_eps: 0.767733\n",
            "  5186/20000: episode: 203, duration: 0.221s, episode steps:  18, steps per second:  81, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 0.017518, mae: 2.372174, mean_q: 4.740438, mean_eps: 0.767057\n",
            "  5217/20000: episode: 204, duration: 0.358s, episode steps:  31, steps per second:  87, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 0.011834, mae: 2.384965, mean_q: 4.800181, mean_eps: 0.765955\n",
            "  5278/20000: episode: 205, duration: 0.694s, episode steps:  61, steps per second:  88, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.426 [0.000, 1.000],  loss: 0.025933, mae: 2.411431, mean_q: 4.819847, mean_eps: 0.763885\n",
            "  5295/20000: episode: 206, duration: 0.200s, episode steps:  17, steps per second:  85, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.294 [0.000, 1.000],  loss: 0.010775, mae: 2.397251, mean_q: 4.824525, mean_eps: 0.762130\n",
            "  5317/20000: episode: 207, duration: 0.262s, episode steps:  22, steps per second:  84, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 0.027711, mae: 2.426501, mean_q: 4.851994, mean_eps: 0.761253\n",
            "  5329/20000: episode: 208, duration: 0.150s, episode steps:  12, steps per second:  80, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.045402, mae: 2.484036, mean_q: 4.946940, mean_eps: 0.760487\n",
            "  5343/20000: episode: 209, duration: 0.166s, episode steps:  14, steps per second:  84, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 0.024544, mae: 2.480782, mean_q: 4.951885, mean_eps: 0.759903\n",
            "  5390/20000: episode: 210, duration: 0.526s, episode steps:  47, steps per second:  89, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 0.019983, mae: 2.473623, mean_q: 4.962852, mean_eps: 0.758530\n",
            "  5412/20000: episode: 211, duration: 0.266s, episode steps:  22, steps per second:  83, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.026560, mae: 2.500656, mean_q: 5.019168, mean_eps: 0.756978\n",
            "  5462/20000: episode: 212, duration: 0.577s, episode steps:  50, steps per second:  87, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 0.028357, mae: 2.492577, mean_q: 4.987548, mean_eps: 0.755358\n",
            "  5480/20000: episode: 213, duration: 0.204s, episode steps:  18, steps per second:  88, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.026646, mae: 2.515123, mean_q: 5.030345, mean_eps: 0.753827\n",
            "  5507/20000: episode: 214, duration: 0.315s, episode steps:  27, steps per second:  86, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.407 [0.000, 1.000],  loss: 0.023629, mae: 2.529023, mean_q: 5.055145, mean_eps: 0.752815\n",
            "  5529/20000: episode: 215, duration: 0.267s, episode steps:  22, steps per second:  82, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 0.027728, mae: 2.522002, mean_q: 5.032800, mean_eps: 0.751712\n",
            "  5560/20000: episode: 216, duration: 0.353s, episode steps:  31, steps per second:  88, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 0.027498, mae: 2.524298, mean_q: 5.080743, mean_eps: 0.750520\n",
            "  5630/20000: episode: 217, duration: 0.768s, episode steps:  70, steps per second:  91, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.021475, mae: 2.552495, mean_q: 5.128838, mean_eps: 0.748248\n",
            "  5690/20000: episode: 218, duration: 0.676s, episode steps:  60, steps per second:  89, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 0.018559, mae: 2.589654, mean_q: 5.213160, mean_eps: 0.745322\n",
            "  5702/20000: episode: 219, duration: 0.137s, episode steps:  12, steps per second:  88, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 0.013524, mae: 2.629042, mean_q: 5.301475, mean_eps: 0.743703\n",
            "  5712/20000: episode: 220, duration: 0.125s, episode steps:  10, steps per second:  80, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.015336, mae: 2.613059, mean_q: 5.247547, mean_eps: 0.743208\n",
            "  5734/20000: episode: 221, duration: 0.245s, episode steps:  22, steps per second:  90, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.318 [0.000, 1.000],  loss: 0.017313, mae: 2.590904, mean_q: 5.189555, mean_eps: 0.742488\n",
            "  5810/20000: episode: 222, duration: 0.863s, episode steps:  76, steps per second:  88, episode reward: 76.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.553 [0.000, 1.000],  loss: 0.026015, mae: 2.631097, mean_q: 5.260171, mean_eps: 0.740282\n",
            "  5865/20000: episode: 223, duration: 0.609s, episode steps:  55, steps per second:  90, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 0.030575, mae: 2.666792, mean_q: 5.348914, mean_eps: 0.737335\n",
            "  5887/20000: episode: 224, duration: 0.262s, episode steps:  22, steps per second:  84, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 0.039075, mae: 2.670958, mean_q: 5.322038, mean_eps: 0.735602\n",
            "  5900/20000: episode: 225, duration: 0.153s, episode steps:  13, steps per second:  85, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 0.022320, mae: 2.714757, mean_q: 5.443130, mean_eps: 0.734815\n",
            "  5915/20000: episode: 226, duration: 0.172s, episode steps:  15, steps per second:  87, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 0.011348, mae: 2.737673, mean_q: 5.503952, mean_eps: 0.734185\n",
            "  5934/20000: episode: 227, duration: 0.285s, episode steps:  19, steps per second:  67, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 0.044113, mae: 2.700747, mean_q: 5.397716, mean_eps: 0.733420\n",
            "  5979/20000: episode: 228, duration: 0.769s, episode steps:  45, steps per second:  59, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 0.027046, mae: 2.720659, mean_q: 5.468823, mean_eps: 0.731980\n",
            "  6008/20000: episode: 229, duration: 0.505s, episode steps:  29, steps per second:  57, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 0.010177, mae: 2.712005, mean_q: 5.490263, mean_eps: 0.730315\n",
            "  6080/20000: episode: 230, duration: 1.187s, episode steps:  72, steps per second:  61, episode reward: 72.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 0.021274, mae: 2.736307, mean_q: 5.523572, mean_eps: 0.728043\n",
            "  6131/20000: episode: 231, duration: 0.595s, episode steps:  51, steps per second:  86, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 0.039110, mae: 2.757309, mean_q: 5.544521, mean_eps: 0.725275\n",
            "  6154/20000: episode: 232, duration: 0.263s, episode steps:  23, steps per second:  88, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 0.018082, mae: 2.774589, mean_q: 5.584755, mean_eps: 0.723610\n",
            "  6193/20000: episode: 233, duration: 0.443s, episode steps:  39, steps per second:  88, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 0.011239, mae: 2.731018, mean_q: 5.536366, mean_eps: 0.722215\n",
            "  6206/20000: episode: 234, duration: 0.149s, episode steps:  13, steps per second:  87, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 0.021063, mae: 2.811275, mean_q: 5.657777, mean_eps: 0.721045\n",
            "  6223/20000: episode: 235, duration: 0.201s, episode steps:  17, steps per second:  85, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 0.014387, mae: 2.820437, mean_q: 5.722629, mean_eps: 0.720370\n",
            "  6251/20000: episode: 236, duration: 0.304s, episode steps:  28, steps per second:  92, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 0.008678, mae: 2.793674, mean_q: 5.657797, mean_eps: 0.719358\n",
            "  6316/20000: episode: 237, duration: 0.730s, episode steps:  65, steps per second:  89, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 0.016686, mae: 2.834733, mean_q: 5.720458, mean_eps: 0.717265\n",
            "  6333/20000: episode: 238, duration: 0.208s, episode steps:  17, steps per second:  82, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.353 [0.000, 1.000],  loss: 0.023119, mae: 2.842048, mean_q: 5.731450, mean_eps: 0.715420\n",
            "  6351/20000: episode: 239, duration: 0.219s, episode steps:  18, steps per second:  82, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.013523, mae: 2.922872, mean_q: 5.892842, mean_eps: 0.714633\n",
            "  6420/20000: episode: 240, duration: 0.748s, episode steps:  69, steps per second:  92, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 0.050960, mae: 2.881602, mean_q: 5.780025, mean_eps: 0.712675\n",
            "  6440/20000: episode: 241, duration: 0.226s, episode steps:  20, steps per second:  89, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.350 [0.000, 1.000],  loss: 0.048348, mae: 2.927367, mean_q: 5.846517, mean_eps: 0.710673\n",
            "  6529/20000: episode: 242, duration: 0.974s, episode steps:  89, steps per second:  91, episode reward: 89.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.573 [0.000, 1.000],  loss: 0.035268, mae: 2.924075, mean_q: 5.892799, mean_eps: 0.708220\n",
            "  6544/20000: episode: 243, duration: 0.176s, episode steps:  15, steps per second:  85, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.054312, mae: 2.921153, mean_q: 5.841617, mean_eps: 0.705880\n",
            "  6560/20000: episode: 244, duration: 0.180s, episode steps:  16, steps per second:  89, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 0.063008, mae: 3.000925, mean_q: 6.023908, mean_eps: 0.705183\n",
            "  6588/20000: episode: 245, duration: 0.326s, episode steps:  28, steps per second:  86, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.028599, mae: 2.956093, mean_q: 5.927610, mean_eps: 0.704193\n",
            "  6620/20000: episode: 246, duration: 0.385s, episode steps:  32, steps per second:  83, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 0.025626, mae: 2.930811, mean_q: 5.917753, mean_eps: 0.702843\n",
            "  6646/20000: episode: 247, duration: 0.305s, episode steps:  26, steps per second:  85, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 0.014127, mae: 2.978427, mean_q: 6.010123, mean_eps: 0.701537\n",
            "  6699/20000: episode: 248, duration: 1.018s, episode steps:  53, steps per second:  52, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.434 [0.000, 1.000],  loss: 0.022238, mae: 2.986938, mean_q: 6.010312, mean_eps: 0.699760\n",
            "  6779/20000: episode: 249, duration: 1.268s, episode steps:  80, steps per second:  63, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 0.017638, mae: 3.021622, mean_q: 6.111743, mean_eps: 0.696767\n",
            "  6821/20000: episode: 250, duration: 0.498s, episode steps:  42, steps per second:  84, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.595 [0.000, 1.000],  loss: 0.026595, mae: 3.010803, mean_q: 6.076454, mean_eps: 0.694022\n",
            "  6858/20000: episode: 251, duration: 0.420s, episode steps:  37, steps per second:  88, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 0.017978, mae: 3.079598, mean_q: 6.240235, mean_eps: 0.692245\n",
            "  6876/20000: episode: 252, duration: 0.220s, episode steps:  18, steps per second:  82, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.052791, mae: 3.095072, mean_q: 6.233996, mean_eps: 0.691008\n",
            "  6909/20000: episode: 253, duration: 0.504s, episode steps:  33, steps per second:  65, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.424 [0.000, 1.000],  loss: 0.037274, mae: 3.102260, mean_q: 6.268825, mean_eps: 0.689860\n",
            "  6998/20000: episode: 254, duration: 1.483s, episode steps:  89, steps per second:  60, episode reward: 89.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 0.036685, mae: 3.101305, mean_q: 6.251636, mean_eps: 0.687115\n",
            "  7053/20000: episode: 255, duration: 0.872s, episode steps:  55, steps per second:  63, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  loss: 0.027239, mae: 3.114842, mean_q: 6.298323, mean_eps: 0.683875\n",
            "  7070/20000: episode: 256, duration: 0.195s, episode steps:  17, steps per second:  87, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 0.030588, mae: 3.168043, mean_q: 6.489027, mean_eps: 0.682255\n",
            "  7104/20000: episode: 257, duration: 0.369s, episode steps:  34, steps per second:  92, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 0.035469, mae: 3.196172, mean_q: 6.463586, mean_eps: 0.681107\n",
            "  7119/20000: episode: 258, duration: 0.166s, episode steps:  15, steps per second:  90, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 0.063882, mae: 3.255996, mean_q: 6.510228, mean_eps: 0.680005\n",
            "  7157/20000: episode: 259, duration: 0.421s, episode steps:  38, steps per second:  90, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 0.033946, mae: 3.239144, mean_q: 6.529916, mean_eps: 0.678812\n",
            "  7175/20000: episode: 260, duration: 0.211s, episode steps:  18, steps per second:  85, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 0.045743, mae: 3.211096, mean_q: 6.423349, mean_eps: 0.677553\n",
            "  7185/20000: episode: 261, duration: 0.110s, episode steps:  10, steps per second:  91, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.048137, mae: 3.184890, mean_q: 6.482214, mean_eps: 0.676922\n",
            "  7214/20000: episode: 262, duration: 0.320s, episode steps:  29, steps per second:  91, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  loss: 0.028153, mae: 3.225066, mean_q: 6.506183, mean_eps: 0.676045\n",
            "  7239/20000: episode: 263, duration: 0.282s, episode steps:  25, steps per second:  89, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 0.024837, mae: 3.233669, mean_q: 6.537360, mean_eps: 0.674830\n",
            "  7332/20000: episode: 264, duration: 1.034s, episode steps:  93, steps per second:  90, episode reward: 93.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.398 [0.000, 1.000],  loss: 0.040396, mae: 3.267601, mean_q: 6.589368, mean_eps: 0.672175\n",
            "  7361/20000: episode: 265, duration: 0.342s, episode steps:  29, steps per second:  85, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.379 [0.000, 1.000],  loss: 0.037046, mae: 3.254499, mean_q: 6.572766, mean_eps: 0.669430\n",
            "  7400/20000: episode: 266, duration: 0.534s, episode steps:  39, steps per second:  73, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 0.034780, mae: 3.286120, mean_q: 6.618138, mean_eps: 0.667900\n",
            "  7435/20000: episode: 267, duration: 0.486s, episode steps:  35, steps per second:  72, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 0.032103, mae: 3.254185, mean_q: 6.598320, mean_eps: 0.666235\n",
            "  7483/20000: episode: 268, duration: 0.623s, episode steps:  48, steps per second:  77, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 0.048981, mae: 3.349061, mean_q: 6.759192, mean_eps: 0.664367\n",
            "  7499/20000: episode: 269, duration: 0.253s, episode steps:  16, steps per second:  63, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 0.058252, mae: 3.310707, mean_q: 6.708912, mean_eps: 0.662927\n",
            "  7535/20000: episode: 270, duration: 0.484s, episode steps:  36, steps per second:  74, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 0.044586, mae: 3.344006, mean_q: 6.750145, mean_eps: 0.661757\n",
            "  7606/20000: episode: 271, duration: 0.907s, episode steps:  71, steps per second:  78, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 0.035958, mae: 3.339169, mean_q: 6.797421, mean_eps: 0.659350\n",
            "  7667/20000: episode: 272, duration: 0.831s, episode steps:  61, steps per second:  73, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.574 [0.000, 1.000],  loss: 0.043960, mae: 3.399817, mean_q: 6.874659, mean_eps: 0.656380\n",
            "  7710/20000: episode: 273, duration: 0.604s, episode steps:  43, steps per second:  71, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.465 [0.000, 1.000],  loss: 0.024858, mae: 3.387444, mean_q: 6.880991, mean_eps: 0.654040\n",
            "  7726/20000: episode: 274, duration: 0.298s, episode steps:  16, steps per second:  54, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 0.014985, mae: 3.418264, mean_q: 6.911019, mean_eps: 0.652713\n",
            "  7747/20000: episode: 275, duration: 0.390s, episode steps:  21, steps per second:  54, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 0.022975, mae: 3.443290, mean_q: 7.006641, mean_eps: 0.651880\n",
            "  7759/20000: episode: 276, duration: 0.186s, episode steps:  12, steps per second:  64, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.014364, mae: 3.389412, mean_q: 6.949554, mean_eps: 0.651138\n",
            "  7809/20000: episode: 277, duration: 0.777s, episode steps:  50, steps per second:  64, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  loss: 0.036710, mae: 3.474796, mean_q: 7.047881, mean_eps: 0.649743\n",
            "  7868/20000: episode: 278, duration: 1.332s, episode steps:  59, steps per second:  44, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 0.054652, mae: 3.428960, mean_q: 6.977452, mean_eps: 0.647290\n",
            "  7935/20000: episode: 279, duration: 1.263s, episode steps:  67, steps per second:  53, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.433 [0.000, 1.000],  loss: 0.049004, mae: 3.472996, mean_q: 6.995498, mean_eps: 0.644455\n",
            "  7963/20000: episode: 280, duration: 0.522s, episode steps:  28, steps per second:  54, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.070554, mae: 3.558855, mean_q: 7.210857, mean_eps: 0.642317\n",
            "  8002/20000: episode: 281, duration: 0.707s, episode steps:  39, steps per second:  55, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.436 [0.000, 1.000],  loss: 0.033340, mae: 3.506181, mean_q: 7.085410, mean_eps: 0.640810\n",
            "  8025/20000: episode: 282, duration: 0.319s, episode steps:  23, steps per second:  72, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 0.083320, mae: 3.552575, mean_q: 7.166249, mean_eps: 0.639415\n",
            "  8062/20000: episode: 283, duration: 0.434s, episode steps:  37, steps per second:  85, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.649 [0.000, 1.000],  loss: 0.049481, mae: 3.539140, mean_q: 7.165665, mean_eps: 0.638065\n",
            "  8189/20000: episode: 284, duration: 1.408s, episode steps: 127, steps per second:  90, episode reward: 127.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 0.030708, mae: 3.595218, mean_q: 7.273910, mean_eps: 0.634375\n",
            "  8205/20000: episode: 285, duration: 0.185s, episode steps:  16, steps per second:  86, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.188 [0.000, 1.000],  loss: 0.109327, mae: 3.555408, mean_q: 7.137432, mean_eps: 0.631158\n",
            "  8224/20000: episode: 286, duration: 0.228s, episode steps:  19, steps per second:  83, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.368 [0.000, 1.000],  loss: 0.046111, mae: 3.624162, mean_q: 7.352028, mean_eps: 0.630370\n",
            "  8240/20000: episode: 287, duration: 0.186s, episode steps:  16, steps per second:  86, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  loss: 0.078484, mae: 3.655803, mean_q: 7.366113, mean_eps: 0.629582\n",
            "  8269/20000: episode: 288, duration: 0.340s, episode steps:  29, steps per second:  85, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 0.060908, mae: 3.652609, mean_q: 7.373580, mean_eps: 0.628570\n",
            "  8287/20000: episode: 289, duration: 0.220s, episode steps:  18, steps per second:  82, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.074760, mae: 3.605236, mean_q: 7.281104, mean_eps: 0.627512\n",
            "  8302/20000: episode: 290, duration: 0.185s, episode steps:  15, steps per second:  81, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 0.037013, mae: 3.609408, mean_q: 7.344002, mean_eps: 0.626770\n",
            "  8367/20000: episode: 291, duration: 0.760s, episode steps:  65, steps per second:  86, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.446 [0.000, 1.000],  loss: 0.050542, mae: 3.688331, mean_q: 7.456252, mean_eps: 0.624970\n",
            "  8441/20000: episode: 292, duration: 0.832s, episode steps:  74, steps per second:  89, episode reward: 74.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 0.093548, mae: 3.756990, mean_q: 7.557263, mean_eps: 0.621842\n",
            "  8462/20000: episode: 293, duration: 0.283s, episode steps:  21, steps per second:  74, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.025203, mae: 3.728698, mean_q: 7.583494, mean_eps: 0.619705\n",
            "  8477/20000: episode: 294, duration: 0.185s, episode steps:  15, steps per second:  81, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.018117, mae: 3.711315, mean_q: 7.519216, mean_eps: 0.618895\n",
            "  8497/20000: episode: 295, duration: 0.251s, episode steps:  20, steps per second:  80, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 0.013753, mae: 3.711756, mean_q: 7.540078, mean_eps: 0.618108\n",
            "  8524/20000: episode: 296, duration: 0.318s, episode steps:  27, steps per second:  85, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 0.014914, mae: 3.757677, mean_q: 7.671428, mean_eps: 0.617050\n",
            "  8549/20000: episode: 297, duration: 0.303s, episode steps:  25, steps per second:  82, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  loss: 0.046275, mae: 3.718212, mean_q: 7.549919, mean_eps: 0.615880\n",
            "  8599/20000: episode: 298, duration: 0.579s, episode steps:  50, steps per second:  86, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.045418, mae: 3.781380, mean_q: 7.645923, mean_eps: 0.614193\n",
            "  8609/20000: episode: 299, duration: 0.125s, episode steps:  10, steps per second:  80, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.138104, mae: 3.786420, mean_q: 7.620056, mean_eps: 0.612842\n",
            "  8635/20000: episode: 300, duration: 0.308s, episode steps:  26, steps per second:  84, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 0.040883, mae: 3.765510, mean_q: 7.647652, mean_eps: 0.612032\n",
            "  8661/20000: episode: 301, duration: 0.306s, episode steps:  26, steps per second:  85, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 0.026190, mae: 3.752206, mean_q: 7.629688, mean_eps: 0.610863\n",
            "  8685/20000: episode: 302, duration: 0.304s, episode steps:  24, steps per second:  79, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 0.070370, mae: 3.859542, mean_q: 7.805074, mean_eps: 0.609737\n",
            "  8708/20000: episode: 303, duration: 0.275s, episode steps:  23, steps per second:  84, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.652 [0.000, 1.000],  loss: 0.072727, mae: 3.844651, mean_q: 7.819568, mean_eps: 0.608680\n",
            "  8730/20000: episode: 304, duration: 0.280s, episode steps:  22, steps per second:  78, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 0.058697, mae: 3.841126, mean_q: 7.824154, mean_eps: 0.607667\n",
            "  8802/20000: episode: 305, duration: 0.846s, episode steps:  72, steps per second:  85, episode reward: 72.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 0.123711, mae: 3.892273, mean_q: 7.789775, mean_eps: 0.605553\n",
            "  8899/20000: episode: 306, duration: 1.418s, episode steps:  97, steps per second:  68, episode reward: 97.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 0.049878, mae: 3.912903, mean_q: 7.935640, mean_eps: 0.601750\n",
            "  8929/20000: episode: 307, duration: 0.537s, episode steps:  30, steps per second:  56, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 0.025919, mae: 3.939412, mean_q: 8.007960, mean_eps: 0.598893\n",
            "  8946/20000: episode: 308, duration: 0.327s, episode steps:  17, steps per second:  52, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 0.051956, mae: 3.904254, mean_q: 7.996083, mean_eps: 0.597835\n",
            "  8983/20000: episode: 309, duration: 0.651s, episode steps:  37, steps per second:  57, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.595 [0.000, 1.000],  loss: 0.046343, mae: 3.930618, mean_q: 8.016481, mean_eps: 0.596620\n",
            "  9004/20000: episode: 310, duration: 0.395s, episode steps:  21, steps per second:  53, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 0.044185, mae: 3.927682, mean_q: 8.030098, mean_eps: 0.595315\n",
            "  9019/20000: episode: 311, duration: 0.185s, episode steps:  15, steps per second:  81, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.011580, mae: 3.940473, mean_q: 8.041151, mean_eps: 0.594505\n",
            "  9055/20000: episode: 312, duration: 0.429s, episode steps:  36, steps per second:  84, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 0.083340, mae: 3.944621, mean_q: 8.055673, mean_eps: 0.593357\n",
            "  9159/20000: episode: 313, duration: 1.250s, episode steps: 104, steps per second:  83, episode reward: 104.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 0.079503, mae: 4.007768, mean_q: 8.102586, mean_eps: 0.590207\n",
            "  9174/20000: episode: 314, duration: 0.190s, episode steps:  15, steps per second:  79, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 0.134156, mae: 3.947710, mean_q: 7.838449, mean_eps: 0.587530\n",
            "  9193/20000: episode: 315, duration: 0.230s, episode steps:  19, steps per second:  83, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 0.099645, mae: 4.119512, mean_q: 8.258435, mean_eps: 0.586765\n",
            "  9203/20000: episode: 316, duration: 0.120s, episode steps:  10, steps per second:  83, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.046619, mae: 4.071420, mean_q: 8.306743, mean_eps: 0.586112\n",
            "  9240/20000: episode: 317, duration: 0.479s, episode steps:  37, steps per second:  77, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 0.068639, mae: 4.030699, mean_q: 8.185585, mean_eps: 0.585055\n",
            "  9437/20000: episode: 318, duration: 2.258s, episode steps: 197, steps per second:  87, episode reward: 197.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 0.034178, mae: 4.061514, mean_q: 8.281167, mean_eps: 0.579790\n",
            "  9457/20000: episode: 319, duration: 0.241s, episode steps:  20, steps per second:  83, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 0.106578, mae: 4.169869, mean_q: 8.424351, mean_eps: 0.574907\n",
            "  9475/20000: episode: 320, duration: 0.222s, episode steps:  18, steps per second:  81, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.278 [0.000, 1.000],  loss: 0.147489, mae: 4.140630, mean_q: 8.388112, mean_eps: 0.574052\n",
            "  9509/20000: episode: 321, duration: 0.432s, episode steps:  34, steps per second:  79, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 0.048177, mae: 4.095234, mean_q: 8.356762, mean_eps: 0.572883\n",
            "  9544/20000: episode: 322, duration: 0.415s, episode steps:  35, steps per second:  84, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.045881, mae: 4.124281, mean_q: 8.447486, mean_eps: 0.571330\n",
            "  9590/20000: episode: 323, duration: 0.581s, episode steps:  46, steps per second:  79, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.074476, mae: 4.147971, mean_q: 8.426068, mean_eps: 0.569507\n",
            "  9611/20000: episode: 324, duration: 0.258s, episode steps:  21, steps per second:  81, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 0.055657, mae: 4.157586, mean_q: 8.501793, mean_eps: 0.568000\n",
            "  9724/20000: episode: 325, duration: 1.332s, episode steps: 113, steps per second:  85, episode reward: 113.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  loss: 0.071917, mae: 4.211458, mean_q: 8.566860, mean_eps: 0.564985\n",
            "  9761/20000: episode: 326, duration: 0.456s, episode steps:  37, steps per second:  81, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.649 [0.000, 1.000],  loss: 0.127862, mae: 4.271820, mean_q: 8.696128, mean_eps: 0.561610\n",
            "  9781/20000: episode: 327, duration: 0.240s, episode steps:  20, steps per second:  83, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 0.300854, mae: 4.426208, mean_q: 8.831653, mean_eps: 0.560327\n",
            "  9800/20000: episode: 328, duration: 0.231s, episode steps:  19, steps per second:  82, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.368 [0.000, 1.000],  loss: 0.144408, mae: 4.290339, mean_q: 8.612653, mean_eps: 0.559450\n",
            "  9817/20000: episode: 329, duration: 0.218s, episode steps:  17, steps per second:  78, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 0.060772, mae: 4.366910, mean_q: 8.837947, mean_eps: 0.558640\n",
            "  9851/20000: episode: 330, duration: 0.521s, episode steps:  34, steps per second:  65, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 0.049744, mae: 4.281466, mean_q: 8.742135, mean_eps: 0.557493\n",
            "  9872/20000: episode: 331, duration: 0.367s, episode steps:  21, steps per second:  57, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.124163, mae: 4.281841, mean_q: 8.676636, mean_eps: 0.556255\n",
            "  9895/20000: episode: 332, duration: 0.446s, episode steps:  23, steps per second:  52, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.391 [0.000, 1.000],  loss: 0.057475, mae: 4.350147, mean_q: 8.792724, mean_eps: 0.555265\n",
            "  9938/20000: episode: 333, duration: 0.749s, episode steps:  43, steps per second:  57, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 0.074153, mae: 4.315913, mean_q: 8.806429, mean_eps: 0.553780\n",
            " 10003/20000: episode: 334, duration: 1.021s, episode steps:  65, steps per second:  64, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 0.028024, mae: 4.289141, mean_q: 8.754833, mean_eps: 0.551350\n",
            " 10031/20000: episode: 335, duration: 0.356s, episode steps:  28, steps per second:  79, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 0.062336, mae: 4.332357, mean_q: 8.791836, mean_eps: 0.549258\n",
            " 10071/20000: episode: 336, duration: 0.494s, episode steps:  40, steps per second:  81, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 0.055020, mae: 4.352117, mean_q: 8.864466, mean_eps: 0.547727\n",
            " 10088/20000: episode: 337, duration: 0.216s, episode steps:  17, steps per second:  79, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 0.047277, mae: 4.350144, mean_q: 8.789923, mean_eps: 0.546445\n",
            " 10137/20000: episode: 338, duration: 0.565s, episode steps:  49, steps per second:  87, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 0.047508, mae: 4.398826, mean_q: 9.004717, mean_eps: 0.544960\n",
            " 10169/20000: episode: 339, duration: 0.367s, episode steps:  32, steps per second:  87, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 0.149260, mae: 4.406709, mean_q: 8.956937, mean_eps: 0.543138\n",
            " 10259/20000: episode: 340, duration: 1.010s, episode steps:  90, steps per second:  89, episode reward: 90.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 0.082951, mae: 4.432691, mean_q: 9.006434, mean_eps: 0.540392\n",
            " 10287/20000: episode: 341, duration: 0.333s, episode steps:  28, steps per second:  84, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 0.031918, mae: 4.443014, mean_q: 9.100431, mean_eps: 0.537737\n",
            " 10352/20000: episode: 342, duration: 0.747s, episode steps:  65, steps per second:  87, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 0.093529, mae: 4.426094, mean_q: 9.055878, mean_eps: 0.535645\n",
            " 10373/20000: episode: 343, duration: 0.255s, episode steps:  21, steps per second:  82, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 0.059243, mae: 4.546856, mean_q: 9.136769, mean_eps: 0.533710\n",
            " 10404/20000: episode: 344, duration: 0.364s, episode steps:  31, steps per second:  85, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 0.042159, mae: 4.487344, mean_q: 9.193981, mean_eps: 0.532540\n",
            " 10434/20000: episode: 345, duration: 0.364s, episode steps:  30, steps per second:  82, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 0.097206, mae: 4.491795, mean_q: 9.186288, mean_eps: 0.531167\n",
            " 10452/20000: episode: 346, duration: 0.209s, episode steps:  18, steps per second:  86, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 0.064390, mae: 4.494174, mean_q: 9.242948, mean_eps: 0.530087\n",
            " 10464/20000: episode: 347, duration: 0.147s, episode steps:  12, steps per second:  81, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.033237, mae: 4.554962, mean_q: 9.323985, mean_eps: 0.529412\n",
            " 10545/20000: episode: 348, duration: 0.939s, episode steps:  81, steps per second:  86, episode reward: 81.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 0.051325, mae: 4.507451, mean_q: 9.227867, mean_eps: 0.527320\n",
            " 10610/20000: episode: 349, duration: 0.724s, episode steps:  65, steps per second:  90, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 0.075686, mae: 4.538948, mean_q: 9.313327, mean_eps: 0.524035\n",
            " 10642/20000: episode: 350, duration: 0.368s, episode steps:  32, steps per second:  87, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 0.068430, mae: 4.638665, mean_q: 9.414894, mean_eps: 0.521853\n",
            " 10654/20000: episode: 351, duration: 0.148s, episode steps:  12, steps per second:  81, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.136548, mae: 4.564120, mean_q: 9.231769, mean_eps: 0.520862\n",
            " 10714/20000: episode: 352, duration: 0.692s, episode steps:  60, steps per second:  87, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 0.108857, mae: 4.646780, mean_q: 9.548365, mean_eps: 0.519242\n",
            " 10753/20000: episode: 353, duration: 0.456s, episode steps:  39, steps per second:  85, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 0.063698, mae: 4.712493, mean_q: 9.547514, mean_eps: 0.517015\n",
            " 10767/20000: episode: 354, duration: 0.158s, episode steps:  14, steps per second:  88, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 0.036360, mae: 4.678492, mean_q: 9.555316, mean_eps: 0.515822\n",
            " 10815/20000: episode: 355, duration: 0.550s, episode steps:  48, steps per second:  87, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 0.071824, mae: 4.644240, mean_q: 9.537233, mean_eps: 0.514427\n",
            " 10837/20000: episode: 356, duration: 0.250s, episode steps:  22, steps per second:  88, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.409 [0.000, 1.000],  loss: 0.227212, mae: 4.693572, mean_q: 9.530210, mean_eps: 0.512853\n",
            " 10859/20000: episode: 357, duration: 0.369s, episode steps:  22, steps per second:  60, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.208575, mae: 4.766756, mean_q: 9.677007, mean_eps: 0.511862\n",
            " 10895/20000: episode: 358, duration: 0.650s, episode steps:  36, steps per second:  55, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 0.242965, mae: 4.765940, mean_q: 9.541249, mean_eps: 0.510557\n",
            " 10939/20000: episode: 359, duration: 0.783s, episode steps:  44, steps per second:  56, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.088783, mae: 4.696251, mean_q: 9.524809, mean_eps: 0.508757\n",
            " 10962/20000: episode: 360, duration: 0.423s, episode steps:  23, steps per second:  54, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  loss: 0.117842, mae: 4.768913, mean_q: 9.730763, mean_eps: 0.507250\n",
            " 11048/20000: episode: 361, duration: 1.207s, episode steps:  86, steps per second:  71, episode reward: 86.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 0.093148, mae: 4.773069, mean_q: 9.742175, mean_eps: 0.504797\n",
            " 11077/20000: episode: 362, duration: 0.329s, episode steps:  29, steps per second:  88, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  loss: 0.045958, mae: 4.799408, mean_q: 9.846967, mean_eps: 0.502210\n",
            " 11114/20000: episode: 363, duration: 0.450s, episode steps:  37, steps per second:  82, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.568 [0.000, 1.000],  loss: 0.096095, mae: 4.808932, mean_q: 9.824084, mean_eps: 0.500725\n",
            " 11139/20000: episode: 364, duration: 0.302s, episode steps:  25, steps per second:  83, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.640 [0.000, 1.000],  loss: 0.119502, mae: 4.777689, mean_q: 9.738498, mean_eps: 0.499330\n",
            " 11187/20000: episode: 365, duration: 0.549s, episode steps:  48, steps per second:  87, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 0.112775, mae: 4.812957, mean_q: 9.828258, mean_eps: 0.497688\n",
            " 11200/20000: episode: 366, duration: 0.160s, episode steps:  13, steps per second:  81, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 0.052480, mae: 4.903681, mean_q: 9.924747, mean_eps: 0.496315\n",
            " 11230/20000: episode: 367, duration: 0.353s, episode steps:  30, steps per second:  85, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.367 [0.000, 1.000],  loss: 0.060124, mae: 4.829840, mean_q: 9.880750, mean_eps: 0.495348\n",
            " 11248/20000: episode: 368, duration: 0.225s, episode steps:  18, steps per second:  80, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.389 [0.000, 1.000],  loss: 0.163947, mae: 4.820265, mean_q: 9.764225, mean_eps: 0.494268\n",
            " 11305/20000: episode: 369, duration: 0.682s, episode steps:  57, steps per second:  84, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 0.108844, mae: 4.912172, mean_q: 10.024008, mean_eps: 0.492580\n",
            " 11334/20000: episode: 370, duration: 0.352s, episode steps:  29, steps per second:  82, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.448 [0.000, 1.000],  loss: 0.089164, mae: 4.922500, mean_q: 10.134942, mean_eps: 0.490645\n",
            " 11396/20000: episode: 371, duration: 0.728s, episode steps:  62, steps per second:  85, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 0.102724, mae: 4.852352, mean_q: 9.952044, mean_eps: 0.488597\n",
            " 11546/20000: episode: 372, duration: 1.711s, episode steps: 150, steps per second:  88, episode reward: 150.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 0.145252, mae: 4.981103, mean_q: 10.149983, mean_eps: 0.483827\n",
            " 11591/20000: episode: 373, duration: 0.522s, episode steps:  45, steps per second:  86, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 0.088360, mae: 4.999573, mean_q: 10.220745, mean_eps: 0.479440\n",
            " 11621/20000: episode: 374, duration: 0.361s, episode steps:  30, steps per second:  83, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.152362, mae: 5.039043, mean_q: 10.223760, mean_eps: 0.477753\n",
            " 11654/20000: episode: 375, duration: 0.417s, episode steps:  33, steps per second:  79, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.576 [0.000, 1.000],  loss: 0.123070, mae: 4.955916, mean_q: 10.208565, mean_eps: 0.476335\n",
            " 11705/20000: episode: 376, duration: 0.633s, episode steps:  51, steps per second:  81, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 0.161026, mae: 5.023194, mean_q: 10.234922, mean_eps: 0.474445\n",
            " 11719/20000: episode: 377, duration: 0.173s, episode steps:  14, steps per second:  81, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.154829, mae: 5.041477, mean_q: 10.306774, mean_eps: 0.472983\n",
            " 11734/20000: episode: 378, duration: 0.173s, episode steps:  15, steps per second:  87, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 0.084543, mae: 5.181621, mean_q: 10.498623, mean_eps: 0.472330\n",
            " 11761/20000: episode: 379, duration: 0.308s, episode steps:  27, steps per second:  88, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 0.181160, mae: 5.114614, mean_q: 10.423699, mean_eps: 0.471385\n",
            " 11772/20000: episode: 380, duration: 0.142s, episode steps:  11, steps per second:  77, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 0.148659, mae: 5.012769, mean_q: 10.268751, mean_eps: 0.470530\n",
            " 11793/20000: episode: 381, duration: 0.248s, episode steps:  21, steps per second:  85, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 0.196677, mae: 5.074209, mean_q: 10.316325, mean_eps: 0.469810\n",
            " 11810/20000: episode: 382, duration: 0.209s, episode steps:  17, steps per second:  81, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 0.064954, mae: 5.023831, mean_q: 10.334473, mean_eps: 0.468955\n",
            " 11897/20000: episode: 383, duration: 1.344s, episode steps:  87, steps per second:  65, episode reward: 87.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 0.131748, mae: 5.122553, mean_q: 10.456348, mean_eps: 0.466615\n",
            " 11913/20000: episode: 384, duration: 0.290s, episode steps:  16, steps per second:  55, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 0.113214, mae: 5.091533, mean_q: 10.459487, mean_eps: 0.464297\n",
            " 11925/20000: episode: 385, duration: 0.217s, episode steps:  12, steps per second:  55, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.093749, mae: 5.101161, mean_q: 10.540651, mean_eps: 0.463668\n",
            " 11937/20000: episode: 386, duration: 0.208s, episode steps:  12, steps per second:  58, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 0.117037, mae: 5.174944, mean_q: 10.517047, mean_eps: 0.463127\n",
            " 11958/20000: episode: 387, duration: 0.365s, episode steps:  21, steps per second:  58, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 0.167201, mae: 5.155878, mean_q: 10.484684, mean_eps: 0.462385\n",
            " 12036/20000: episode: 388, duration: 1.036s, episode steps:  78, steps per second:  75, episode reward: 78.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 0.115648, mae: 5.163743, mean_q: 10.591968, mean_eps: 0.460157\n",
            " 12048/20000: episode: 389, duration: 0.144s, episode steps:  12, steps per second:  84, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.137566, mae: 5.108340, mean_q: 10.420664, mean_eps: 0.458133\n",
            " 12151/20000: episode: 390, duration: 1.180s, episode steps: 103, steps per second:  87, episode reward: 103.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 0.115185, mae: 5.184371, mean_q: 10.644495, mean_eps: 0.455545\n",
            " 12197/20000: episode: 391, duration: 0.542s, episode steps:  46, steps per second:  85, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 0.079637, mae: 5.261061, mean_q: 10.794343, mean_eps: 0.452193\n",
            " 12215/20000: episode: 392, duration: 0.220s, episode steps:  18, steps per second:  82, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.389 [0.000, 1.000],  loss: 0.153776, mae: 5.256432, mean_q: 10.737263, mean_eps: 0.450752\n",
            " 12340/20000: episode: 393, duration: 1.459s, episode steps: 125, steps per second:  86, episode reward: 125.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 0.171221, mae: 5.257084, mean_q: 10.734249, mean_eps: 0.447535\n",
            " 12355/20000: episode: 394, duration: 0.173s, episode steps:  15, steps per second:  87, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 0.154459, mae: 5.374963, mean_q: 10.880026, mean_eps: 0.444385\n",
            " 12390/20000: episode: 395, duration: 0.415s, episode steps:  35, steps per second:  84, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 0.197577, mae: 5.308945, mean_q: 10.877423, mean_eps: 0.443260\n",
            " 12430/20000: episode: 396, duration: 0.478s, episode steps:  40, steps per second:  84, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 0.211561, mae: 5.362351, mean_q: 10.900538, mean_eps: 0.441573\n",
            " 12493/20000: episode: 397, duration: 0.704s, episode steps:  63, steps per second:  90, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 0.165394, mae: 5.371234, mean_q: 11.003343, mean_eps: 0.439255\n",
            " 12525/20000: episode: 398, duration: 0.362s, episode steps:  32, steps per second:  88, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.344 [0.000, 1.000],  loss: 0.174391, mae: 5.340871, mean_q: 10.946481, mean_eps: 0.437117\n",
            " 12578/20000: episode: 399, duration: 0.600s, episode steps:  53, steps per second:  88, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.547 [0.000, 1.000],  loss: 0.223604, mae: 5.382974, mean_q: 11.002912, mean_eps: 0.435205\n",
            " 12623/20000: episode: 400, duration: 0.517s, episode steps:  45, steps per second:  87, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 0.298041, mae: 5.423898, mean_q: 11.040051, mean_eps: 0.433000\n",
            " 12674/20000: episode: 401, duration: 0.579s, episode steps:  51, steps per second:  88, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.549 [0.000, 1.000],  loss: 0.119079, mae: 5.433269, mean_q: 11.145022, mean_eps: 0.430840\n",
            " 12712/20000: episode: 402, duration: 0.444s, episode steps:  38, steps per second:  86, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.068947, mae: 5.455236, mean_q: 11.267356, mean_eps: 0.428837\n",
            " 12752/20000: episode: 403, duration: 0.478s, episode steps:  40, steps per second:  84, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.219251, mae: 5.431025, mean_q: 11.135389, mean_eps: 0.427082\n",
            " 12770/20000: episode: 404, duration: 0.234s, episode steps:  18, steps per second:  77, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.208611, mae: 5.492740, mean_q: 11.206854, mean_eps: 0.425777\n",
            " 12781/20000: episode: 405, duration: 0.138s, episode steps:  11, steps per second:  80, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.142527, mae: 5.565538, mean_q: 11.282811, mean_eps: 0.425125\n",
            " 12826/20000: episode: 406, duration: 0.508s, episode steps:  45, steps per second:  89, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 0.201731, mae: 5.470433, mean_q: 11.219116, mean_eps: 0.423865\n",
            " 12885/20000: episode: 407, duration: 0.874s, episode steps:  59, steps per second:  68, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 0.140208, mae: 5.468104, mean_q: 11.262041, mean_eps: 0.421525\n",
            " 12906/20000: episode: 408, duration: 0.384s, episode steps:  21, steps per second:  55, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 0.184977, mae: 5.588479, mean_q: 11.435365, mean_eps: 0.419725\n",
            " 12985/20000: episode: 409, duration: 1.360s, episode steps:  79, steps per second:  58, episode reward: 79.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.544 [0.000, 1.000],  loss: 0.117431, mae: 5.495149, mean_q: 11.321931, mean_eps: 0.417475\n",
            " 13041/20000: episode: 410, duration: 0.737s, episode steps:  56, steps per second:  76, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 0.219914, mae: 5.485933, mean_q: 11.323665, mean_eps: 0.414437\n",
            " 13097/20000: episode: 411, duration: 0.640s, episode steps:  56, steps per second:  87, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.145734, mae: 5.578341, mean_q: 11.543322, mean_eps: 0.411917\n",
            " 13139/20000: episode: 412, duration: 0.511s, episode steps:  42, steps per second:  82, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 0.181240, mae: 5.598884, mean_q: 11.530658, mean_eps: 0.409712\n",
            " 13188/20000: episode: 413, duration: 0.608s, episode steps:  49, steps per second:  81, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.551 [0.000, 1.000],  loss: 0.211133, mae: 5.633965, mean_q: 11.522088, mean_eps: 0.407665\n",
            " 13199/20000: episode: 414, duration: 0.141s, episode steps:  11, steps per second:  78, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.160326, mae: 5.629625, mean_q: 11.646638, mean_eps: 0.406315\n",
            " 13288/20000: episode: 415, duration: 1.069s, episode steps:  89, steps per second:  83, episode reward: 89.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.449 [0.000, 1.000],  loss: 0.128705, mae: 5.634829, mean_q: 11.582732, mean_eps: 0.404065\n",
            " 13301/20000: episode: 416, duration: 0.167s, episode steps:  13, steps per second:  78, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 0.166893, mae: 5.619858, mean_q: 11.569824, mean_eps: 0.401770\n",
            " 13328/20000: episode: 417, duration: 0.334s, episode steps:  27, steps per second:  81, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 0.200284, mae: 5.661897, mean_q: 11.713999, mean_eps: 0.400870\n",
            " 13424/20000: episode: 418, duration: 1.082s, episode steps:  96, steps per second:  89, episode reward: 96.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 0.156563, mae: 5.714835, mean_q: 11.765461, mean_eps: 0.398102\n",
            " 13474/20000: episode: 419, duration: 0.580s, episode steps:  50, steps per second:  86, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  loss: 0.187646, mae: 5.800178, mean_q: 11.917897, mean_eps: 0.394817\n",
            " 13515/20000: episode: 420, duration: 0.460s, episode steps:  41, steps per second:  89, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 0.229653, mae: 5.840551, mean_q: 11.922997, mean_eps: 0.392770\n",
            " 13715/20000: episode: 421, duration: 2.225s, episode steps: 200, steps per second:  90, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 0.172804, mae: 5.792041, mean_q: 11.914077, mean_eps: 0.387347\n",
            " 13787/20000: episode: 422, duration: 0.788s, episode steps:  72, steps per second:  91, episode reward: 72.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 0.161813, mae: 5.868544, mean_q: 12.051375, mean_eps: 0.381227\n",
            " 13802/20000: episode: 423, duration: 0.182s, episode steps:  15, steps per second:  83, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 0.126587, mae: 5.912234, mean_q: 12.066212, mean_eps: 0.379270\n",
            " 13839/20000: episode: 424, duration: 0.429s, episode steps:  37, steps per second:  86, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 0.210584, mae: 5.862128, mean_q: 12.100494, mean_eps: 0.378100\n",
            " 13879/20000: episode: 425, duration: 0.557s, episode steps:  40, steps per second:  72, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.131318, mae: 5.877655, mean_q: 12.142269, mean_eps: 0.376367\n",
            " 13904/20000: episode: 426, duration: 0.468s, episode steps:  25, steps per second:  53, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 0.295198, mae: 5.964420, mean_q: 12.326355, mean_eps: 0.374905\n",
            " 14019/20000: episode: 427, duration: 1.943s, episode steps: 115, steps per second:  59, episode reward: 115.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 0.217968, mae: 5.936873, mean_q: 12.195714, mean_eps: 0.371755\n",
            " 14078/20000: episode: 428, duration: 0.691s, episode steps:  59, steps per second:  85, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 0.230167, mae: 6.015847, mean_q: 12.421202, mean_eps: 0.367840\n",
            " 14144/20000: episode: 429, duration: 0.784s, episode steps:  66, steps per second:  84, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 0.190854, mae: 5.937024, mean_q: 12.297865, mean_eps: 0.365027\n",
            " 14170/20000: episode: 430, duration: 0.308s, episode steps:  26, steps per second:  84, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 0.477611, mae: 6.224037, mean_q: 12.582621, mean_eps: 0.362957\n",
            " 14183/20000: episode: 431, duration: 0.163s, episode steps:  13, steps per second:  80, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 0.279756, mae: 6.071045, mean_q: 12.456848, mean_eps: 0.362080\n",
            " 14311/20000: episode: 432, duration: 1.446s, episode steps: 128, steps per second:  89, episode reward: 128.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 0.214143, mae: 6.030058, mean_q: 12.455240, mean_eps: 0.358907\n",
            " 14325/20000: episode: 433, duration: 0.171s, episode steps:  14, steps per second:  82, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.204906, mae: 6.116061, mean_q: 12.496243, mean_eps: 0.355712\n",
            " 14431/20000: episode: 434, duration: 1.176s, episode steps: 106, steps per second:  90, episode reward: 106.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.547 [0.000, 1.000],  loss: 0.429125, mae: 6.222606, mean_q: 12.655640, mean_eps: 0.353013\n",
            " 14454/20000: episode: 435, duration: 0.263s, episode steps:  23, steps per second:  88, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.609 [0.000, 1.000],  loss: 0.163493, mae: 6.137339, mean_q: 12.676187, mean_eps: 0.350110\n",
            " 14475/20000: episode: 436, duration: 0.255s, episode steps:  21, steps per second:  82, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 0.336512, mae: 6.068528, mean_q: 12.505173, mean_eps: 0.349120\n",
            " 14519/20000: episode: 437, duration: 0.519s, episode steps:  44, steps per second:  85, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 0.083607, mae: 6.081400, mean_q: 12.650621, mean_eps: 0.347658\n",
            " 14576/20000: episode: 438, duration: 0.687s, episode steps:  57, steps per second:  83, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 0.204938, mae: 6.170101, mean_q: 12.792628, mean_eps: 0.345385\n",
            " 14590/20000: episode: 439, duration: 0.175s, episode steps:  14, steps per second:  80, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.208796, mae: 6.191410, mean_q: 12.807191, mean_eps: 0.343787\n",
            " 14601/20000: episode: 440, duration: 0.149s, episode steps:  11, steps per second:  74, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.103022, mae: 6.312826, mean_q: 12.950268, mean_eps: 0.343225\n",
            " 14639/20000: episode: 441, duration: 0.430s, episode steps:  38, steps per second:  88, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.272655, mae: 6.113349, mean_q: 12.673648, mean_eps: 0.342122\n",
            " 14651/20000: episode: 442, duration: 0.154s, episode steps:  12, steps per second:  78, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 0.255258, mae: 6.258700, mean_q: 12.860484, mean_eps: 0.340998\n",
            " 14681/20000: episode: 443, duration: 0.351s, episode steps:  30, steps per second:  85, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.272240, mae: 6.160553, mean_q: 12.869618, mean_eps: 0.340052\n",
            " 14737/20000: episode: 444, duration: 0.657s, episode steps:  56, steps per second:  85, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 0.253108, mae: 6.199789, mean_q: 12.833155, mean_eps: 0.338117\n",
            " 14937/20000: episode: 445, duration: 2.618s, episode steps: 200, steps per second:  76, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 0.239196, mae: 6.282044, mean_q: 12.966984, mean_eps: 0.332357\n",
            " 14956/20000: episode: 446, duration: 0.332s, episode steps:  19, steps per second:  57, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 0.355270, mae: 6.422235, mean_q: 13.220651, mean_eps: 0.327430\n",
            " 15004/20000: episode: 447, duration: 0.847s, episode steps:  48, steps per second:  57, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 0.318124, mae: 6.332973, mean_q: 13.115602, mean_eps: 0.325922\n",
            " 15018/20000: episode: 448, duration: 0.273s, episode steps:  14, steps per second:  51, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 0.357194, mae: 6.385107, mean_q: 13.222452, mean_eps: 0.324527\n",
            " 15069/20000: episode: 449, duration: 0.670s, episode steps:  51, steps per second:  76, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 0.219932, mae: 6.357174, mean_q: 13.214173, mean_eps: 0.323065\n",
            " 15123/20000: episode: 450, duration: 0.635s, episode steps:  54, steps per second:  85, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.574 [0.000, 1.000],  loss: 0.173758, mae: 6.369229, mean_q: 13.270682, mean_eps: 0.320703\n",
            " 15136/20000: episode: 451, duration: 0.157s, episode steps:  13, steps per second:  83, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 0.193636, mae: 6.487415, mean_q: 13.263795, mean_eps: 0.319195\n",
            " 15210/20000: episode: 452, duration: 0.846s, episode steps:  74, steps per second:  87, episode reward: 74.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 0.499567, mae: 6.449904, mean_q: 13.208593, mean_eps: 0.317237\n",
            " 15382/20000: episode: 453, duration: 1.909s, episode steps: 172, steps per second:  90, episode reward: 172.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 0.254502, mae: 6.414440, mean_q: 13.294839, mean_eps: 0.311702\n",
            " 15401/20000: episode: 454, duration: 0.226s, episode steps:  19, steps per second:  84, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 0.267770, mae: 6.522943, mean_q: 13.314370, mean_eps: 0.307405\n",
            " 15558/20000: episode: 455, duration: 1.732s, episode steps: 157, steps per second:  91, episode reward: 157.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 0.240279, mae: 6.469826, mean_q: 13.421442, mean_eps: 0.303445\n",
            " 15608/20000: episode: 456, duration: 0.551s, episode steps:  50, steps per second:  91, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.340457, mae: 6.556323, mean_q: 13.537231, mean_eps: 0.298787\n",
            " 15734/20000: episode: 457, duration: 1.379s, episode steps: 126, steps per second:  91, episode reward: 126.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  loss: 0.430100, mae: 6.638969, mean_q: 13.668574, mean_eps: 0.294827\n",
            " 15787/20000: episode: 458, duration: 0.602s, episode steps:  53, steps per second:  88, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.453 [0.000, 1.000],  loss: 0.408343, mae: 6.672112, mean_q: 13.670406, mean_eps: 0.290800\n",
            " 15809/20000: episode: 459, duration: 0.256s, episode steps:  22, steps per second:  86, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.290303, mae: 6.766660, mean_q: 13.741389, mean_eps: 0.289112\n",
            " 15836/20000: episode: 460, duration: 0.336s, episode steps:  27, steps per second:  80, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 0.314259, mae: 6.691012, mean_q: 13.729292, mean_eps: 0.288010\n",
            " 15872/20000: episode: 461, duration: 0.406s, episode steps:  36, steps per second:  89, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 0.395072, mae: 6.666071, mean_q: 13.818717, mean_eps: 0.286592\n",
            " 15890/20000: episode: 462, duration: 0.213s, episode steps:  18, steps per second:  85, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: 0.216844, mae: 6.685118, mean_q: 13.798982, mean_eps: 0.285377\n",
            " 16076/20000: episode: 463, duration: 2.973s, episode steps: 186, steps per second:  63, episode reward: 186.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 0.282339, mae: 6.718182, mean_q: 13.867422, mean_eps: 0.280787\n",
            " 16098/20000: episode: 464, duration: 0.262s, episode steps:  22, steps per second:  84, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.442318, mae: 6.771093, mean_q: 13.867955, mean_eps: 0.276107\n",
            " 16296/20000: episode: 465, duration: 2.214s, episode steps: 198, steps per second:  89, episode reward: 198.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 0.285610, mae: 6.812771, mean_q: 14.064977, mean_eps: 0.271157\n",
            " 16467/20000: episode: 466, duration: 1.879s, episode steps: 171, steps per second:  91, episode reward: 171.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 0.312854, mae: 6.860408, mean_q: 14.172703, mean_eps: 0.262855\n",
            " 16633/20000: episode: 467, duration: 1.826s, episode steps: 166, steps per second:  91, episode reward: 166.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 0.412313, mae: 6.978722, mean_q: 14.360513, mean_eps: 0.255272\n",
            " 16648/20000: episode: 468, duration: 0.175s, episode steps:  15, steps per second:  86, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.424578, mae: 7.131194, mean_q: 14.549332, mean_eps: 0.251200\n",
            " 16759/20000: episode: 469, duration: 1.252s, episode steps: 111, steps per second:  89, episode reward: 111.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 0.382962, mae: 7.014611, mean_q: 14.474387, mean_eps: 0.248365\n",
            " 16878/20000: episode: 470, duration: 1.312s, episode steps: 119, steps per second:  91, episode reward: 119.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 0.463562, mae: 7.035940, mean_q: 14.530702, mean_eps: 0.243190\n",
            " 16894/20000: episode: 471, duration: 0.174s, episode steps:  16, steps per second:  92, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.312 [0.000, 1.000],  loss: 0.312179, mae: 7.061640, mean_q: 14.588141, mean_eps: 0.240152\n",
            " 16959/20000: episode: 472, duration: 0.713s, episode steps:  65, steps per second:  91, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 0.359996, mae: 7.098362, mean_q: 14.630564, mean_eps: 0.238330\n",
            " 17039/20000: episode: 473, duration: 1.290s, episode steps:  80, steps per second:  62, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 0.321919, mae: 7.143802, mean_q: 14.757562, mean_eps: 0.235067\n",
            " 17068/20000: episode: 474, duration: 0.490s, episode steps:  29, steps per second:  59, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  loss: 0.347032, mae: 7.191039, mean_q: 14.846624, mean_eps: 0.232615\n",
            " 17187/20000: episode: 475, duration: 1.645s, episode steps: 119, steps per second:  72, episode reward: 119.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.496 [0.000, 1.000],  loss: 0.325113, mae: 7.162994, mean_q: 14.755516, mean_eps: 0.229285\n",
            " 17316/20000: episode: 476, duration: 1.437s, episode steps: 129, steps per second:  90, episode reward: 129.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.465 [0.000, 1.000],  loss: 0.346094, mae: 7.255791, mean_q: 14.907423, mean_eps: 0.223705\n",
            " 17373/20000: episode: 477, duration: 0.628s, episode steps:  57, steps per second:  91, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 0.339077, mae: 7.192550, mean_q: 14.882198, mean_eps: 0.219520\n",
            " 17401/20000: episode: 478, duration: 0.301s, episode steps:  28, steps per second:  93, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 0.229504, mae: 7.297523, mean_q: 15.043323, mean_eps: 0.217607\n",
            " 17504/20000: episode: 479, duration: 1.136s, episode steps: 103, steps per second:  91, episode reward: 103.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 0.241191, mae: 7.207389, mean_q: 15.011454, mean_eps: 0.214660\n",
            " 17577/20000: episode: 480, duration: 0.837s, episode steps:  73, steps per second:  87, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 0.444768, mae: 7.278252, mean_q: 15.052953, mean_eps: 0.210700\n",
            " 17777/20000: episode: 481, duration: 2.164s, episode steps: 200, steps per second:  92, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 0.333725, mae: 7.367595, mean_q: 15.194746, mean_eps: 0.204557\n",
            " 17803/20000: episode: 482, duration: 0.318s, episode steps:  26, steps per second:  82, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 0.526762, mae: 7.319524, mean_q: 15.068666, mean_eps: 0.199472\n",
            " 18003/20000: episode: 483, duration: 2.258s, episode steps: 200, steps per second:  89, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.510404, mae: 7.486592, mean_q: 15.434346, mean_eps: 0.194387\n",
            " 18174/20000: episode: 484, duration: 2.790s, episode steps: 171, steps per second:  61, episode reward: 171.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 0.277193, mae: 7.509617, mean_q: 15.571188, mean_eps: 0.186040\n",
            " 18285/20000: episode: 485, duration: 1.330s, episode steps: 111, steps per second:  83, episode reward: 111.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 0.516564, mae: 7.610338, mean_q: 15.755218, mean_eps: 0.179695\n",
            " 18364/20000: episode: 486, duration: 0.895s, episode steps:  79, steps per second:  88, episode reward: 79.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 0.685755, mae: 7.729475, mean_q: 15.840195, mean_eps: 0.175420\n",
            " 18493/20000: episode: 487, duration: 1.532s, episode steps: 129, steps per second:  84, episode reward: 129.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 0.417902, mae: 7.662871, mean_q: 15.882541, mean_eps: 0.170740\n",
            " 18574/20000: episode: 488, duration: 0.939s, episode steps:  81, steps per second:  86, episode reward: 81.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 0.486620, mae: 7.683130, mean_q: 15.930992, mean_eps: 0.166015\n",
            " 18649/20000: episode: 489, duration: 0.855s, episode steps:  75, steps per second:  88, episode reward: 75.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 0.402803, mae: 7.704042, mean_q: 16.021710, mean_eps: 0.162505\n",
            " 18713/20000: episode: 490, duration: 0.748s, episode steps:  64, steps per second:  86, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 0.429363, mae: 7.772779, mean_q: 16.112798, mean_eps: 0.159377\n",
            " 18779/20000: episode: 491, duration: 0.748s, episode steps:  66, steps per second:  88, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 0.385486, mae: 7.762475, mean_q: 16.085135, mean_eps: 0.156452\n",
            " 18905/20000: episode: 492, duration: 1.469s, episode steps: 126, steps per second:  86, episode reward: 126.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 0.485424, mae: 7.844941, mean_q: 16.220261, mean_eps: 0.152132\n",
            " 19034/20000: episode: 493, duration: 1.451s, episode steps: 129, steps per second:  89, episode reward: 129.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.465 [0.000, 1.000],  loss: 0.364741, mae: 7.872480, mean_q: 16.292275, mean_eps: 0.146395\n",
            " 19083/20000: episode: 494, duration: 0.804s, episode steps:  49, steps per second:  61, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.551 [0.000, 1.000],  loss: 0.786150, mae: 7.911255, mean_q: 16.287003, mean_eps: 0.142390\n",
            " 19210/20000: episode: 495, duration: 2.100s, episode steps: 127, steps per second:  60, episode reward: 127.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 0.419207, mae: 7.982107, mean_q: 16.428662, mean_eps: 0.138430\n",
            " 19357/20000: episode: 496, duration: 1.671s, episode steps: 147, steps per second:  88, episode reward: 147.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 0.424659, mae: 7.945057, mean_q: 16.423523, mean_eps: 0.132265\n",
            " 19509/20000: episode: 497, duration: 1.706s, episode steps: 152, steps per second:  89, episode reward: 152.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 0.431843, mae: 8.012853, mean_q: 16.631008, mean_eps: 0.125537\n",
            " 19599/20000: episode: 498, duration: 0.999s, episode steps:  90, steps per second:  90, episode reward: 90.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 0.283220, mae: 7.998285, mean_q: 16.658374, mean_eps: 0.120092\n",
            " 19638/20000: episode: 499, duration: 0.450s, episode steps:  39, steps per second:  87, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 0.433997, mae: 8.033456, mean_q: 16.665196, mean_eps: 0.117190\n",
            " 19736/20000: episode: 500, duration: 1.108s, episode steps:  98, steps per second:  88, episode reward: 98.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 0.499854, mae: 8.063252, mean_q: 16.702994, mean_eps: 0.114107\n",
            " 19869/20000: episode: 501, duration: 1.528s, episode steps: 133, steps per second:  87, episode reward: 133.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.451 [0.000, 1.000],  loss: 0.367676, mae: 8.115510, mean_q: 16.933603, mean_eps: 0.108910\n",
            "done, took 253.746 seconds\n"
          ]
        }
      ],
      "source": [
        "memory = SequentialMemory(limit=10000 ,window_length=1)\n",
        "\n",
        "policy =  LinearAnnealedPolicy(inner_policy= EpsGreedyQPolicy(), \n",
        "                               attr='eps',            \n",
        "                               value_max=1.,\n",
        "                               value_min=.1, \n",
        "                               value_test=.05,\n",
        "                               nb_steps=20000)\n",
        "\n",
        "# define the agent\n",
        "dqn = DQNAgent(model=model, \n",
        "               nb_actions=env.action_space.n,\n",
        "               memory=memory,\n",
        "               nb_steps_warmup=10,\n",
        "               target_model_update=0.001, \n",
        "               policy=policy) \n",
        "\n",
        "dqn.compile(Adam(lr = 0.001), metrics=['mae'])\n",
        "\n",
        "history = dqn.fit(env, nb_steps=20000, visualize=False, verbose=2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "id": "Vfw_AHVIbed5",
        "outputId": "2a3c4d5d-7d2b-415f-ec79-61061d1067fd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEGCAYAAABhMDI9AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAABlxUlEQVR4nO19eZgdRbn++51l9mSyLySESSDskABhX2QTAygoV1FQRESRn6h43S6oV7niwr1c4IIoCoKICsoii4IsRiTsIYEQEpYkhCRknayTyeznnPr90V3dVdVVfbrPnJ5zMlPv88wz51RXV33dp/v76luLGGOwsLCwsLDgSFWaAAsLCwuL6oIVDBYWFhYWEqxgsLCwsLCQYAWDhYWFhYUEKxgsLCwsLCRkKk1AfzFmzBjW0tJSaTIsLCwsdiksWLBgM2NsrO7YLi8YWlpaMH/+/EqTYWFhYbFLgYhWmY5ZU5KFhYWFhQQrGCwsLCwsJFjBYGFhYWEhwQoGCwsLCwsJVjBYWFhYWEhIVDAQ0e5E9DQRvUlES4jocrd9FBE9RUTL3P8j3XYiopuIaDkRLSKiQ5Okz8LCwsIiiKQ1hhyAbzLG9gdwFIDLiGh/AFcAmMMYmw5gjvsdAE4HMN39uwTALQnTZ2FhYWGhIFHBwBhbzxh71f3cDuAtAJMAnA3gd2633wH4qPv5bAB3MQcvARhBRBOTpNHCwmLoYXnrTry0YktJ5z6xZAM2tfeUmSLg4YVr8cSSDXhvc0fZx46LAUtwI6IWAIcAeBnAeMbYevfQBgDj3c+TALwvnLbGbVsvtIGILoGjUWDKlCnJEW1hYTEocer1zwAAVl5zZqzzOnpy+NLvF2C/icPx98uPLxs9S9a14fI/LfS+x6Wr3BgQ5zMRNQF4AMDXGWM7xGPM2Sko1m5BjLFbGWOzGGOzxo7VZnRbWFhYlB15d2OzNVs7I5+zaM12rN4S3r+zN98vusqNxAUDEWXhCIU/Msb+4jZv5CYi93+r274WwO7C6ZPdNgsLizKjL1/AM0s3VZqMQY+zbn4eJ1z7dKXJiIWko5IIwO0A3mKMXS8cegTAhe7nCwE8LLR/1o1OOgpAm2BysrCwKCOue3IpLrxjHl4u0dZuMXiRtI/hWAAXAHiDiBa6bd8FcA2Ae4noYgCrAJzrHnsMwBkAlgPoBHBRwvRZWAxZrHSdnFs7eitMya4DFsvovesiUcHAGHsOABkOn6LpzwBcliRNFhYWFiWDCwYTVxsksJnPFhYWFjFRbrlQbZqIFQwWFkMULF4woEWCYFUmGaxgsLCwsLCQYAWDhcUQBw1ye3k5MVS0LCsYLCwsLCKiyiw+icEKBgsLC4uIGCJywQoGCwsLi6jgTmIqs/2t2gSOFQwWFhYWEZEUA682E5UVDBYWFhYREYeBt3f3oeWKR5MjJkFYwWBhYWEREXGikjbv3HVLjVjBYGFhYREVVWbySQpWMFhYWFhERGI+hiqTOFYwWFhYWEQE9zFECUpSu1Rb2YswWMFgYWFhERFxVvZqz758jHMrLESsYLCwsLCIiP7w6758IXLfXMEKBgsLiwpgF7JsVA36c8tCBYMycC6GdpEErGCwsBjysFX0oqI/Jp44pqS+QnTtIgkkvefzHUTUSkSLhbY/E9FC928l3/KTiFqIqEs49qskabOwsLCIi6RMSeqwldYYkt7z+U4ANwO4izcwxj7JPxPRdQDahP7vMsZmJkyThYWFRb9Qio4Vx8cQp28SSHrP57lE1KI7Rk4VqnMBnJwkDRYWFhblQv80hhimpAoLhkr6GI4HsJExtkxom0pErxHRM0R0vOlEIrqEiOYT0fxNmzYlT6mFhYUF+peIFisqaQg7n88DcI/wfT2AKYyxQwB8A8DdRDRcdyJj7FbG2CzG2KyxY8cOAKkWFhYW/dMY4jD73GB2PptARBkA5wD4M29jjPUwxra4nxcAeBfA3pWgz8JiKMBGq8ZHQbMfQy5fwM8eewtbdvZIfVU/RG+Y81n5MXpzQ1NjOBXA24yxNbyBiMYSUdr9PA3AdAArKkSfhcWQgd3zOTp07Prpdzbh13NX4AePLAk9NzwqSR55UGsMRHQPgBcB7ENEa4joYvfQpyCbkQDgBACL3PDV+wFcyhjbmiR9FhYWQxtx8xK8WklCW95l4rkiPoS2rr7I81Q68znpqKTzDO2f07Q9AOCBJOmxsLAIYihnQDMWV2MK3qyo92/99q7os9haSRYWFpWAtSDF97Po+DVvIuWOql3XtXVHnqfC0apWMFhYWAxdxDYlhRwrpnms3WbWGFQyClZjsLCwsKgMyqIxRBxkbYgpSR2iYKurWlhYVAJD2LXgIe7KPCzBTdUYVG1kS4cczhrWt8JywQoGC4uhjqEcrhrXYqPbwc0kLNTWsLnUQ3lrSrKwsLDYNRDGr1Xnc5xzVclgfQwWFhYWFUJsjSFGuGqcsdVxrY/BwsLCokKIWxQvlNkHFAa5c1gElHoobwWDhYWFRWVQDv5rGkJl9mFTqXRY57OFhYVFhVBqSQwdVIUhlvM5EJVkNQYLC4sKYCiXwuCInccg5Dl7bYYbGdQYQkxJyncrGCwsLCqKIRytWnK4qg7bOnvR3Zf3+wZ8DNHHtT4GCwsLi0ohdlSSGc8v34LP3j7P7xvDxxB0VMejq9ywgsHCwmLIIn5UUni46ryVW7Xtuu8iVAXBagwWFhYWFUJc/ts/dh09XNX6GCwsLCqKoeyDLkdUkrkkRgwfg5rgZgWDhYVFJVFpe3YlEf/S+Z7PEXrG8DEENYawvsn/YElv7XkHEbUS0WKh7SoiWktEC92/M4RjVxLRciJ6h4g+lCRtFhYWFuWISoo6Rmjms/Ld5GO495X3MfXKx7Cp3VyptRxIWmO4E8BsTfsNjLGZ7t9jAEBE+8PZC/oA95xfElE6YfosLCyGsDEprvOZ8+soIb7xNIZo5TN+NfddAMDWjt4IFJSORAUDY2wugK1FOzo4G8CfGGM9jLH3ACwHcERixFlYWACorCnp9ufeQ8sVj6I3V5m9LONrDDGK6CWQx7DB3R40m042+6RSPoavENEi19Q00m2bBOB9oc8aty0AIrqEiOYT0fxNmzYlTauFxaBGJfWFm+YsAwB09OQqMn/86qrR2nRjh5uSom3U09mbDz1eLlRCMNwCYE8AMwGsB3Bd3AEYY7cyxmYxxmaNHTu2zORZWAwtDG3ncxmrqwbGDv8eNm6xqKSko5YGXDAwxjYyxvKMsQKA2+Cbi9YC2F3oOtlts7CwSBBxmeNgQnn2YzDVSoruZIgrGJJOgBtwwUBEE4WvHwPAI5YeAfApIqoloqkApgOYp55vYWFRXlSDxlCp7UVjXzp3PkcJV40xVzAqKdhH3LwnaY0hk+TgRHQPgBMBjCGiNQB+COBEIpoJ516sBPAlAGCMLSGiewG8CSAH4DLGWF4zrIWFRVlQeYkwEDH5YYi7U1ocH0OgX8i1qoxex/h7BWlRSNhXn6hgYIydp2m+PaT/TwD8JDmKLCwsVFRePOw6iOVjiFNETzUlaQRWjxC5la8WHwMRXU5Ew8nB7UT0KhGdliRxFhYWyaPSq3agcjuWlcPHYOb4pZfE0DH+npxvQKkm5/PnGWM7AJwGYCSACwBckwhVFhYWQwKcvVVKOCUalRTQGOIU0Qv26ekTTUnVIxi4u+UMAL9njC3B0N7jw8JiUKAKFIaKmbPKk8dgKqLnI0XFNAYZOsYv+hiqKSppARE9CUcwPEFEwwBUJl3RwsKibKiGcNVKVRONOyvXbCjCmli8pBRR6FxRnM+ixpC0jyGO8/liOElpKxhjnUQ0GsBFiVBlYWExYKgGjaFSsimuQNJqDKaSGMKBFFGsPAYd4xc1hqR/s8iCgTFWIKIWAJ8hIgbgOcbYg4lRZmFhkSiqSSDsKs7nOL5nsZ0oXAgFch60Pgbf+Vw1piQi+iWASwG8AScp7UtE9IukCLOwsBgYVIOAqJw5K67GEL2/eF+piI9BPahj/JKPoYpMSScD2I+5+hER/Q5OMpqFhcUujCqQC7uMxhBnPwZRiDg+hugaQzEfQ9JRXHGcz8sBTBG+7w5gWXnJsbCwGCjwsg6VzGOofLhqzP4lkpkiCq+uGjfBrYoyn4cBeIuI5sG5n0cAmE9EjwAAY+ysBOizsLBIGNWgMVRKNpXqfBZrJRk1AdWUFIMOnQbVmx84H0McwfCDxKiwsLCoHKpAMlRKMJRjox5jX+GzozFEp0Ob+ayYkm54aikOntyMU/YbH5mmqIhsSmKMPQOn6F3W/TwPwKuMsWfc7xYWFrsgqiGPoVI0lCXBzRiu6n9OFUl7CEYlFXc+3/7ce3jh3S3hA5eIOFFJXwRwP4Bfu02TATyUAE0WFhYDgKqIRnKJqJjzuQwlMczhqkoeA6Lv3aAzFUkJbgWGXKGATDGJUyLiOJ8vA3AsgB0AwBhbBmBcEkRZWFgMHKpJQAz8vLHPiDyIHK5KYV0D0PsY5AS3XJ4hXQWCoYcx1su/EFEGVWGdtLCw6A+q4SWulMYQF5xO0rSpUBPc1DYAeHbZJvTmCgHnc1tXH15ZudX7vnRjO5a37vS+OxoDQyadzF5rcUZ9hoi+C6CeiD4I4D4Af02EKgsLiwFDJTUGpvk0kNBFJb21fge2dfRqevv3ioSwpCjmoZQmNPjlFVtwwe3zcNOcZYHf4Kk3N+ITv3oR7d19AIDTbpiLB1/zdzruc7WHajAlXQFgE5zM5y8BeIwx9r2wE4joDiJqJaLFQtu1RPQ2ES0iogeJaITb3kJEXUS00P37VfzLsbAYvHhz3Q509ubKPm5VOJ+rKCrp9BufxYd//py+v+ZeRdEYPB+D0LZ6aycAYF1bl/EX6Msz7HCFgwhuVqoGU9JXGWO3McY+wRj7OGPsNiK6vMg5dwKYrbQ9BeBAxtjBAJYCuFI49i5jbKb7d2kM2iwsBjU6e3M446Zn8ZW7Xyv72NXgY6ic81mPtdu79P01J6hax4a27kCflMbH0NHjCPmm2kzob7Bk7Y5AW6+b7JZNV14wXKhp+1zYCYyxuQC2Km1PMsb4suclONFNFhYWIeCMYP7KrUV6xkcVyIVYWsv2zl607ggy35LmLUN1VRVH/WxOoLPvY/Abd7qCobE2Y97TgTG8u8n3LfBxeBZ0OpWMj6FoghsRnQfgfABTeZazi+FQmH4J+DyAPwvfpxLRa3Ain77PGHvWQNMlAC4BgClTpui6WFhYFEFVCAReXTVGiYdDr34KBQasvObM/s8ft79GkJiyp/Xhqv7xnT1OJnOYxlBgQE6IRsqmUujNF7yFQlI+hiiZzy8AWA9gDIDrhPZ2AItKnZiIvgcgB+CPbtN6AFMYY1uI6DAADxHRAe52ohIYY7cCuBUAZs2aVQ3Pt4VFotA5Pcs+eAURR2Mop9mpHGGypSa4iaakdo0fAXCETl4YJ50iIO/7GDIJmZKKCgbG2CoAq4joVABd7r4MewPYF44jOjaI6HMAPgzgFF6tlTHWA6DH/byAiN4FsDeA+aXMYWFhEQ2VFwvV5XyO29/ofC6Sx8AFQ00mZaQjX2BSQT2uISStMcQxUM0FUEdEkwA8CeACOM7lWCCi2QC+A+Asxlin0D6WiNLu52kApgNYEXd8CwuLeKhsuCqrKA2xTUmaM6Ls+RzmYygws76ULzDJVMU1hN6EfQxxRiWXkZ8D4JeMsU8AOCD0BKJ7ALwIYB8iWkNEFwO4GU6l1qeUsNQTACwiooVwSm9cyhgrv6fNwmIXhK6qZ9nG3sVMSWWdtwwaQ+StPZW+Hb1cMJjHyBeYVFCPC4Kko5LiVFclIjoawKfh7P8MAOmwExhj52mabzf0fQDAAzHosbAYMkiSeVdeLFRyo54y1EoyOp99cItPrsDAGAMRec5nxphRMOYZk+bkgqCa8hguh5Nz8CBjbIlr7nk6EaosLCwkJME3OUupAoVh19moR9MWxcfANYYZ//Ukzrr5eQBAJzclFZhxjEKBSQX1VFNSJaOSAHg5CXOF7ysAfI1/J6KfM8a+Wl7yLCwsAD8kspxsoArkgR+uWiFiYm/Uo+lvHsI/IEaTvbG2TZq7wMyD5JniY3BNST2eYKi8j6EYji3jWBYWFgKSXFBXg4CoGBVxfQzuf9HXE0W4hC3sizqfBanJTUeeKakKMp8tLKoaf35lNVqueNQLAxxMSFQwVIEtqVpKYhS9F3F8DJrMZxFiCGtouKqYx0CEFAG9Occ/UQ3hqhYWVY1b/vUuAKC1vafClJQfnikpibCkKkCl8xharngUt/zr3aJ06MNVTX19pEJ+t0KY81mJSiJyxurdhUxJg/OJtbCoAsS1hcdBNZTdTvL6wuf35/3vx98ualmKUkRP1zdMoBeYWWMqKD6GFBFSKUo88zm2YCCiBsOhG/tJi4WFhQHJ+hicwbv78jjoqifwxJINyU1moqFizmf1ezghuqPGPAapVpLfzv0EvKmghKSKyBcg+RhSKcec5Ce4VVgwENExRPQmgLfd7zOI6Jf8OGPszvKTZ2ERH9VgMy83vFpJCY69dnsX2rtzuObvbycwSxEaKpbgxqTnpagpqcSSGKIpaUR9FoDvdygUmFGIqD6GlOdjcBPcqsCUdAOADwHYAgCMsdfhZCtbWFgkjCRMLaoArYgt2CWhkiUxRMZbTEDx46JlKEpJDJHZN7uCgcMJV/W/i1pAgcl5DMRNSdWiMQAAY+x9pSlfRlosLMqCweigTZJvVoN+VTElj8lCtxSNIUpJDPGZrMnIbFcNVxUdyvmCrNGkyBEG1eRjeJ+IjgHAiChLRN8C8FYiVFlYuHhr/Q5ccPvL6O4b2muQgXQ+V8IUV0nnszh1CdGqke6Xjn0zT1uSmb9Y/0iNSnJMSSRs1FN5wXApgMsATAKwFsBM97uFRWL4z4cW49llm71s0aEKzjgSKaLnmUcqp2lVTGFQNYZipiQvA10095jH5tCFq4qZz+IY2bSsMcg+BjlcNSkfQ5ySGJvhFNCzsBgwiA66qBiMzuckE8Cqoex2pTQGtbJpsfscx5QkQse//XIgaqE8QTAwOfOZQEinks98jrK1588RItAZY18zHbOw6C+87NAK01FpDCTfrMi9rliCmxwRJC4q7l+wBh8/bHKgv4o4W3vKx/n5ct9sRtBGCqrz2RmLT1nJzOf5ABYAqANwKIBl7t9MADWJUGVh4UKM9Y58ziB0PvvXX/5r880jlUPFwlWhRiX5+NZ9r2v7B9pKTHDjvylTNYaUojEEwlX9sSpWXZUx9jsAIKL/B+A4xljO/f4rAM8mQpWFhQvvJYjBNwanKak819S6oxt3vbgK3/jg3lWhhXnmlEJ4v/LOKUchST6GInR4+SRSuGp4X0BfRI/Pm1fsVxnF+SxlPqdkh3NSJTHibNQzEsBwAHxXtSa3zcIiMXg+hhhcrFIF2ZJEuWTdN+59Hc8t34yT9h1rHLsScjXKlMtbd+L55Zv7P5c0mRKVVDSPIQizKcmH1pTkdtje1Yf7F6zx2oPOZzUqyR+jYj4GAdcAeI2InoajdZ4A4KqwE4joDgAfBtDKGDvQbRsF4M8AWgCsBHAuY2wbObrWjQDOANAJ4HOMsVfjXIzF4IO3JWKs9e3gkwy6lWop4GG/BSZs1KP8rwSiaERn3/wcOnr7H7YszuQ4n+PkMQQ7RMpj0B53/otCAQgKBl2CG0fFq6syxn4L4EgAD8LZgvNobmYKwZ0AZittVwCYwxibDmCO+x0ATgcw3f27BMAtUWmzGLywGoODcpmS9Dby8s5RCqJMXQ6h4MylmpL8Y6XcA2O4qvA5zMegQsxjUCOWUuTUSuKouGBwcQSA4+FoC4cX6+zu+rZVaT4bABcovwPwUaH9LubgJQAjiGhiTPosBini+A0qyeCSQrmvSGQnXBvj93ggHcGetjKAvxmTPiu1koqdq+1QXDLo+LfpkmWNQfZBqM7niie4EdE1cPZ9ftP9+xoR/bSEOcczxta7nzcAGO9+ngRALLmxxm3T0XIJEc0novmbNm0qgQSLXQWpEsJVB6FcSGRrTw4/AzeBwYvOzYXRQM4pfy4w/THtuRpKTY5zU7gq1x7MGoMalSSOA8+UlElRYhF4cXwMZwCYyZjjtyei3wF4DcB3S52cMcaIKPYzwRi7FcCtADBr1qxByAYsOLygpChlB4q8cLsyyrWi1q2OxXj6SmFg8zTkeyCbloo4n7mvxzCeaRxd8JDpfouCoaA4n4mcBDcgOW0BiG9KGiF8bi5xzo3cROT+b3Xb1wLYXeg32W2zGMLwNIYIjMNbfQ4+uVDyNa3a0oFXV2/zx3H/EwXHLFTw/kUR5uVaHMsaA1NMS0XO1bTpGLyaUS2u7H2hUdzHkDOUxACqRzD8DE5U0p2utrAAwE9KmPMRABe6ny8E8LDQ/llycBSANsHkZDFE4Se4RT9nMAoGfv1xmeMHrv0XzvnlC5oj5LOlKhCoUaYuBxts6+rDi+9u8edlyVRXVSum6msl6efIKBqDyceQTsiMBMSrlXQPEf0LvtP5PxhjoVs9EdE9AE4EMIaI1gD4IZyw13uJ6GIAqwCc63Z/DI65ajmccNWLol+GxWCFv2H60DYlJbkfg29KqpyAiPz79pO4L/1+Pl5a4cfDMMgr8qj7MUhthjIZksagHBP/q5CqqwZ8DORpCqkENYbIgoGIjgWwkDH2CBF9BsB3iOhGxtgq0zmMsfMMh07R9GWw1VotFJQWrjr4BEO5LkmbuVsFtysKDeVgg+9saA/MKzL2Uoro6Z43xlTnc3AM0zXXhCS4ObWSnM/VYkq6BUAnEc0A8A0A7wK4KxGqLCxc+M9+dO5VBXwuEl5dvS2yU1lX7rkUeD4GCAyqghVOVW0lDOWwnKhRPEzxBZTi5Nf7HdT8Azk3QfyvQiyJUSgwKepJNCUlKBdiCYacu6o/G8AvGGO/ADAsGbIsLBxwRhhFY/Cdz9UvGp5+pxXn/PIF/P4lo8ItIYmIIT9/Ibk5itJQZPUsor9C0RlDmR9xfQyugJYYfbCfUzFVmFd0PheZSw1XlTfq8TUFnd+iXIgjGNqJ6EoAnwHwKBGlAGSLnGMxSLG8tX1AGDAP84sz1S4gF/D+1k4AwLKNOyP1L1vSmYYJ+oyqcjcukrai8MFS6FV5qbqyL835rPcxQGHo6hgm+mvUcFWD87laBMMnAfQAuNh1Ok8GcG0iVFlUNRat2Y5Tr5+L3zz7XuJz+RpDHOdzoiSVFVEZfqlRSaFzK1NHvW+tO7rLvtVqKVFJpckxVbrE3MFN16YTFgUYo5K4QDBHJanhqqKPwa+VVBU+BsbYBsbY9YyxZ93vqxlj1scwBPH+1i4AwMI125OfzHM+R+cCA20r7+7LY1tHr/H4hrbuQFvcVzqJWkmqKSmqKe6In87Bp3/zclno0RJmgCoU496R9W1dmjGUqKRSNAZdNrTqY0iJx/w+OgRNSf4xIoDLjSS3HSkqGIjoOfd/OxHtUP8nR5pF1WMA+G+qhBDUgbaI/NstL+CQq5/SHnt19TYcfc0crNzcIbXHJbHcZh4xAcsTEDHOX7BqW5npiaARKuI0zjPx8MK1OPpn/8Sm9h6p3RlCjEoqpjFohICmJEZeGUfSGIrcbzXzmSkmqYFIcIuyUc9x7n/raLYYcPBHPx9jI5eBtpUvWWdeH21u7wFjwJaOXrSMaSx5jnKHq0oZv3wFW0EbnG7mnpxjrqrNpAFoNIYY5M5fqRdkjvM5nI5ic5o1BtEEJBwr8LFMGoO4UY9cRI8gmJKqIcENAIjoUADHwbl/zzHGXkuEKgsLF3xRFIdpVZOPgdOi7tIV35RU2nkmMM1nPkcSt6+7L4+6bNp4XLdS3/8HT6ChJo03rvpQ5HPioqAkkEWNSpLH0PUz+xjCzgPkXdkc+vxjJJTdrqgpiYOIfgCnTPZoAGMA3ElE30+KMIvyQ1VLdwVwh7KqmoehUvsH68CZTq6fe1eW229SEOIpky63vWLTTuz7n4/jL6+uMfbRXV6+wNDenfO+JxGFEzePQRfBZUp6K5bHYNQYMkqCmyJBUlVWRO/TAA5njP2QMfZDAEcBuCAZsizKje6+PKZ99zFc/9TSSpMSC/x9UlfcYagmjYHTHYd+HcpmSvKS2XTO5/LMoYJnGz+xxFxBJ1JJjMA5/aHKHQNqVFKR/hqtyqRFiK26THPTI1ETEpUEoOrCVdcBqBO+18JWP91l0OnufhU1oSoSElRl/SniO5+rqSSGpzHk+ysYnPP7W39fdDgHw1VlQaFDUn6IUkYty+8cN48BwXukoyPoNJY1hjBBmE4pRfSUvgOR4BbHx9AGYAkRPQXnd/wggHlEdBMAMMa+lgB9FmWCX1Jh1wLXlmMx1uqRC4IpSU9UVN5Wbn4s2sCZ0FacjoQEQ4Rh+xuuqp0XxZm81F8QrGF0qMNICW6a4yKEoCTkWVCAcx+Dbo+HciGOYHjQ/eP4V3lJsUgS/Nkq645PA8CASwlXrSaNgUdT5VUfQ8zfoRT7f9iqVFy1+qaN4nPE8fXEQZwERo5y+MsYi1kSQ9OmL4lh9jGoc6qQtAul7Dbg34eqiEpijP2OiOoBTGGMvZMYRRaJgD+Hu5zG4K6KdlUfQzGNIfo48c/py4sMj4GIJPu2rzEovoYQIaTjZ129eez3g8dx/bkzcM6hkwPH/V34zLSWwuPL8TsHopKKZT5r/DHGstviWCT3D6NdFAw6HwPXKJLa1hOIF5X0EQALATzufp9JRI8kRJdFmcEf0vJUqOz/GDFmAxAzKqnCGsPLK7ag5YpH0dre7dnk++98jn9+n5D8oYaiMmFFG0UgcOiuY/NOJ2nsuidLD2yIojEEgnDK42JQHMlF+mt8DPqoJPm7nOAGHPKjJ41ziH23dfYG/DrpaiqJAeAqAEcA2A4AjLGFAKaVnSKLROA/vAOvM9w0ZxnO/sXzJZ3r7cewC2kMtz/3HgDg1VXbPIHW12/nc/xzZMEgD6AyREBMvDKPqRPQtW54ZU+ufyG5xaCukIsJk9n/Nxe/eXZFaB/H3h/dlKQTRqbNe2RTkv+5L19AR6+51lStEK7auqMnkMcwEKakOIKhjzHWprQl+yRYlA39XbH2B9c/tRSvv7+9pHP5C1XNmc+B+d3/RCRoDOXJY4jDC3o1gsEv4MY8Lqju5BYGFnIZfcYfqTjRxZj8VY8swValHlUxet/e0I4fP/pWaB/VrBO1iJ60uY/mstVwVVEL6AoRCgAwaWQ9rjnnIMw+YAJa27uDUUnVlOAGJyLpfABpIppORD8HoNtMtiiIaB8iWij87SCirxPRVUS0Vmg/o5TxLYLggqEcD1MlnLvxEtwqCzECjDMdY1RSxDH772NQ5tVpDCU6nzltvf3QGIpNfecLKzXnlOeXlqOSivUNCtEoRfRI8RuEgQj41BFTsP9uw7Gtsy9QybbaTElfBXAAnNLbd8MJX/16KZMyxt5hjM1kjM0EcBicPZ4fdA/fwI8xxh4rZXyLIAoCs+ovuJAZiAxjz1kay5TUP7r+94l38PM5y2Kfp5ZTThEZE9zi/g4l+RgERq3OLzIudRUcNpM2Zt9t47WNTAgbt5RfLBHnc8Rw1WJ05Auy8zkOD+e5O+OG1QIA2rtz0oKOf66KPAbGWCeA77l/ARDRzxljXy2BhlMAvMsYW5Wkl32oo5waw0CapfhM8ZzP/Zvz5qeXAwC+esr0WOcVmFMSmTOXVKqcCW7O/zi/n87HIEclyW2R8hg0v31BEYgqotBcijAvx8IkEK4aOM6K776m8zswuU8cJs67jh/u5xNnUynPNOjnMVSHxlAMx5Z43qcA3CN8/woRLSKiO4hopO4EIrqEiOYT0fxNmzaVOO3Qgl+Erf8P04AKBs50BlBjKBUqgyRQorWSunrz+Oo9r2Ht9i7tOb2aqCQfosYQzthF6AR0OW53SWPEOMfEl5kyt87kpvsuCqUwLcqbPyqhQt8RDf4GmWIym5/5HGPQmEgwd644iKgGwFkA7nObbgGwJ4CZANYDuE53HmPsVsbYLMbYrLFjxw4Eqbs8ysksB5Lx8qniCKNK+Z69Vbn7nch3mvc3j0F39uNL1uOvr6/D/zz+tvYcNY9BpjWoKUQqiWEwm0RBeB5D/PsTNm3U8dSVffA+yd8jl8RgSkmMErj4sDrfoMO1BMaqLyopCZwO4FXG2EYAYIxtZIzlGWMFALfBCY+1KAPKa0rq/xhRwV+6SldXvW3uCrywfHP4vB6j9QWER7/BlBT1snwfkf8D7nQrjzbV6i3CoilJ62PgNCj/Q+kIMSXFRawwUd35IRRHlcPODm5mU5I6juqXAQDdTysKXiDee8f7NtWKGoM/AE9w21VMSaVQeR4EMxIRTRSOfQzA4v4SZeHAEwzlGKsCGkMUUxK/tn5abbT4yWNv4fwi21n6piTf/MXvexSN4ba5K/Da6m2GsYNtO3scZ29TXXHB4Ce4+WYjtSRGlFW2fnUcfo7pmYsTDaSnxXwsjhYTVhLDJHzCtAzeJrYW8zGMbqwRvjl9G2v9/SvE8z0fQ4KmpFgb9QAAEQ0HwBhj7cqhG2OO0winEN+XhOb/IaKZcATySuWYRT/gx8H3/2kayJ2+vI1uNC/f3KWbsL2rD2fN2A2Av4orZQX78MK1GNlQgxP2Lt006TFf979YzkD1MYjmJo6fPObE3K+85szg4Jpr2tnTBwBoqtG/xuKUUYSAf9/M90/HcKPuYRDWnkQtqChwoofM5wV9DMFxdfckmPkcTof4XvKPjcLvKp5PnmCogqgkIjocwB0AhjlfaTuAzzPGFgAAY+zOOBMzxjrgbPojttn9HRJCGC+/Z95qzNx9BPabODzSWAMbleSaYjRawGfvmAcAnmDwz4mPy/+0EICBKYfg8cXrvc+qxiBusqJqDCqjLgbPoS3wAs+UZNAYRGFkMom43yLTUorGwAwCJ07xOv245mNRBYPqC1AXPUEfQ/BT3CJ6OkiMn7elCE21GezsyXkOZ6Jk8xc8emL0vR3AlxljLYyxPQBcBuC3yZBlUW6EMfMr//IGTr/x2chjVcL5HMeUNJCZz5f+4VXvM88KFpPauKaj+hjial26e97e4wiGmoz+NRbP4XSIWlXQ+RyFjmBbsYWC6XCc/AEdwk6JungpFJikWakC3OhjEM1gBr+LqIsU09RTksbgf+bmJLGt2gRDnjHmcQ/G2HMAciH9LaoIpZRUMKFUjaG0yBOXsUY412d6sacpCzwaufmrUDBmPqs0mgTFH15a5RTj0/kYXI3BdK6oZal9GAtG2ESJStKbTcJvuOl5kez0oSPoUQ7nc17xBeic9Lo5w8xPQDDaqdhrp9MYAN+clNaYmpJEUVMSER3qfnyGiH4Nx1nMAHwSdk+GXQZljUoqcUWeLzBk0vEIiKMxqOf0F2pyUzGopqRcnhmrq6rMROecXt66E99/aDH+vng9TtpnXOD4TldjMDFesV31LciZz7JAC4PexxB+jkngFAv5LIawRyLq81Jg8tzzVm6Vjqu1oXQ+GmO4qvC9FB8DANTXpAPnJxmmyhHFx6DmEvzA/U8oTdBXBZa3tuMvr67FZ49uwYTmuuIn7OLgL0pZEtxKzOItLfIkusbgRSWVSTL05RlqMvr7tWxjOza55aY51DyGvBSVVND2hdBXRVuXUzSuszfvZz4Lx7lgiGKqUU1Jjsbgf9bRxPHe5g6s3tqJD+w9VmLmr67ehkOnjJTnKbCAqcM0bv99DGEaQ3RTkjjOLf96V54j4Bfh7cG2QD9h3GKhpabd2LZ3OgEGIxtrsK6tG0CVmJIYYycxxk6Ck3PwGwBzADwDR1v4V5LEJYmVmzvxy3+9i9b27kqTMiDQOS+B0sw7pWoMpZU9cOeMpTGULhjEeXjtH93q84M3zMX5t8nhqyqDdaKSnDa1JIY/pPOhTxNju0PIUzBlPovzhV2LziSi1kbSMT0A+M2zK/Dt+153xhTGOeeXLwTG1hXSM4UPi+eVsmgIOyXqM5ovBLfOFKHLGA/2Ka4xmDTPjGb/ZnHxxrPaPyBEyw1E6aA4PoaHAHwEQB+AncLfLgkudStZjnogYXpRSlrFuyfFzRcoSTAo5plI58SexQe32wM+k4uatayWxMgL4arFTEmiFsbvL6dlWF0mnAka6Atz7hZY8D6ZbPZdvXmvvEax8g86AadqUiINxeYOQ6HA8LdF6/TO34jPpmNKCjuuCFQm/wcMfpeCGpWkHz+lEwxC3y+fuCd2H1WPySMbvDZujU0yxiJOHsNkxtjsxCgZYPAfpJr2B04SvilJRimCMS+siGPRUJK5wJ0zxsn9ybPY0d3nfQ5jhtp5FdtMLiTBTR1SPN7Rm8OwuqxnKhI1Bt1q0ST0ZY2BT8y/+5JBjUpShUhPvuAJLn0EjjCnxsxovH+iYCjhJ7t73mr89vmV+PFH+/CZo/aINqeGtrC+ZsEQbgZzBK9gSgrRGHph9v19Z/a++M7sfXH3y6u9tqowJQl4gYgOSoySAQZ34AxkeYdKwnc+R7P/ho9V2rmlCCF/xR3nHP/zlp09eGONs7/Ui+9uCdS2V9HeH42Ba1KeMCv44arKGOp30QfBTUjtrpBqqs2GmzsiOJ/9UunwaPRlhWYZLKCnr+DdA9PqmEOvMejpzkvMNf6zscG1uW9WfD2A/tq1NBTCdZWAw1wTlWSK1IqiMaSLaAzqvM7x6hIMxwFYQETvuNVP3yCiRUkRljRK2WR+VwZ/CdVHqrRoEDOTCEN/YtVLNSWddfPz+MjNz+HtDTtw3m0v4cePvhl6brugMfCtKuMWieMvcU5wbKq7mwWikoSVdoerKbR1ObTUZVOh984kNGUbvsrhBKerojGo6M0XvHtgdLS62NDWjeWtsoXZdP9EYViKxuCXeQkySnG83lzBGHKhJrjpjmvHZeY+Tj+5zRT0oauUGs74aUA0hjimpNMTo6IC4BrDUDEleQ+q8kyVZErymESypqRtHb1YsblDmjMKxJeSO+/4tpAq01LR0RvUGCInS3Efg8vv8nnflBT0MXBaETjOtRoekZJnvhNb/Pn4GWZTktBX6SJrDDL96mi9ubzHxIsxwbNudvb2FjPITYxXFIbBRLLi95zTouOTr72/zfsctrNcMVNSUGOQ/4t0yOPKfUy8Xu98Dke1hKsCABhjq5IkZKAx5JzPplVlCaY0E7OLel5UnPZ/c7GpvafouYUCk8IBde+5LxfDXyqRWcXVGAJ5DAVmLLvNGZ8unJXPu93VGPJ5PfPiJhwTExUFhp9L4NOo2stN/LE35yTqFQp6OopFAJl8F1KehSEsNAz8nqp88rXV27wSJ4C8L4WK4lFJeo1BzmPQnyebksI1BlMegw4DkeBW6bLbFQNnJANZKbSSMJmSSrn+gTIlcaEAhG90o16DKRMV0MeLizZyXeil5MQNE1CM0+rfH1UAeDQrphlRcPT0OfO2uRpDzsC8TP4LHa2qj8HxPcttppUzZ6x5xrRzFXsMTIX33t7QLnxXxyz+rJj8Zqu3dkrfwwSDGJW0z/hhweMFJ+fJo1PjkTDdE8n5bOC0pVRKrTbn86CCZ0oaIhpDwfAS9ceUVOzc9W1dkhrfn1sdNpc5R0Bs44Ix+FLlJAbqt4sMUW3TQfUniLWSggluMl3iNXBTEvcxiAJGvDTvd4gTleR9DwobXSgm4AsqMfxWHSsMuuP3LViDL941X5jbrE0UG1dllKrpaEObfoc7QNaCmoUd0zj+PH81Tr1+rr8XB5P+SXSIYELUFxASleRurmDKY9DBCoYEMfRMSXqNIan6RV29eRz9s3/iyr+84dPQD+2sLyTbOljS2mzu0L2fUgSLQGOPy6DzGvOSDt7qP88Fp++0VYWXmp8haQyeKanXO6YKEkDMJzFoDJqoH5H5q4LAFJ/DhWFfvqA1PRZ7hnS+i8Vr25Q+6pihQwIwP9Oq8L7qEXPAQV5g4FlNuZZXVm4D4GshosbFYS6i58PkUOZzys5nI7kA4pVpKRVDVjCkhpjz2cQYS2HWuZCYdo4ul6nOeXuj19Yf7SxMgHPHMofukjjNupWbKFhyAlPRagxFHJniefJ+DKpWI5uS8gINAedzoaDdUtI3JenpCdMYRLbFDH04RJOa7nnRCYtCyNxA8HcI+hiCQs00rzpWn/IbqRFh8jz+XNl0kB3y36I2m5LG4vS2dfZpFy2Fgky3aZFfk3E34ynifBa1CG+bzwQrEg1ZweBrDBUmZIBgdFCWYkqKoDFwiA95f4RwWC7BB679l5SfoBNAnPHqFlsiYxMZv28yCwoLHfj1cebh7Mcgj+X3lWlVnd65fMHLqcgXBAeuwAyK5ZOIv49pfiCovajPipjPob23mvnF+6SjL1iaRf6u1l/Sgc/b2ZuX6BLnHlabQWevOXelIGhjGY0jwBMMLgP3BANzPs/40ZPaZyKq89krmS52LqIQDIAlaSgLBuf/kHE+u8+uar8UmWKcGvbimDrobOL9sdrlikhw0cSjm6YvosYgri778sHr7AlJkOOPEmfyso9BpkoN+RWPd/flvSQ3p28hEAornhvF+ez/Hv686m8UFpXE59FNpVt0iL+HTpgEnsOAYDAf4+DXfcM/luJHf/PNRaJwb6oLFwxOgpszjq5gIj+3NsM1Bv+eLd2obmIp0qyYkgz9al1GJPIhbV6G5qkuR0FMEyomGIhopZskt5CI5rtto4joKSJa5v4fmdT8qSHmfI5iSgozk8jnOP/D7l3eY2SiSSEZUxIgCw7dPJz5616lvLTa9D/nPJNQPI3BM0EJZbdVwVbMx8Adz94xzeUXyyeR9mPgGofw3zvLa9OP0yNoDFpTkuY0XoBQNzeg09yYVOm0WFlrtf3OF1Z650iCoTaDrt6c0eji5DE4n8M0Bh7FKC4clqzdYRg1uBAw+QW4xiD+VuEuBCEsexCbkk5ijM1kjM1yv18BYA5jbDqcKq5XJDXxUHM+RymbIL7MUcYK07Z0mbL9EcK6UgsiZNOF5niOm5LCo5JEBs7bTaYmFXyYPo2PwWTK4dPlpTyGPLZ3+n4TMRpItw1lFFNSMB4/uB+DSRsQBV34XtE+5Gi04HHVHNLTV8C07z6G659a6swl/iYRnl0AOOcWp9prj/AbNtZm0NWXDy00yK9Jt1eIV8G2oAgdBry1IUQw5KOVxOCCQXw3BsBSVBSVFgwqzgbwO/fz7wB8NKmJuMYwVASDLuZ71ZYOnHr9M973yBpDhHBVPzJHZE7R6Q3MWWQPCCcJS7M0ddHdF/QxTLvyUVz023kSjeKKUKcxhDsymZQw5dRKcscyOJ/9EFfRlBTUGPyoJH8MNY/hZ39/Cy1XPOrPYYi24uOoK06dfOmVBGVB+5uHCWKRDlFAqAKab1N614urAmOazIgqLa+t3h6Ye1hdBgXm//66MThZNRrnMw+i4HOJzueuEBNVX74g3V+jj8GdMyxPpxKopGBgAJ4kogVEdInbNp4xxndX3wBgvO5EIrqEiOYT0fxNmzaVNHl6ABPc/veJd6QXthLQrdoWrZFDBsNCMUUUS6wSj4XtCRAHfcpc5936kvSdZ+c68wTP59qQuHIrMODpdzZJTFsyJWkYWljYLGNMFiyFsB3c+P/g8Z5c3mNuKVI0Bsn5zM91vv/6mRXOmBqNTg1XlTOfZVpESkUmu3prJ75x7+uB69ZpgjoNThYMcn9+33j4Jotwz03Pq0gz3xrzgVfXaPsWi0ryNTvnA38mGNNrMleffYDXT9IYDJzW0xiEvjqtdnRjLQBgYnPdoM98Po4xdiicGkyXEdEJ4kHmPBnaJ4IxditjbBZjbNbYsWN1XYpiIE1JNz+9HEBpOQPlQhSmHGY/l8aKoDHoIpf6c6/VcxevCwo10bGqwtMYNIp63qAR8Bc/J/kdzNdQYPL5IkMPJLh5zBv46C+ex5f/+KpEK7/emkxKYjKyU1Z/vVr/DreACAKCH/UFBP/gjyUy39ffl+85n0N3v2f/37NeAptPD3DB7S/j7F88H/gduC0/7ZXDF67H8NyYAgG6hPbG2vCqP5KPIWTbWX4NYoTW/QuCwua0AyY4x5V3qbiPIdyU9KEDxuOWTx+KL5+4p5HGcqJigoExttb93wrgQQBHANhIRBMBwP3fmtT86X7ux/D7l1ah5YpHi0bLiAhbbSYNPyrJjDCmJ41lYEjyfEHh0R+5qJpwehTTQK+QfKWbxtMYUkBnb07S4MSVX582XFVvalKRLzDpN5b2YwhkZ/umpIXvbw/QymmqzaSdPAZlxW+iz9SuagM6HwNT+gLFzYtigT8VT725URovzxieXbYZr7+/PWBz73J/T+4AlrU0PQ0mjUE08QyrCxcMYla5TmMQ+4XRwpHxnNSyP8ZkSuJzSoJB05WIcPpBE71M6aRREcFARI1ENIx/BnAagMUAHgFwodvtQgAPJ0VDup8+hh+74XFRzS9A8YcqSfgbvZj7qKvdj/7iefzm2RWBfnF8DNJ5ESXD399Yj72//3ftnM7YhYB2U8zZ2eP5GAjrtsvbuYq0yuGqwQS3Yj4Gs8agNyWp97A2k0JPztcYajMpJUzUFyjeGKrGoDElqdqF08UXNovXtuHn/1wu0QZEEAwGjUGEp+0IA6uMsttl5nzVLo5pcj6b3j0xPLWxNh1Km2hS02U+c/iCIfxaU+SUxc4VCtHCVbUaQ+Xdz3HKbpcT4wE86KpXGQB3M8YeJ6JXANxLRBcDWAXg3KQISPXTlBSlLISKSgoG3/ls7iO+gH99fR0Wvr8d723uwBeOnyb1M5kwdH2Ktelw7ZPvBBiSVEtIwxBE57NuGs5ECMF7IK2QFbNRe3cfLrh9ntcW7mMImqJMQtS0ZWlTbQY9fYLGkE1JsfY6P4pqovSjqUTBoP4XNQbgDy+tEvqK96O4YChmItWF1Yq/QSZF6OxznM9ckxeHNGnlpig60ZTUVBusfySiUPDp0oWrejRE1BiInOsJRiXpXzxPMIidKy8XKiMYGGMrAMzQtG8BcMpA0NBfU5JXQTOGeShq1E8UXP6n17DHqAZ847R9IvUPKyLHIdraeSje0dNGB/qJCVwm6I5F9bHookNEG70uGkQWDAxn3vQsvnD8VO+473ymwEsqmZIUxi5WAFXpUOFoDP5YTmKa/jzfHCeP0VCbdjUGp39tJi1FJfF7KI5nNCXpNAbuPIXsY5ASBA0ag24RFGZKAoBXVm7F7c+95/b120Wbe0NN2svyzmgWbCZhrC9FwZQ8hnCNIc98d76XhaxBIapgACGbTjmmJCkqSd+fm5KYQWhWCtUWrjpg6O/Wnsx74WMIhjJqDA8vXIebXNU/CnyGYKZXfNG27vQLuAXGihSVFLzWqJevcwIWmH8Nuu05+/IFIUwUWLJuB/79z34ETY8Q5RNWyVMNV1UrWaq2Y5VGKQ8iXzyPQY3oaazJoFvUGFxTkpqlLGesy3RwoZGX+qimJPk6TKY4cVWuW9gUCvpS3Bw/FjKSpWsV5hhWl/XqQqU1PoYooZxnzdgNAAI5C3XZ4qYkPyrJzJF9jaHI+07OotPxCwnNMZzPUZFkLMuQFQxca+xvEb0oDy1/JirqfBYiQ4x9hIdzSwcXDDoGH0UwBNui3muTEzAXIhh68+bVOSD7GFRhZ/Ix5AossFtWX14fyw8419drMCWpAoXfC7VcQ2NtBr3Czm+1mZRkltCFoqr3lV++XBKD9/W/C0eVTGVRMPjXo7vv4valOojXJ9U/kgSDb7jIej4GeY5iGOmWzO7ozUm/YbFKpAUh4ivMlKTWwTIhRc41qOHVxRLcikUlDTSGrGDor/OZI0okD/+hK+tjcP6HMWeRPk8waK5PF8YZ7KNZXfZTMPDfqkvDoBwG5hzXrWw548sXWIBuqSRGTl6pqhTn8gUjo2KCKak+m8a2jl4lvFTuCwA7e3LiEKjLptCXK3g0OlFJYh5DkGb1GfY0hhDnsxSVpJiSTM5nnU1fLEKngygYTNnMTUJIqe9jEPpGeMdGNNQAcMyM4jwHTWrGuGG1xvPyTIxKCtEY8hFNSUTIpFLI5eVnJ2WQDDXaqKTKi4ahKxjKlMcQZTXDbdrl9DHEhc4xq1IuaQw7nd3TdC8CX32H7qqmM0FFvHzTC8rLYuh8DN+5fxG2ueYInclO3KZTpduU2ZzLs4DjU1zNqygUfFPSHqMbsHZ7l2RSE+fhH1XBUJ9NSxnGtdkUcoKvgv9oYTWoPLORxvksag6ieUoN/9XVHbpn3vuBaxbLfugg7qEthdoKzH54ve8gzmjyGKKEhHON4eq/vSm9k+kU4T9m7yv1/fSRU/C5Y1pwzJ6j0dbVh/990inDUSxclQv+ML5NcEyhcUtiyEX0iiF5wTFkBQMRgaj/piSdLV0FFwxRNYb1bV34zv2vR65dFAW6sgSqCaAvz/CLp5fj2WWbsEXxMXT25vDNe1/H1o5ejy7R7q8izDdRDEaNIW/WGAAhO1WzwuwWShuYKp0CQVOSKmSu/tubAYc0h2hK2mN0A3pyBWzeKdc88j6790LdS6Ium0Zf3qexJp3Cu5s6tDkBunE53byPZ5pRzH9i9U/GmFRfyDnu/I8SlRSqMfQIGoMhBFU0JenyGKIsvrjG8I+3WiUBnE5RwGfVVJvBVWcdgKbaDN7f6u/uFpYjkBe0wTABQuQcd0xJIt3hGoPIRqpAYRi6ggFwzEn91Rj4w3LPvNXeyxuAxsfw9Dut+L0QIijiO/cvwr3z12Dee1u1x/uzh0JYeYdcoYBrn3gHF9w+z2O+fLX2wII1eODVNbjxH0sls4OpuF3Uuv06mF68dW1d+P5Db3gRLCboTUl+xqoqOIxRSfmCVsh86fcLtPM6zmenf8voRgAy42/r6sOVf3kDO3tyWrt8ipwVZK8YleQ6T3ntJH5WmMYgCgGV0UqaI9ciEMwi5v1UTUKFs+eE+XeVSmMYzF8NNYJgSAc1+Sh+PPGZEX+zFAVDRbmpRg0sCDMlvbV+B3762FsOjSEbIhDIDVctRNIYdKV5iuUxnLzvOHxy1u740dkHhvbrDyqVx1AVSKUIecZwz7zV2GNUA47Za4x0vC9fwDV/fxuXnbQXRjXWaMfgDzDfwnLlNWcG5/EEg/+AX/TbVwAAFxy1R6A/X2UOqwvGYM9dugnLW3cWu7QAvKgkUZ1XQyg1L7hYfx5wHmBRMOTyDLqqA/0JVzW9oN9/aDFeW709sMoO0mwWDAUWNCXJm7zIK1XdWCbNR6yV1DKmMXD818+swD3zVmNic512lZ1JpVCTdkxHosagm1vyHwSikgTBkCagD7h/wRqMH14XLH8B55lQhalaQtyEPCue4MaxYnOHdB5HQ40fOZTR5DFECdo4di8/rFp89lJEAUbOb6kqMML2Un544TppTBOIHM2jL2Ieg04QFtMYajIp/PfHDw7v1E8MacGQJkKhwAJMfcGqbVje2o7G2gxuf+49xw75iRnaMaKYh/gKQPeS/XzOMnzl5L0kh5NYclnFZ++YZzwWBp3GoDJv3Quo7mPAmLO6JDIXEgP652MwOd94vaOOnnATm97HYHY+G8tu5/WCISxclfefMLzOSJfJLp9JO2aPPtePwSNc5LnhXQeHMfO5wDzBMn/VNlx05yuSDV/a3zqwHaYj8KNlPjufsy7tUSD6GOo1IaXScxphzOb6LL52ynTcNGeZ9BumUxRg+JxJqw7hME1ARLFe2TRhe2cv/vGWb0EwCYYwYVRJDG1TUoq0YZX/dssL+I8H3vDU6NANaSI5n53/6l60AHDdU0slOycAbHMFQzmjmPyoJLFNFQzyfHXZlP9SCg92T66AJlf9j1oSGYhuStLdJ3Euk4/BOz/EYa5zPstRSbKzuFfDlExXISa4NWlq9HjPkSEpLJNyk6NyjsaQSaUCjNk3Jenpd+j2FwGqWU485peQDkYccZ9MnJIYYbZ3FaIwrhc0Bl1F22gh4eRlEYvPR4qCPgbPlKTw5HRIuKp8fvixTIowf9W2SOdEFUYDjSEtGFJFnM/8hQuT6lFWSOQ5nw2ra4UGvjJWGWR/nNG6EgwqPWq0z/C6rOdDEPcmzhWYV7UyjsYQ1ZRkEoh8zLDtNZ3zg/NIUUlRfQwFphVSpssQmW2jYDfnTOFPrzhRPQz6xUZtNo2adAp9blRSOkUBIcjvYVgeAzcReqYkLa3+fZr33hbJSQ44guEPL62SzD86iMl3YZnDKsT7LCah8d9GvD1RtRCuHYl7L6RSQYbPX2d1FR9ZYwgzJYG0+RBmjSHY1zqfKwxHYzA/dJxhhJbjjaAx+AlueoZnYpiqSWTVls6ic5nA6QzzMXT2yk7d5vqs96JyBtXtCidenExMOrtt7go/MkgblRSNVhMj4GOKzPLqjwYdcHofQ1hUktM/RfLcjhCMrrUxwZQkxuarfgLGZGZ+2v7jvX7clJTLmwQDp1kwJQWux/3PzKt48R519wUzvLd29OL7Dy3GPfNWG6+Xz82H0pUyMUFMlhNNSf5zKl5ftN9AJ5h0PgaTKcmUa6CiqMag4RcmZUQnjKqhiN6QFwxhoXD8gQzTGHKFQtEtK/nZJkeeiQaVQd79cvhL+vjiDVi1Rb/Ci+Jj6FA0hub6rEdDpxtvzxOWPI3Bvabrn1qKnzz2Fh59Yz368gX89vn3gjQYrvONNW144d3N3nfjfXLbxagk3U8TFpWUZ0HB4Dl6MylFY9CbkkzIFRh++/xKAEBdTcp76UVTCeCUJRF/h5FuqGUq5TDyvOv0TqcooMV5pqTQcFVfyzOtgtV71KDY+Xd0hUd+eXOz0kxJok+jvkaIKBK0HY7IGoNGMKQp6GPg39Ws9nL4GBzTlUYLMJyl4y1WY6gwUkShZgn+QIqq4b/eacV7gnqdywdj3QPzaDYSFyG+pHLGp9zfGA7r4tI/LMBpN8zVHtPlMagmlYApqT7rvahcIGxzI4K4qYTfo407nFLW+UIBv3thJZasC+6HazLbfeTm53D+bS973033iTNwXlcHcF7uw1tGSv10vwef2glX1fsYatK+YOClKEz+Dh3+8eZGL8chm0p5jErdE8DRGPzvI92ItzSRx1y7+/LIpChQSVZXe0m9raLz2cSsVcFQqwiG7V2+aSmMXz702low5iR9xTEliWbRukxQYxDvT1iC2xePn4oZk5sB6DWWVCqoMXDGq67iozqCQ6OSAGRjMHu9xlB5DO2opBQFatWI4KYV8YH5nBtmypErMG0NGRFeSQwDk5HyAgRmrTK4Hd19aKrNSNmyzktJnkAx1ajv071wykozzJTEtYmtrmPc9zE48/F7UJtJY4chzyBqWGMxH4PkXEwR7rv0GBzyoye9zOfQPRM0piQuEOtr0l7EU03GSVKKE5W0foe/z0M24wiGzt68W/rZDzBQfQw8azeVIi8Kqasv72oM8r1UTUnZNAV8VLoENxWqv0pctQOy8G2okZ+5j8zYDX993QnfvOvFVZix+witySYML63wc3TU/a0BZYEUopF/78z9vc9ZrSkpyPA9U1ISPgaDKcl0hm4sWxKjwkgRoTOEqfOXg/9MupVLrlDAk0vCV/KpIs5ncfUmCgOxP2MMO3tygbovvE8xrYVrRvyFe375ZrTukDesUYXk8LqMxxi50OA5BLycMRcc3d4OXGRcYXJTcU8uj8cXr9d3AtCX09+nLZr8BX5vRQemGuUlQpfgtrWjFylynO38t+A7p2kFg2Hs9duFLFrhJgxTEj12dPVJUSs8mkbWGArIaHwMALB2exdeWekw1mw65dQrEn0O3IFbYKjN6KuLqs9LndJPZNYNqilMEUR8R7ZSdxeb1TIKgBPiq9UYIjqndBpDOhV0BhdLNiuGcB+DwZRkOEdX7bjyYmGIC4Z0irydo3TY3iXX3hFfFo6127rwnQcWhc7DVwAm5i2u3kTTlsiUOnvzYAwYGxAMfMUeLhg40y8wh2F8+jcve1Eyah+HZifk0i+J4RzjwrJBiUrywhvzBaOqzTWG659cikv/8CpeWL5ZOs7HiBOmy9/BYuWVPRo04apbOnoxvD6LdIq838gzJcXwMbS293ifOcMGgqGrfxR8RSQwVNE+3d2XRzpN6OoN3osTr30a//XXN7158oxJGejiftu12WimJNUPEi4YguNRTI2BY2JzHfYa14SV15yJw/YYqQ1XjWrOqzU4n00ag+qbiSoYdKaiYsdNWsD0ccM0fSORkSgqtbXn7kT0NBG9SURLiOhyt/0qIlpLRAvdvzOSpCMt7BylA385eAz8ts6gYFi0ts14fl++gOeXby4alWTWGPzPHa4qP05JnOLnFgtl5SvPgpK5LKJDMBc01mSQSaW8kgeqmYlH3bzbuhMrNu30xuzpK5g1BvdlX9/maCob22WNZUeXuQieCfwl1zEFHfIsqDG8+O4WDK/LSiaAmkzKy3zOpknKaI8SiZZOkcdAm0I2pH/vZ2d6DDWVItS4NDg+hhQuOrYlcI4orLizWsyGF4vomYS0+gyoGoOYZKn6Ds6auVtgPJ2TV8Uxe8qbPrWMbsCLV/r7cqXdUhLPLtskCe/IGoMpKsmQx6COG9nHUKSf3vksY/YBE7DymjO9hd55R+weae6BQqU0hhyAbzLG9gdwFIDLiIgbC29gjM10/x5LkogU6St1crQp1Tp1GckLV283nn/tE+/g0795GZvazZVKxfEBuTaNKDDmLnNW12Ob9BpDsZo2/DoLIT4R0WzRUJP27NN9hUIg25g7n7953+s4+bpnvDF7cgXji1Ngzj1c6UZOqTS/snIbNu7ojqkxxBQMGo1h7fYuNNdnpSgVR2NwTEmqKSLMLyWCC8JiG9JzRpJO+YEO3Mdw2Ul74Vun7W08tybtZO+fedNzXttfX1+Hdza0I8+YkdmpGkNdjSoY/EVQTSaFt6+e7X3/0AET8NevHCf112Vpq1Ad4aqWm0kRVm7pxAW3z8NvnvWj2qKGDGujkjSZz1wQq6bhsP0YdOeboLsPqsYgfl15zZn42TkHG/tWApXa2nM9gPXu53YiegvApIGmI50i7BQcpTypiMPTGNzVuE5jWLvdbM9e+P526XtcH0NbVx+WbWxHc30W37rvdQDAuOGyYOD9i2kM3Z6Pwc9FUCFqDI5gcF6UXJ4FBKi6yTqPxunJ5UNNSef88gUvaUpdtV5296sAHN9GXESNiNHlMQDOql68LzWZFNq7c+jLmx24xcCn0WVBi+Djp4k8B2pXX96zmXPNg5chEZFJpwK//d8Xb8Cct1qx57imGBqD4nwWTEk16VTAfq/yUJ3JRkVAMCh0i+eLOTtRSmJwOlWkKMjIed6E+hyY8pVq0inpvSzmS+nV+MjUkXW/y6ePnCKZGSuJivsYiKgFwCEAeLziV4hoERHdQUQjDedcQkTziWj+pk2bSp47RbJzT33BuGDgjDushhGH6JjrUGrtm8oL8PYNbd1Yu80XND//53J88Ia5UpRPUGOQnb8miKYkXd+aTEpi/g01Ge8FyOUZOnpz2Ge8bw9tNJhHenKFwEvglxZmUiatSXMxRTXpwN9tk5NVhS7zGXCYlKgx1Lgmmt58IVYY5gVH7YGlPz7dpc3VGAz3av+JwwH4K1XRlNQjJJ1xanXMJJvWl3XpzRfw1vodrhYSPK+oj0FYBNVm0gEtUBUCvNx0GD50wHjpu6oxmhhzf0xJpBFY/FpVk6DqS+GoU/w0xTSGVsVEqoNOXl999oHes1NpVFQwEFETgAcAfJ0xtgPALQD2BDATjkZxne48xtitjLFZjLFZY8eOLXl+NVxVfVC3dDgmoB5PMAQ1BhXiQ6yaHEy2fd5+1M/m4KI7Xwkc39DmP2gm57NJY1ixaSdy+YJvSmJ6hlybTkkRWsPrM5IpqbM3j0OmjPCOGwVDXzCSh79Y6vutJtSJOGwPf03QXJ/FXZ8/QtsvbjkGk8awszsnMT/Hx1BALl+Ilbg1rC7j0RLmY2isSeOhy44FICdciaYkv+Ad846ryLrVWFMUZLx8bN1KvqiPocvsYwCCzDFlmEfEJ2btjuvPneF9D9MYxEuNslGPiU6HVrmdCwC+qLrpvEPw9tWzpfLfIlShWew6NwlBCBzqT6cT8qkUxVqEJImKUUFEWThC4Y+Msb8AAGNsI2MszxgrALgNgJ4blAnpFMl72ioPKn9wevoKaOvqw6qt4XVjAEcF5qsxdXcuVYPg6M0VQk1Ba7b5avXEZsf53OzuerV6Sye6+/JaH8PWjl6cfN0z+MEjSzwtwdEYNIIhm5IEWVNtxnuhcnnH+dxcn8U5hzgWvzFKGXIei9+TyxtXo2oeww5NlBfHMXuOxgP/72gADvOfNLJe248Pyc1PYU48ImDDjm60dwfn3dmTC/oYCsw1JTn3YbSh9LoIUWD6PoZg+fTm+qzHBLgATqV8U1K362MAgBm7jwAAzBQEM0dNJoXtnX0oMGCvcU1e+5kHTQQAvLOhPcDE67Ip9LrPmy47uzaTkqKS9M5zRYMgfY0gDk4Cz/IGNGYx4XyRb6r7J5tgKslh1hic57Qhm0ZdNm3UGNTqr8UEQ6tOMCCoYVUzKhWVRABuB/AWY+x6oX2i0O1jABYnSYcqtXv6CpIK7bXnC5jxX0/iDy+tLmr/PvX6Z/CFu+YDCAoC/l2NA+/NF7B0g3mPhTWCeWlUYw3mffcUXOeWAf/CXfNx9d/e1PoNtroaz90vr/ZspIzpzU61mbTE0OuyaU+178nl0d1XQENNBtedOwMvXHFyQHMZ01SLptoMenLBMhL8xVJV93c2tBvLNmTTKYxtqvNobq4PMlfAZ75fOH4aAGDKKHkfhGnCvgjcpMPLVog4cZ9xkimjvsa5H735gtc+9zsn4ehpowPnimgUmEuY81nMNPbDVX0h0dmb9xjlSfuMw0tXnoJT9h0XGEfUZsYN8yPWrjzD2c5y2timACOrzaS954GfL4a11mZS0jNyiEYgqYsLBr2JpS6bwhtXnYZFV30IQDhTFc/f1iEkvfVbY5Dn5JqBWgvNpBlGDYXmOH76mEBbnZJAGJY9XQ2olMZwLIALAJyshKb+DxG9QUSLAJwE4N+TJEJ9SHtyBcz40ZOBfj19su29GOYu3YR8gQVMSe2uYFBNGT25At7eECwhwSE6uJvqMhg3vE5a4b2+ZrtWY1Cd5TWZlBuuGhQi6kslRiXx2kSNtWkQEXYbUR9wwGXSKdRmUlqNgb9Y6grxxRVb8MCra7SmnZpMynNwFhjDcM2q2znm/D9wUjOe+4+TcPFxU6Xje4xu0F5jbSaF78zeBwBwxNRR+K+zDpBe1nHD6tDZm0d3r+8EbqzNYFSTs+JtNKwuZY3B+a9zPotRVPwxTKdIYk7iAnxCc512lSmOIyY/jh9eh3nfOwU3fHJmgOHVZVOepsyZYl0m7b0PKiM8SSOQuKDmTJfnXYj46ccOwpxvnohhdVlP6wizz4vni9p23OqqYeMCwYVKsWgktVxIsXDl//n4wXhJCMN98cqTA3yjusVC5aKSnoP+3iQanqpCtdmqsfqAY9MXIxIaaqOtHl5+b0ugjUdAqc7P3lxBm9XL8f5Wx5T0t68e5zlZxZd97/HDtP6LbcqYjTVptHX1aTUGNfKmPpv2XhhuVqjX7LQlfq/NpLQCijMa3X4MbxjyQEY2ZD1GVWDm1aCofU0e2RA4Plpw1ovF9+pr0p6w2XNsE2oyKWmhsNsIZ/W9paNXutec+TTXZ7U+Ep3vZVhtUKiJDJ0zmhTJgkH9nXSF2ETNTYxYy6ZTngahOo7rsmlPSHNhWF+Txqvf/yB68nmcd+tLaG3vARHwz2+eiKma3eh2H9WAf37zA3j+3S34z4cWo7M3H0jsmrF7MyaNkE2AYRE9JqFhCvpQF3biM/LLTx/qmeCCGoMclWRyev/vJ2bg8JaR+Ma9r0vtxQRDbSaNCc3+uzKxuT5gPaiGkNQwVIeno0JQFwq6wm8toxsk81JDTRoLvn8qXrzyZK/t6o8eiP934p7SefcqWcWAvwpSBVBbV5+3OY8Oa7Z1gciPYuF0cPT0FSTVnrl+BFXYNNRkUGB6R7W6aqqv8Z3PXDCIewyoq9BMmlCbTTumJNXHkNX7GAC/+N53Zu+Dp/79BK99dGOtJ7h52YBnvn0iPn7YZOn8YuWXxC1ZReGZSZH3O/DVP2eSNekUxrgCpXVHt8Rw+D0ZLpi2br9wlvdZJxjU0F5AjqLi15BWCr4FmUnw+iY0++aj3TWCEQgyxtVbfZ/VTJd5dvTk0NyQxbhhdd52s4xBKxQ4po1tkkyr6t4COtNMmCnJtFHOdoMvStUQxN9pWF3GE0rFopJMAqk2k8IeoxsD5x/sFu2LA/W3q3K5MLQFg/qDf/+hoEtjQnO9xGAbajIY3VSLic3+SmhsU43EtAHgoYXrQCTPwQXDYT/+h9T3nnmr8Yiwp6wzj884NuzoRlNNRlr5HbDbcNx/6dGYMbkZ7T05eR/mAsO+//m4t2UpB2dQuqQ+ddVUl015woLXRxJpUu/dzN1HoDaTQkdPTmNKcsbJK5rSIVNGYOVmh0kNq8tiuhAOO6qpxrtevkDbY3Qj/u1QWTAUK8wnOjtFZFIpjHezyPeZMMy9JudYQ23aM5Wsa+vGZMHxzZmdKBgOmeJHUIkmpkNd27zOVCHa9EWNQWRuqmDQmVQmCpnwI0zXatgeFAA+fLDj1hOZ7CdmRc/CFR3TqtapEwIiE1ad+WaNQS8Y1PnE88V7bopK4r6LYs5k1bLwo7MPxHlHTJHa7rzocPzjGycgKkqoHjKgGNKCIYoDSM0b0GXYNtfXaFdHP/zw/t5D2FSbwc7unHHvhvVt3dLqTF15Dlecr0SEWS2jMLw+i53dfVI+hmlDH27n1GXu6pKAOEPhArOhRs8E7r/0aFx5+n7o7stjztuteEgRcpw5X/fUUql9txH1XgnzWuX+jRE0BrHO2NF7jsb9lx6Nj7nRUcWszzxaCpCjdtIpwlkzdsM9XzzK00I4g2isyUjObvF3USOUmmozGCH0FX+3Oz9/hGP+09QrEp+jnCAYxGdSjWpburE9MM5EwVRjYnAHTxqhbQeA46aPwf2XHo0vnjBNew3FIJrqTBnGUpvw3Dzx7zIjNdEvatOHt4zEzecfAiBoXhTNM+I86rA8NJff92LhyMFFUxoH7CYvBIfVZbCXUvfopStPwdPfOhFAULOths14wjCkBUOUcgrnHFo8IXvyyHrtGB87dLJnMhjRkEVvvoB1beZM6XHDaj3ThxoiaCqr0FiTwaurt+Oav7/ttZ16/TPavnxMXcVO1fY7ZVRD4IUR/Sti/1kto1CTSWGlQSCNbtKvZCePqPeL1inMc7SkMchv1ayWUUJuRLhoEFfR933paMxy8yOyaQIR4eg9R3sMhTPlptqMJIhFwcB5z9Qxjbj7i0fihStPlswC4u80vC6LAyc1Y/zwOtz7paMlusRVLBde08Y2SkxMFeC66JiJginJtAq99hMH48+XHKU9lk2n3Pspj/3ilSfjsa8dL7X985sfwDPfPlFqEx3e6vOi+2n4Pd5n/DDPXMehChIu/FWNgWtoYQxdzomQx+XPlaip6eDt2xBhAam71gnNdUZT3JTRerNftWBICwb+IIoOsqOmjZL6HDipGfd88SgcOdVp1z0Au42o1z6kzfVZzznIV1a6VR/HyIYaTHBNA6pdmtuCVRQrtyCCr5h1TnYu2D4yYzfc9tlZmH3gBBw5dRR+9ZlDvT6ijyFOJU2TiWNvwXSk2osbavwoGR3r57/ZKMPYHNyJDDgb4hznhhLqnH/8mkRTEiALBu4TmTqmEcfsOQbD67LSWKqzleOIqfJzJfp5jpw2GrdfOAvfOm0fTBvbhB+725Wq0VrfPWNf/P7iI3DrBYd5oauczjFNtUaHZkNNBkcawmyzBrv+xOZ67K+siqeNbcIeo2VGJzq/1RW/7ncL20ddJf+zR+9hoNnpKJoeg32Ks7YpoxzmXLSWVYRnvZjm6uWkTG7Gbz47C5d+YM8iZ1QWQ3qjHu64Gzus1ivVsMeoRmkTEcAxX2zauQdefm+rtn66E2aof3iu/fjBmPfeVqTIqVv/jiZf4fPHTsUdz7+HLR09mNBch2Wt7Z4Zhe9LfZThxVaTbzhuPv8Q7DWuCV29eXzsly8AgGdT/8XT7yKTIjz8lWOxtaMXNekU3tnYjmeXbca0MY34oLsHcSZNmH2gn1rSEBKVpMMBuw3HknU7AmYwjgMn+U48nVlAzfwVcekH9sTUMU2YfeAE4/y3XnAYDp48QmrjkUi6JD++kmyqlU1J+07wGSTPKVFXgr/93OE4YLfhodEmD375GMxduhk3/GNpQBs4ZT8/a/nsmbtp/V3D6rI4frqT6X/89LF4Zmkr9hjdiHu+eFSok5jjuf84CevbutFYk8EZNz0LAMhm+mfSEBdE6jOheyd8803wmFqocWJzPcYNq0Vrew8aa9LocEvPjxteh99+7nAcpuzcx2nIFczFA0Vcd+4MvLJyK3YfpV+982dFN5bquyq2vW9dNo27Pn8EDprU7O3YV80Y0oJh6hhnBS0ypYkj6rR9uVPRZLkQd4+6/9KjvciHAyc148BJzXhiyQYAwH8//rZ03siGLL5wvCMYUkSY2FyHumzaczR+67R9MLqpBh+ZESx1DAAd7up/3wnD8LljWnCF63D+8MHB/iLPcuykPmM+vGUUGmsy+KCmrMKMyc14fU2bFK6qe1me+PoJeGLJBlz/1FLUZFKYNrbJEQx1GZy0z1g8/c4mfOmEafj13BUAgD3H+syM/wZzvvkBbNnp2JS5Cq+755l0CmcePDF4QMBpBwSFBhduUzTMgAvjhpq0ZFoRnw+uNammAF2sv4pDpoz0Imx05jx1jjDU16Q9oX30nvpFg4rJIxsCIb1RK4pGAbfFf/H4qThsj5Ha8GGuGeoy2T93TAumjW3E5X9aCMBZybeMaURre48XHswfBdP9bq7PuiHGxQXDsLosTt5Xft6dXJwCbvzUTC9RbTeNFnjGQRNwwydn4MklG/H3xRsCZTN0OGHv0sv3DDSGuGBwHlzRyTdhuF4wcF+ByKT++IUjPRvrdMGxyXekEnHUVP3LO6G5HruNqMcvzj8Uh0wZgVye4ZT9xmHh+2140y2Cdm5IlAjftP3rp07H7AMnYtrYpkCfP1x8JMYOq8VfXl3jtamFwVIpwr8poaAcd150BP61tFWyCetWxvtMGIZsmnD9U0vRmyt4JS+G12Xxf586BI8vXo9zZ+2OX89d4UQ9pVOeRsQZxp5jm7Cn+/5ErY8fB7yOk46xcMbGna+3XnCYF7HEceOnZuKVldukLOM44I7qsNLdxer9lxOlVo7Vga+iJzTXS5qmiP13G47rPjEDp2kWICMba3D2zEmeYGioSWPamEbMe28rJjTXYV1bd9GyJFwwmKD6elT84xsfwNKN7ZIGt++EoMmKiPCxQybj1P3G49T9xgc0010dQ1owTBrhCIYR9Vk8+rXjsKm9B8dPH4sd3X346WPyyt4PkPElw7F7+anvuno4Ipobsnjwy8dgR3cOTy7ZgD++vBqHThmBX33mMACQVr9TRjfghOljMbrRrClw8BUwZ9qqLRuAZ1fn+yAAvlkpCkY21uBjh+iFhgpxdbXDrUk0rM4xzXzycCfE71efOdQzz5xx0ET89fV12qQhzh+j1CgKgzjfJ2btjlyB4TNHBe3XvHosL4Km0zjGDa8rqqmEgTPPsH1AACc3Yo8BcFCWI9Hqr185Dju6+3Dk1FHIplM4/8gpof1NCxAdbV84fiqaG7L4xGGTMX/lNpx+UPi9bxnTiBWbO4wFK3Xvh4jdRzUETEvc19JYk8Z9lx4jHRtWl418PbsShrRg2Ht8E3509gGYfeAEaQX4heOm4aePvS2VS+bmBFPNHsDJTDaFigJ+NAXf3Oejh0wK7MjGkUmncOExLUWv4aqzDsAhU0ZI1UhNEPML1L2jS8GNn5op+QkAOXKGO7RV57O4mvzZOQfhgN2Ga19YIsL1587A4RoNLA7E+eqyaVx07FRtv5P3HY+r/vpmaHG//oL7W3RJbyLEFWu14yAh4SvKM1sMj3/9eKzY5Cxi9ho3DFeevp/3uRhuOHcmHnh1TSCc9FefOQwtY0oTtAdNasbVHz0QH9p/vPF9HWwgtaDbroZZs2ax+fPnl33ce+atxlHTRntOPcYYfj13Bc6dtbuUTVsKuvvyuG3uClzygWmR9xEoB9a3deGi376Ctze048MHT8TN5x9a/KQS8Oii9dhtRB0mNtfjsTfW46JjWwa0BMDitW1YsbkDZxXRtnT4/YsrccxeY7CnxiRXLvz+xZU4cZ9xRqdnf/DwwrXYc2xTQGCrWLqxHYvWtAUyyS2GDohoAWNslvaYFQxDC/fNfx/fvn8RLjq2BT/8yAGVJsfCwqJCCBMMQ9qUNBTxkRm7YenGdnzl5OmVJsXCwqJKYQXDEENdNo3vnbl/pcmwsLCoYgzpzGcLCwsLiyCsYLCwsLCwkGAFg4WFhYWFhKoUDEQ0m4jeIaLlRHRFpemxsLCwGEqoOsFARGkAvwBwOoD9AZxHRNZbamFhYTFAqDrBAOAIAMsZYysYY70A/gTg7ArTZGFhYTFkUI2CYRIAccPkNW6bByK6hIjmE9H8TZs2DShxFhYWFoMd1SgYioIxditjbBZjbNbYsbtOKVsLCwuLXQHVmOC2FoBYZ3qy26bFggULNhPRqhLnGgNgc4nn7qqw1zw0YK95aKA/16zfIg9VWCuJiDIAlgI4BY5AeAXA+YyxJQnMNd9UK2Swwl7z0IC95qGBpK656jQGxliOiL4C4AkAaQB3JCEULCwsLCz0qDrBAACMsccAPFZpOiwsLCyGInZJ53MZcWulCagA7DUPDdhrHhpI5JqrzsdgYWFhYVFZDHWNwcLCwsJCgRUMFhYWFhYShqxgGKyF+ojoDiJqJaLFQtsoInqKiJa5/0e67UREN7n3YBERJbMJdMIgot2J6GkiepOIlhDR5W77oLxuIqojonlE9Lp7vf/ltk8lopfd6/ozEdW47bXu9+Xu8ZaKXkA/QERpInqNiP7mfh/U10xEK4noDSJaSETz3bbEn+shKRgGeaG+OwHMVtquADCHMTYdwBz3O+Bc/3T37xIAtwwQjeVGDsA3GWP7AzgKwGXu7zlYr7sHwMmMsRkAZgKYTURHAfhvADcwxvYCsA3AxW7/iwFsc9tvcPvtqrgcwFvC96FwzScxxmYK+QrJP9eMsSH3B+BoAE8I368EcGWl6Srj9bUAWCx8fwfARPfzRADvuJ9/DeA8Xb9d+Q/AwwA+OBSuG0ADgFcBHAknAzbjtnvPOJycoKPdzxm3H1Wa9hKudbLLCE8G8DcANASueSWAMUpb4s/1kNQYEKFQ3yDDeMbYevfzBgDj3c+D7j64JoNDALyMQXzdrkllIYBWAE8BeBfAdsZYzu0iXpN3ve7xNgCjB5Tg8uD/AHwHQMH9PhqD/5oZgCeJaAERXeK2Jf5cV2WCm0VyYIwxIhqUMcpE1ATgAQBfZ4ztICLv2GC7bsZYHsBMIhoB4EEA+1aWomRBRB8G0MoYW0BEJ1aYnIHEcYyxtUQ0DsBTRPS2eDCp53qoagyxCvUNAmwkookA4P5vddsHzX0goiwcofBHxthf3OZBf92Mse0AnoZjRhnh1hoD5Gvyrtc93gxgy8BS2m8cC+AsIloJZ4+WkwHciMF9zWCMrXX/t8JZAByBAXiuh6pgeAXAdDeioQbApwA8UmGaksQjAC50P18IxwbP2z/rRjMcBaBNUFF3GZCjGtwO4C3G2PXCoUF53UQ01tUUQET1cPwpb8EREB93u6nXy+/DxwH8k7lG6F0FjLErGWOTGWMtcN7XfzLGPo1BfM1E1EhEw/hnAKcBWIyBeK4r7VypoFPnDDhVXN8F8L1K01PG67oHwHoAfXBsjBfDsa3OAbAMwD8AjHL7EpzorHcBvAFgVqXpL/Gaj4Nji10EYKH7d8ZgvW4ABwN4zb3exQB+4LZPAzAPwHIA9wGoddvr3O/L3ePTKn0N/bz+EwH8bbBfs3ttr7t/SzifGojn2pbEsLCwsLCQMFRNSRYWFhYWBljBYGFhYWEhwQoGCwsLCwsJVjBYWFhYWEiwgsHCwsLCQoIVDBYWJYCIfkREp5ZhnJ3loMfCopyw4aoWFhUEEe1kjDVVmg4LCxFWY7CwcEFEn3H3OVhIRL92C9XtJKIb3H0P5hDRWLfvnUT0cffzNeTsBbGIiP7XbWshon+6bXOIaIrbPpWIXnRr7P9Ymf/bRPSKew7fY6GRiB4lZ++FxUT0yYG9KxZDEVYwWFgAIKL9AHwSwLGMsZkA8gA+DaARwHzG2AEAngHwQ+W80QA+BuAAxtjBADiz/zmA37ltfwRwk9t+I4BbGGMHwclQ5+OcBqeO/hFw9lg4jIhOgLO3xjrG2AzG2IEAHi/zpVtYBGAFg4WFg1MAHAbgFbec9SlwShIUAPzZ7fMHOOU3RLQB6AZwOxGdA6DTbT8awN3u598L5x0Lp2wJb+c4zf17Dc7+CvvCERRvAPggEf03ER3PGGvr32VaWBSHLbttYeGA4Kzwr5Qaif5T6Sc55RhjOSI6Ao4g+TiAr8Cp/BkGnWOPAPyMMfbrwAFni8YzAPyYiOYwxn5UZHwLi37BagwWFg7mAPi4W/ee76u7B5x3hFfvPB/Ac+JJ7h4QzYyxxwD8O4AZ7qEX4FQBBRyT1LPu5+eVdo4nAHzeHQ9ENImIxhHRbgA6GWN/AHAtgF1qf2qLXRNWY7CwAMAYe5OIvg9nt6wUnOq0lwHoAHCEe6wVjh9CxDAADxNRHZxV/zfc9q8C+C0RfRvAJgAXue2XA7ibiP4DfrlkMMaedP0cL7obDO0E8BkAewG4logKLk3/r7xXbmERhA1XtbAIgQ0ntRiKsKYkCwsLCwsJVmOwsLCwsJBgNQYLCwsLCwlWMFhYWFhYSLCCwcLCwsJCghUMFhYWFhYSrGCwsLCwsJDw/wE+3wxGAAMsCQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 20 episodes ...\n",
            "Episode 1: reward: 200.000, steps: 200\n",
            "Episode 2: reward: 200.000, steps: 200\n",
            "Episode 3: reward: 200.000, steps: 200\n",
            "Episode 4: reward: 200.000, steps: 200\n",
            "Episode 5: reward: 200.000, steps: 200\n",
            "Episode 6: reward: 200.000, steps: 200\n",
            "Episode 7: reward: 200.000, steps: 200\n",
            "Episode 8: reward: 200.000, steps: 200\n",
            "Episode 9: reward: 200.000, steps: 200\n",
            "Episode 10: reward: 200.000, steps: 200\n",
            "Episode 11: reward: 200.000, steps: 200\n",
            "Episode 12: reward: 200.000, steps: 200\n",
            "Episode 13: reward: 200.000, steps: 200\n",
            "Episode 14: reward: 200.000, steps: 200\n",
            "Episode 15: reward: 200.000, steps: 200\n",
            "Episode 16: reward: 200.000, steps: 200\n",
            "Episode 17: reward: 200.000, steps: 200\n",
            "Episode 18: reward: 200.000, steps: 200\n",
            "Episode 19: reward: 200.000, steps: 200\n",
            "Episode 20: reward: 200.000, steps: 200\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f74dc950700>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# summarize the history for number  of episode steps\n",
        "plt.plot(history.history['nb_episode_steps'])\n",
        "plt.ylabel('nb_episode_steps')\n",
        "plt.xlabel('episodes')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "dqn.test(env, nb_episodes=20, visualize=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}